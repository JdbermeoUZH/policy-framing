run:
  supress_warnings: True
  n_jobs: -1

dataset:
  languages: ['en']#, 'en', 'fr', 'po', 'it', 'ge']   # 'en, 'it', 'fr', 'po', 'ru', 'ge'
  subtask: 2                        # 1, 2, 3
  data_dir: './data'               # Relative path to directory with data

preprocessing:
  analysis_unit: 'raw_text'             # 'title', 'title_and_first_paragraph', 'title_and_5_sentences', 'title_and_10_sentences', # 'title_and_first_sentence_each_paragraph', 'raw_text', 'all'
  load_preproc_input_data: True     # Whether or not to rerun the script that produces the units of analysis
  spacy_model_size: 'small'         # small or large. small should be enough for preprocessing, it is mostly tokenizing so far
  use_tfidf: True                   # True := TFIDFVectorizer, False := CountVectorizer
  min_df: 0.01                      # [0, 1.0] minimum document frequency for a term to be included
  max_df: 0.8                      # [0, 1.0] maximum document frequency for a term to be included
  max_features: 10000               # Maximum number of features
  ngram_range: [1,3]                # ngrams to use around a word. 1 is the word itself
  min_var: 0.0                        # Minimum variance a vector should have to be used in the trainset
  corr_threshold: 1.0              # Remove columns that have correlation >= threshold to another higher

evaluate:
  train_set: 'train_and_dev'        # May be 'train_and_dev' or 'train'
  eval_set: 'test'                  # May be 'test' or 'dev'
  output_dir: './predictions'

  models:
    n_folds: 4
    #model_list: ['LogisticRegressionRidgeDual', 'LogisticRegressionRidgeDual_ROS', 'LogisticRegressionRidgeDual_SMOTE', 'LogisticRegressionRidgeDual_BorderlineSMOTE', 'LogisticRegressionRidgeDual_SVMSMOTE', 'SVM_sigmoid', 'SVM_sigmoid_ROS', 'SVM_sigmoid_SMOTE', 'SVM_sigmoid_BorderlineSMOTE', 'SVM_sigmoid_SVMSMOTE', 'LinearSVMDual', 'LinearSVMDual_ROS', 'LinearSVMDual_SMOTE', 'LinearSVMDual_BorderlineSMOTE', 'XGBoost_broad', 'ComplementNaiveBayes', 'ComplementNaiveBayes_ROS', 'ComplementNaiveBayes_SMOTE', 'ComplementNaiveBayes_BorderlineSMOTE', 'ComplementNaiveBayes_SVMSMOTE', 'RandomForest', 'RandomForest_ROS', 'RandomForest_SMOTE', 'RandomForest_BorderlineSMOTE', 'RandomForest_SVMSMOTE']
    model_list: ['LogisticRegressionRidgeDual']
    hyperparam_module: 'evaluate.hyperparam_space_config_best_models'           # .py file with models and distribution of hyperparams to tune and benchmark
    mlb_cls_independent: True       # If True, and 'independent' multilabel model is used. If False, a 'chain' mlb model is used. 'independent' learns probabilities indep. for each class
    ranking_score: 'f1_micro'       # Metric to choose best performing model in each inner fold


metric_logging:
  logging_path: './mlruns'          # Directory to store metrs. ir must be named 'mlruns'
  experiment_base_name: 'en_tune_best_models_tune_preproc_v3' # Name of the experiment under which to store metrics
  rewrite_experiment: False         # Whether to delete/overwrite runs under experiment_name. In most of the cases it should be False
  logging_level: 'outer_cv'         # Which performance data to log ('model_wide', 'outer_cv', or 'inner_cv')
