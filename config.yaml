run:
  supress_warnings: True
  n_jobs: -1

dataset:
  languages: ['it']  # 'en, 'it', 'fr', 'po', 'ru', 'ge'
  subtask: 2                        # 1, 2, 3
  data_dir: 'data'                  # Relative path to directory with data

preprocessing:
  split: 'train_and_dev'                  # Split of the data to use
  analysis_unit: ['title_and_first_sentence_each_paragraph']             # 'title', 'title_and_first_paragraph', 'title_and_5_sentences', 'title_and_10_sentences', 'title_and_first_sentence_each_paragraph', 'raw_text', 'all'
  load_preproc_input_data: True           # Whether or not to rerun the script that produces the units of analysis
  spacy_model_size: 'small'               # small or large. small should be enough for preprocessing, it is mostly tokenizing so far
  preprocessing_hyperparam_module: 'training.italian.preprocesing_params_config'
  use_same_params_across_units: False     # Use the same preprocessing parameters for all units of analysis
  param_search:
    tune_preprocessing_params: False       # If True, will do LHS over the search space of the preprocessing parameters
    n_samples: 7

training:
  mlb_cls_independent: True       # If True, and 'independent' multilabel model is used. If False, a 'chain' mlb model is used. 'independent' learns probabilities indep. for each class
  default_params: False             # If True, it runs cross validation with outer_folds folds. If False, it runs nested cross validation
  metric_to_report: 'f1_micro'      # Metric to report
  return_train_metrics: True        # Whether to return the metrics on the train set or not

  # Default model list
  #model_list: ['DummyProbSampling', 'DummyUniformSampling', 'DummyMostFrequent', 'LogisticRegression', 'LogisticRegressionRidge', 'LogisticRegressionRidgeDual', 'LogisticRegressionLasso', 'LogisticRegressionElasticNet', 'RidgeClassifier', 'SVM', 'LinearSVM', 'LinearSVMDual', 'RandomForest', 'XGBoost', 'ComplementNaiveBayes', 'NaiveBayes', 'kNN']

  # Best model list
  model_list: ['LogisticRegression', 'LogisticRegression_ROS', 'LogisticRegression_SMOTE', 'LogisticRegression_BorderlineSMOTE', 'LogisticRegression_SVMSMOTE', 'LogisticRegressionRidgeDual', 'LogisticRegressionRidgeDual_ROS', 'LogisticRegressionRidgeDual_SMOTE', 'LogisticRegressionRidgeDual_BorderlineSMOTE', 'LogisticRegressionRidgeDual_SVMSMOTE', 'RakelD_LogisticRegression', 'LogisticRegressionLasso', 'LogisticRegressionLasso_ROS', 'LogisticRegressionLasso_SMOTE', 'LogisticRegressionLasso_BorderlineSMOTE', 'LogisticRegressionLasso_SVMSMOTE', 'LogisticRegressionElasticNet', 'LogisticRegressionElasticNet_ROS', 'LogisticRegressionElasticNet_SMOTE', 'LogisticRegressionElasticNet_BorderlineSMOTE', 'LogisticRegressionElasticNet_SVMSMOTE', 'RidgeClassifier', 'RidgeClassifier_ROS', 'RidgeClassifier_SMOTE', 'RidgeClassifier_BorderlineSMOTE', 'RidgeClassifier_SVMSMOTE', 'SVM_rbf', 'SVM_rbf_ROS', 'SVM_rbf_SMOTE', 'SVM_rbf_BorderlineSMOTE', 'SVM_rbf_SVMSMOTE', 'SVM_sigmoid', 'SVM_sigmoid_ROS', 'SVM_sigmoid_SMOTE', 'SVM_sigmoid_BorderlineSMOTE', 'SVM_sigmoid_SVMSMOTE', 'RakelD_SVM', 'LinearSVMDual', 'LinearSVMDual_ROS', 'LinearSVMDual_SMOTE', 'LinearSVMDual_BorderlineSMOTE', 'LinearSVMDual_SVMSMOTE', 'RakelD_LineaSVM', 'kNN', 'kNN_ROS', 'kNN_SMOTE', 'kNN_BorderlineSMOTE', 'kNN_SVMSMOTE', 'XGBoost_narrow', 'XGBoost_broad', 'XGBoost_narrow_ROS', 'XGBoost_broad_ROS', 'XGBoost_narrow_SMOTE', 'XGBoost_broad_SMOTE', 'XGBoost_narrow_BorderlineSMOTE', 'XGBoost_broad_BorderlineSMOTE', 'XGBoost_narrow_SVMSMOTE', 'XGBoost_broad_SVMSMOTE', 'ComplementNaiveBayes', 'ComplementNaiveBayes_ROS', 'ComplementNaiveBayes_SMOTE', 'ComplementNaiveBayes_BorderlineSMOTE', 'ComplementNaiveBayes_SVMSMOTE', 'RakelD_ComplementNB', 'NaiveBayes', 'NaiveBayes_ROS', 'NaiveBayes_SMOTE', 'NaiveBayes_BorderlineSMOTE', 'NaiveBayes_SVMSMOTE', 'RandomForest', 'RandomForest_ROS', 'RandomForest_SMOTE', 'RandomForest_BorderlineSMOTE', 'RandomForest_SVMSMOTE', 'BRkNNaClassifier', 'BRkNNbClassifier', 'MLkNN', 'MLARAM']

  model_hyperparam_module: 'training.italian.hyperparam_space_config'           # .py file with models and distribution of hyperparams to tune and benchmark

  nested_cv:                        # Total training runs are outer_folds(inner_folds * num_search_iters + 1)
    outer_folds: 3                  # Number of outer fold: Number of "best" models after tuning
    inner_folds: 3                  # Number of inner folds: Number of folds to validate each hyperparam combination
    n_search_iter: 60               # Number of search iterations per outer fold (it is measured with cv of #inner_folds)
    ranking_score: 'f1_micro'       # Metric to choose best performing model in each inner fold


metric_logging:
  logging_path: './mlruns'          # Directory to store metrs. ir must be named 'mlruns'
  experiment_base_name: 'benchmark_fixed' # Name of the experiment under which to store metrics
  rewrite_experiment: False         # Whether to delete/overwrite runs under experiment_name. In most of the cases it should be False
  logging_level: 'outer_cv'         # Which performance data to log ('model_wide', 'outer_cv', or 'inner_cv')
