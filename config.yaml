run:
  supress_warnings: True
  n_jobs: -1

dataset:
  languages: ['ru']   # 'en, 'it', 'fr', 'po', 'ru', 'ge'
  subtask: 2                        # 1, 2, 3
  data_dir: 'data'                  # Relative path to directory with data

preprocessing:
  split: 'train'                    # Split of the data to use
  analysis_unit: ['title', 'title_and_first_paragraph', 'title_and_5_sentences', 'title_and_10_sentences', 'title_and_first_sentence_each_paragraph', 'raw_text']              # 'title', 'title_and_first_paragraph', 'title_and_5_sentences', 'title_and_10_sentences',
                                    # 'title_and_first_sentence_each_paragraph', 'raw_text', 'all'
  load_preproc_input_data: True     # Whether or not to rerun the script that produces the units of analysis
  spacy_model_size: 'small'         # small or large. small should be enough for preprocessing, it is mostly tokenizing so far
  use_tfidf: True                   # True := TFIDFVectorizer, False := CountVectorizer
  min_df: 0.05                      # [0, 1.0] minimum document frequency for a term to be included
  max_df: 0.95                      # [0, 1.0] maximum document frequency for a term to be included
  max_features: 10000               # Maximum number of features
  ngram_range: [1,1]                # ngrams to use around a word. 1 is the word itself
  min_var: 0.001                    # Minimum variance a vector should have to be used in the trainset
  corr_threshold: 0.9               # Remove columns that have correlation >= threshold to another higher

  tune_preprocessing_params: False  # If True, will do LHS over the ranges of the parameters next
  param_search:
    n_samples: 4
    min_df_range: [0, 0.3]
    max_df_range: [0.6, 1]

training:
  default_params: False             # If True, it runs cross validation with outer_folds folds. If False, it runs nested cross validation
  model_list: ['RandomForestSK_V1', 'RandomForestSK_V2', 'RandomForestSK_V3']
  model_hyperparam_module: 'training.hyperparam_space_config_russian'           # .py file with models and distribution of hyperparams to tune and benchmark
  nested_cv:                        # Total training runs are outer_folds(inner_folds * num_search_iters + 1)
    outer_folds: 3                  # Number of outer fold: Number of "best" models after tuning
    inner_folds: 3                  # Number of inner folds: Number of folds to validate each hyperparam combination
    n_search_iter: 2                # Number of search iterations per outer fold (it is measured with cv of #inner_folds)
    ranking_score: 'f1_micro'       # Metric to choose best performing model in each inner fold
  metric_to_report: 'f1_micro'      # Metric to report
  return_train_metrics: True        # Whether to return the metrics on the train set or not
  mlb_cls_independent: True         # If True, and 'independent' multilabel model is used. If False, a 'chain' mlb model is used. 'independent' learns probabilities indep. for each class


metric_logging:
  logging_path: './mlruns'          # Directory to store metrics. Dir must be named 'mlruns'
  experiment_base_name: 'ru_rf' # Name of the experiment under which to store metrics
  rewrite_experiment: False         # Whether to delete/overwrite runs under experiment_name. In most of the cases it should be False
