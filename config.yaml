run:
  supress_warnings: True
  n_jobs: -1

dataset:
  languages: ['it']   # 'en, 'it', 'fr', 'po', 'ru', 'ge'
  subtask: 2                        # 1, 2, 3
  data_dir: 'data'                  # Relative path to directory with data

preprocessing:
  split: 'train'                    # Split of the data to use
  analysis_unit: ['raw_text']              # 'title', 'title_and_first_paragraph', 'title_and_5_sentences', 'title_and_10_sentences',
                                    # 'title_and_first_sentence_each_paragraph', 'raw_text', 'all'
  load_preproc_input_data: True     # Whether or not to rerun the script that produces the units of analysis
  spacy_model_size: 'small'         # small or large. small should be enough for preprocessing, it is mostly tokenizing so far
  use_tfidf: False                   # True := TFIDFVectorizer, False := CountVectorizer
  min_df: 0.05                      # [0, 1.0] minimum document frequency for a term to be included
  max_df: 0.95                      # [0, 1.0] maximum document frequency for a term to be included
  max_features: 10000               # Maximum number of features
  ngram_range: [1,1]                # ngrams to use around a word. 1 is the word itself
  min_var: 0.001                    # Minimum variance a vector should have to be used in the trainset
  corr_threshold: 0.9               # Remove columns that have correlation >= threshold to another higher

  tune_preprocessing_params: False  # If True, will do LHS over the ranges of the parameters next
  param_search:
    n_samples: 4
    min_df_range: [0, 0.3]
    max_df_range: [0.6, 1]
    max_features_range: [1000, 10000]
    min_var_range: [0, 0.001]

training:
  default_params: False             # If True, it runs cross validation with outer_folds folds. If False, it runs nested cross validation
  # Regressions
  #model_list: ['LogisticRegression', 'LogisticRegression_ROS_v1', 'LogisticRegression_ROS_v2', 'LogisticRegression_SMOTE', 'LogisticRegression_BorderlineSMOTE', 'LogisticRegressionRidgeV1', 'LogisticRegressionRidgeV2', 'LogisticRegressionRidge_ROS_v1', 'LogisticRegressionRidge_ROS_v2', 'LogisticRegressionRidge_SMOTE_v1', 'LogisticRegressionRidge_SMOTE_v2', 'LogisticRegressionRidge_BorderlineSMOTE_v1', 'LogisticRegressionRidge_BorderlineSMOTE_v2', 'LogisticRegressionRidgeDual', 'LogisticRegressionRidgeDual_ROS_v1', 'LogisticRegressionRidgeDual_SMOTE_v1', 'LogisticRegressionRidgeDual_SMOTE_v2', 'LogisticRegressionRidgeDual_BorderlineSMOTE_v1', 'LogisticRegressionRidgeDual_BorderlineSMOTE_v2', 'LogisticRegressionLassoV1', 'LogisticRegressionLassoV2', 'LogisticRegressionLasso_SMOTE_v1', 'LogisticRegressionLasso_SMOTE_v2', 'LogisticRegressionLasso_BorderlineSMOTE_v1', 'LogisticRegressionLasso_BorderlineSMOTE_v2',  'LogisticRegressionElasticNetV1', 'LogisticRegressionElasticNetV2', 'LogisticRegressionElasticNetV3', 'LogisticRegressionElasticNet_ROS_v1', 'LogisticRegressionElasticNet_ROS_v2', 'LogisticRegressionElasticNet_SMOTE_v1', 'LogisticRegressionElasticNet_SMOTE_v2', 'LogisticRegressionElasticNet_BorderlineSMOTE_v1', 'LogisticRegressionElasticNet_BorderlineSMOTE_v2', 'RidgeClassifierV1', 'RidgeClassifierV2', 'RidgeClassifier_ROS_v1', 'RidgeClassifier_ROS_v2', 'RidgeClassifier_SMOTE_v1', 'RidgeClassifier_SMOTE_v2', 'RidgeClassifier_BorderlineSMOTE_v1', 'RidgeClassifier_BorderlineSMOTE_v2']
  #model_list: ['LogisticRegressionLasso_ROS_v1', 'LogisticRegressionLasso_ROS_v2', 'LogisticRegressionRidgeDual_ROS_v1', 'LogisticRegressionRidgeDual_ROS_v2'] # regressions that probably take waay too long
  #model_list: ['LogisticRegression_SVMSMOTE', 'LogisticRegressionRidge_SVMSMOTE_v1', 'LogisticRegressionRidge_SVMSMOTE_v2', 'LogisticRegressionRidgeDual_SVMSMOTE_v1', 'LogisticRegressionRidgeDual_SVMSMOTE_v2', 'LogisticRegressionLasso_SVMSMOTE_v1', 'LogisticRegressionElasticNet_SVMSMOTE_v1', 'LogisticRegressionElasticNet_SVMSMOTE_v2', 'RidgeClassifier_SVMSMOTE_v1', 'RidgeClassifier_SVMSMOTE_v2']   # regressions that will probably fail

  # SVM
  #model_list: ['SVM_rbf_V1', 'SVM_rbf_V2', 'SVM_rbf_V3', 'SVM_rbf_V4', 'SVM_rbf_ROS_v1', 'SVM_rbf_ROS_v2', 'SVM_rbf_SMOTE_v1', 'SVM_rbf_SMOTE_v2', 'SVM_rbf_BorderlineSMOTE_v1', 'SVM_rbf_BorderlineSMOTE_v2',  'SVM_sigmoid_narrow_gamma', 'SVM_sigmoid_broader_gamma', 'SVM_sigmoid_V3', 'SVM_sigmoid_V4', 'SVM_sigmoid_ROS_v1', 'SVM_sigmoid_ROS_v2', 'SVM_sigmoid_SMOTE_v1', 'SVM_sigmoid_SMOTE_v2', 'SVM_sigmoid_BorderlineSMOTE_v1', 'SVM_sigmoid_BorderlineSMOTE_v2', 'LinearSVM_V1', 'LinearSVM_V2', 'LinearSVM_V3', 'LinearSVM_V4', 'LinearSVMDual', 'LinearSVMDual_ROS_v1', 'LinearSVMDual_ROS_v2', 'LinearSVMDual_SMOTE_v1', 'LinearSVMDual_SMOTE_v2', 'LinearSVMDual_BorderlineSMOTE_v1', 'LinearSVMDual_BorderlineSMOTE_v2']
  #model_list: ['SVM_rbf_SVMSMOTE_v1', 'SVM_rbf_SVMSMOTE_v2', 'SVM_sigmoid_SVMSMOTE_v1', 'SVM_sigmoid_SVMSMOTE_v2', 'LinearSVMDual_SVMSMOTE_v1', 'LinearSVMDual_SVMSMOTE_v2'] # models that will probably fail

  # Random Forest
  #model_list: ['RandomForestSK_V1', 'RandomForestSK_V2', 'RandomForestSK_V3', 'RandomForest_ROS_v0', 'RandomForest_ROS_v1', 'RandomForest_ROS_v2', 'RandomForest_SMOTE_v0', 'RandomForest_SMOTE_v1', 'RandomForest_SMOTE_v2', 'RandomForest_BorderlineSMOTE_v0', 'RandomForest_BorderlineSMOTE_v1', 'RandomForest_BorderlineSMOTE_v2']
  #model_list: ['RandomForest_SVMSMOTE_v1', 'RandomForest_SVMSMOTE_v2'] # models that will probably fail

  # Xgboost
  model_list: ['XGBoost', 'XGBoostV2']

  # Naive Bayes
  #model_list: ['NaiveBayes', 'NaiveBayes_ROS_v1', 'NaiveBayes_ROS_v2', 'NaiveBayes_SMOTE_v1', 'NaiveBayes_SMOTE_v2', 'NaiveBayes_BorderlineSMOTE_v1', 'NaiveBayes_BorderlineSMOTE_v2', 'ComplementNaiveBayes', 'ComplementNaiveBayes_ROS_v1', 'ComplementNaiveBayes_SMOTE_v1', 'ComplementNaiveBayes_BorderlineSMOTE_v1', 'ComplementNaiveBayes_BorderlineSMOTE_v2']
  #model_list: ['NaiveBayes_SVMSMOTE_v1', 'NaiveBayes_SVMSMOTE_v2', 'ComplementNaiveBayes_SVMSMOTE_v1', 'ComplementNaiveBayes_SVMSMOTE_v2'] # models that will probably fail

  model_hyperparam_module: 'training.hyperparam_space_config_russian'           # .py file with models and distribution of hyperparams to tune and benchmark
  nested_cv:                        # Total training runs are outer_folds(inner_folds * num_search_iters + 1)
    outer_folds: 3                  # Number of outer fold: Number of "best" models after tuning
    inner_folds: 3                  # Number of inner folds: Number of folds to validate each hyperparam combination
    n_search_iter: 5               # Number of search iterations per outer fold (it is measured with cv of #inner_folds)
    ranking_score: 'f1_micro'       # Metric to choose best performing model in each inner fold
  metric_to_report: 'f1_micro'      # Metric to report
  return_train_metrics: True        # Whether to return the metrics on the train set or not
  mlb_cls_independent: True         # If True, and 'independent' multilabel model is used. If False, a 'chain' mlb model is used. 'independent' learns probabilities indep. for each class


metric_logging:
  logging_path: './mlruns'          # Directory to store metrics. Dir must be named 'mlruns'
  experiment_base_name: 'test_xgboost_run3' # Name of the experiment under which to store metrics
  rewrite_experiment: False         # Whether to delete/overwrite runs under experiment_name. In most of the cases it should be False
  logging_level: 'inner_cv'         # Which performance data to log ('model_wide', 'outer_cv', or 'inner_cv')
