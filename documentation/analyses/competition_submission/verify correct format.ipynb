{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(\"task2_scorer\")\n",
    "LANGUAGES = ('en', 'it', 'fr', 'po', 'ru', 'ge')\n",
    "LABELS = ('Economic', 'Capacity_and_resources', 'Morality', 'Fairness_and_equality',\n",
    "          'Legality_Constitutionality_and_jurisprudence', 'Policy_prescription_and_evaluation', 'Crime_and_punishment',\n",
    "          'Security_and_defense', 'Health_and_safety', 'Quality_of_life', 'Cultural_identity', 'Public_opinion',\n",
    "          'Political', 'External_regulation_and_reputation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "submission_dir = os.path.join('..', '..', '..', 'predictions', 'evaluations_to_report')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "output_dir = './correct_format_submissions'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write predictions in expected format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "fr_ComplementNaiveBayes_ROS_y_test.csv\n",
      "en\n",
      "en_ComplementNaiveBayes_ROS_y_test.csv\n",
      "ru\n",
      "ru_ComplementNaiveBayes_SMOTE_y_test.csv\n",
      "it\n",
      "it_ComplementNaiveBayes_ROS_y_test.csv\n",
      "po\n",
      "po_LinearSVMDual_y_test.csv\n",
      "ge\n",
      "ge_ComplementNaiveBayes_SMOTE_y_test.csv\n"
     ]
    }
   ],
   "source": [
    "for prediction_filepath in os.listdir(submission_dir):\n",
    "    language = prediction_filepath.split('_')[0]\n",
    "    eval_df = pd.read_csv(os.path.join(submission_dir, prediction_filepath), index_col='id')\n",
    "    submission_df = eval_df.apply(lambda row: ','.join(list(row[row==1].index)), axis=1)\n",
    "    submission_df.to_csv(os.path.join(output_dir, f'{language}_submission.txt'), sep='\\t', header=False)\n",
    "    print(language)\n",
    "    print(prediction_filepath)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verify their format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code from the `scorer-subtask-2.py` script"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def read_frame_list_from_file(file_full_name):\n",
    "  \"\"\"\n",
    "  Read the list of frame names from a file, one per line.\n",
    "  \"\"\"\n",
    "  with open(file_full_name, encoding='utf-8') as f:\n",
    "    return [ line.rstrip() for line in f.readlines() ]\n",
    "\n",
    "def _read_csv_input_file(file_full_name):\n",
    "  \"\"\"\n",
    "  Read a csv file with two columns TAB separated:\n",
    "   - first column is the id of the example\n",
    "   - second column is the comma-separated list of labels of the example\n",
    "  \"\"\"\n",
    "  a = {}\n",
    "  with open(file_full_name, encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "      ind = line.find(\"\\t\")\n",
    "      if ind < 0:\n",
    "        logger.error('ERROR: the file is supposed to be TAB separated, no TAB found on line' + line)\n",
    "        sys.exit(1)\n",
    "      if ind==len(line)-2: # line ends in \\t\\n\n",
    "        a[line[0:ind]] = []\n",
    "      else:\n",
    "        a[line[0:ind]] = line[ind+1:].rstrip().split(\",\")\n",
    "  return a\n",
    "\n",
    "def _labels_correct(labels, CLASSES, debug=False):\n",
    "  \"\"\"\n",
    "  Make sure all the labels correspond to strings in the CLASSES array\n",
    "  :param labels: a dictionary with strings as values\n",
    "  :param CLASSES: a list of allowed labels\n",
    "  \"\"\"\n",
    "  if debug:\n",
    "    s=\"\"\n",
    "    for articleid in labels.keys():\n",
    "      s += \",\".join([ l for l in labels[articleid] if l not in CLASSES ])\n",
    "    return s\n",
    "  else:\n",
    "    for articleid in labels.keys():\n",
    "      for l in labels[articleid]:\n",
    "        if l not in CLASSES:\n",
    "          return False\n",
    "  return True\n",
    "\n",
    "def _correct_number_of_examples(pred_labels, gold_labels):\n",
    "  \"\"\"\n",
    "  Make sure that the number of predictions is exactly the same as the gold labels\n",
    "  \"\"\"\n",
    "  return len(pred_labels.keys())==len(gold_labels.keys())\n",
    "\n",
    "\n",
    "def _correct_id_list(pred_labels, gold_labels, debug=False):\n",
    "  \"\"\"\n",
    "  Check that the list of keys of pred_labels is the same as the gold file\n",
    "  \"\"\"\n",
    "  if debug:\n",
    "    return \", \".join(set(pred_labels.keys()).symmetric_difference(set(gold_labels.keys())))\n",
    "  return len(set(pred_labels.keys()).symmetric_difference(set(gold_labels.keys())))==0\n",
    "\n",
    "def correct_format(pred_labels, gold_labels, CLASSES):\n",
    "  \"\"\"\n",
    "  Check whether the format of the prediction file is correct.\n",
    "  The number of checks that can be performed depends on the availability of the gold labels\n",
    "  \"\"\"\n",
    "  if not _labels_correct(pred_labels, CLASSES):\n",
    "    logger.error('The following labels in the prediction file are not valid: {}.'\n",
    "                 .format(_labels_correct(pred_labels, CLASSES, True)))\n",
    "    return False\n",
    "  if gold_labels: # we can do further checks if the gold_labels are available\n",
    "    if not _correct_number_of_examples(pred_labels, gold_labels):\n",
    "      logger.error('The number of predictions (%d) is not the expected one (%d)'\n",
    "                   %(len(pred_labels.keys()), len(gold_labels.keys())))\n",
    "      return False\n",
    "    if not _correct_id_list(pred_labels, gold_labels):\n",
    "      logger.error('The list of articles ids is not correct. The following ids are not in the gold file: %s'\n",
    "                   %(_correct_id_list(pred_labels, gold_labels, True)))\n",
    "      return False\n",
    "  return True\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "frame_file_path = os.path.join('..', '..', '..', 'data', 'scorers' , 'frames_subtask2.txt')\n",
    "CLASSES = read_frame_list_from_file(frame_file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file for language: fr is correct\n",
      "Submission file for language: it is correct\n",
      "Submission file for language: ge is correct\n",
      "Submission file for language: po is correct\n",
      "Submission file for language: en is correct\n",
      "Submission file for language: ru is correct\n"
     ]
    }
   ],
   "source": [
    "for submission_file in os.listdir('correct_format_submissions'):\n",
    "  language = submission_file.split('_')[0]\n",
    "\n",
    "  pred_labels = _read_csv_input_file(f'correct_format_submissions/{submission_file}')\n",
    "  gold_labels = os.listdir(os.path.join('..', '..', '..', 'data', 'data', language, 'test-articles-subtask-2'))\n",
    "  gold_labels = {article_name.split('article')[1].split('.txt')[0]: article_name for article_name in gold_labels}\n",
    "\n",
    "  if correct_format(pred_labels, gold_labels, CLASSES):\n",
    "    print(f'Submission file for language: {language} is correct')\n",
    "\n",
    "  else:\n",
    "    print(f'ERROR: Submission file for language: {language} is flawed')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
