{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac7fd1c8-16ff-43f2-a049-a27d37ea2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3de210-17ae-42af-a361-fff5a0f66735",
   "metadata": {},
   "source": [
    "# Group types of models (experiment type and model type) and pick best performing in terms of f1-score per unit of analysis and report them in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd704f0c-6f9a-47d2-b05b-150230e363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filepaths_mean_pred = glob.glob('./logged_performance_per_model/*/*raw_mean*.csv')\n",
    "results_filepaths_majority_pred = glob.glob('./logged_performance_per_model/*/*raw_majority*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def conantenate_results(filepath_list):\n",
    "    dfs_list = []\n",
    "    for results_filepath in filepath_list:\n",
    "        model_name = results_filepath.split('/')[-2]\n",
    "        results_df_i = pd.read_csv(results_filepath)\n",
    "        results_df_i['model_name'] = model_name\n",
    "        dfs_list.append(results_df_i)\n",
    "\n",
    "    results_df_ = pd.concat(dfs_list).set_index(['language', 'model_name', 'unit_of_analysis']).sort_index()\n",
    "    results_df_.rename(columns={'f1-mico_mean': 'f1-micro_mean', 'f1-mico_std': 'f1-micro_std'}, inplace=True)\n",
    "\n",
    "    return results_df_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "results_mean_pred_df = conantenate_results(results_filepaths_mean_pred)\n",
    "results_majority_vote_pred_df = conantenate_results(results_filepaths_majority_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6a452c68-3d2e-43a1-b820-a7688df2cd33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the tables to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c97b164-c359-40ea-b974-a0dc65c1db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_table(df, metric, index_cols=['model_name'], display_=True):\n",
    "    report_table = df.reset_index().copy()\n",
    "    report_table['result'] = report_table[f'{metric}'].map(lambda x: f'{x:.3f}')\n",
    "    report_table['col_title'] = report_table.unit_of_analysis.str.split('_').str.join(' ') \n",
    "    report_table['col_title'] = pd.Categorical(\n",
    "        report_table.col_title,\n",
    "        categories=['title', 'title and first paragraph', 'title and 5 sentences', 'title and 10 sentences',\n",
    "                    'title and first sentence each paragraph', 'raw text'],\n",
    "        ordered=True)\n",
    "    report_table = report_table[index_cols + ['col_title', 'result']]\\\n",
    "        .pivot_table(index=index_cols, columns=['col_title'], values=['result'], aggfunc='first', fill_value=0)\\\n",
    "        .droplevel(0, axis=1)\n",
    "\n",
    "    report_table.columns.names = [None]\n",
    "\n",
    "    # Highlight best scoring models according to their average\n",
    "    mean_perf_arr = report_table.applymap(lambda x: float(str(x).split(' ')[0])).to_numpy()\n",
    "    highlight_mask = mean_perf_arr == mean_perf_arr.max()\n",
    "    report_table_arr = report_table.to_numpy()  # Note it passes the array by reference\n",
    "    report_table_arr[highlight_mask] = '**' + report_table_arr[highlight_mask] + '**'\n",
    "\n",
    "    if display_:\n",
    "        display(Markdown(report_table.to_markdown()))\n",
    "    \n",
    "    return report_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14e8da-1acf-4114-9cfd-eeaa97261f68",
   "metadata": {},
   "source": [
    "### Generate tables for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bea51df3-87ed-4ace-ad56-25d235b5cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_report = ['f1_micro', 'recall_micro', 'precision_micro', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1650fa17-5ccc-439a-9818-d61bf369ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'en': 'English', 'it': 'Italian', 'fr': 'French', 'po': 'Polish', 'ru': 'Russian', 'ge': 'German'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "143ecba4-ffb9-45e9-869c-87d16caea017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_metrics_and_write_to_file(df, grouping_criterion, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    report_tables_dfs_dict = {metric: [] for metric in metrics_to_report}\n",
    "\n",
    "    for language, results_df in df.groupby(level=0):\n",
    "        display(Markdown(f'# {language_dict[language]}'))\n",
    "        \n",
    "        for metric in metrics_to_report:\n",
    "            os.makedirs(os.path.join(output_dir, metric), exist_ok=True)\n",
    "\n",
    "            output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "            output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "            output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "            os.makedirs(output_dir_markdown, exist_ok=True)\n",
    "            os.makedirs(output_dir_latex, exist_ok=True)\n",
    "            os.makedirs(output_dir_csv, exist_ok=True)\n",
    "\n",
    "            display(Markdown(f'## {metric}'))\n",
    "\n",
    "            report_table = display_performance_table(df=results_df, index_cols=grouping_criterion, metric=metric, display_=True)\n",
    "\n",
    "            # Export as markdown\n",
    "            markdown_file = open(os.path.join(output_dir_markdown, f\"{language_dict[language]}_{metric}.md\"), \"w\")\n",
    "            report_table.reset_index().to_markdown(markdown_file, index=False)\n",
    "            markdown_file.close()\n",
    "\n",
    "            # Export as latex table\n",
    "            latex_file = open(os.path.join(output_dir_latex, f\"{language_dict[language]}_{metric}.tex\"), \"w\")\n",
    "            report_table.reset_index().to_latex(latex_file, index=False)\n",
    "            latex_file.close()\n",
    "\n",
    "            # Export as csv\n",
    "            report_table.to_csv(os.path.join(output_dir_csv, f\"{language_dict[language]}_{metric}.csv\"))\n",
    "\n",
    "            # Stack all languages into single table\n",
    "            report_table['language'] = language\n",
    "            report_table = report_table.reset_index().set_index(['language'] + grouping_criterion)\n",
    "\n",
    "            report_tables_dfs_dict[metric].append(report_table)\n",
    "\n",
    "    # Report or store unified table\n",
    "    display(Markdown(f'# All 6 Languages'))\n",
    "    for metric in metrics_to_report:\n",
    "        display(Markdown(f'## {metric}'))\n",
    "        multi_language_report_table_metric = pd.concat(report_tables_dfs_dict[metric])\n",
    "        display(Markdown(multi_language_report_table_metric.reset_index().to_markdown(index=False)))\n",
    "\n",
    "        output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "        output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "        output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "        # Export as markdown\n",
    "        markdown_file = open(os.path.join(output_dir_markdown, f\"all_6_languages_{metric}.md\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_markdown(markdown_file, index=False)\n",
    "        markdown_file.close()\n",
    "\n",
    "        # Export as latex table\n",
    "        latex_file = open(os.path.join(output_dir_latex, f\"all_6_languages_{metric}.tex\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n",
    "        latex_file.close()\n",
    "\n",
    "        # Export as csv\n",
    "        multi_language_report_table_metric.to_csv(os.path.join(output_dir_csv, f\"all_6_languages_{metric}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d754-49ba-4c74-8220-2e144f624044",
   "metadata": {},
   "source": [
    "# Per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5abb824-f93a-44fd-8203-a0b84224b0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# English"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.64  |                       0.7   |                   0.684 | 0.699                    | 0.704                                     |      0.691 |\n| EleutherAI-gpt-neo-1.3B                    |   0.63  |                       0.703 |                   0.707 | 0.693                    | 0.695                                     |      0.698 |\n| EleutherAI-gpt-neo-125M                    |   0.53  |                       0.623 |                   0.637 | 0.634                    | 0.663                                     |      0.671 |\n| bert-base-multilingual-cased               |   0.61  |                       0.677 |                   0.7   | 0.707                    | 0.684                                     |      0.693 |\n| distilbert-base-multilingual-cased         |   0.596 |                       0.665 |                   0.661 | 0.672                    | 0.672                                     |      0.69  |\n| facebook-mbart-large-50                    |   0.677 |                       0.717 |                   0.719 | **0.721**                | **0.721**                                 |      0.707 |\n| gpt2                                       |   0.618 |                       0.693 |                   0.69  | 0.673                    | 0.692                                     |      0.696 |\n| xlm-roberta-large                          |   0.66  |                       0.694 |                   0.718 | 0.715                    | 0.717                                     |      0.704 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.577 |                       0.638 |                   0.611 |                    0.643 | **0.667**                                 | 0.653      |\n| EleutherAI-gpt-neo-1.3B                    |   0.538 |                       0.631 |                   0.619 |                    0.604 | 0.619                                     | 0.633      |\n| EleutherAI-gpt-neo-125M                    |   0.45  |                       0.543 |                   0.562 |                    0.548 | 0.582                                     | 0.599      |\n| bert-base-multilingual-cased               |   0.543 |                       0.606 |                   0.631 |                    0.65  | 0.628                                     | 0.655      |\n| distilbert-base-multilingual-cased         |   0.528 |                       0.592 |                   0.579 |                    0.619 | 0.609                                     | 0.655      |\n| facebook-mbart-large-50                    |   0.599 |                       0.653 |                   0.645 |                    0.653 | **0.667**                                 | **0.667**  |\n| gpt2                                       |   0.548 |                       0.643 |                   0.658 |                    0.638 | 0.653                                     | 0.653      |\n| xlm-roberta-large                          |   0.584 |                       0.636 |                   0.636 |                    0.643 | 0.663                                     | 0.653      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.717 |                       0.774 | 0.776                   |                    0.765 |                                     0.744 |      0.734 |\n| EleutherAI-gpt-neo-1.3B                    |   0.761 |                       0.794 | 0.824                   |                    0.812 |                                     0.793 |      0.778 |\n| EleutherAI-gpt-neo-125M                    |   0.646 |                       0.73  | 0.735                   |                    0.752 |                                     0.77  |      0.763 |\n| bert-base-multilingual-cased               |   0.696 |                       0.765 | 0.787                   |                    0.776 |                                     0.751 |      0.736 |\n| distilbert-base-multilingual-cased         |   0.684 |                       0.759 | 0.769                   |                    0.735 |                                     0.75  |      0.728 |\n| facebook-mbart-large-50                    |   0.778 |                       0.795 | 0.812                   |                    0.804 |                                     0.784 |      0.752 |\n| gpt2                                       |   0.709 |                       0.751 | 0.725                   |                    0.711 |                                     0.736 |      0.746 |\n| xlm-roberta-large                          |   0.759 |                       0.765 | **0.825**               |                    0.804 |                                     0.781 |      0.765 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.087 |                       0.136 | 0.126                   |                    0.068 |                                     0.078 |      0.058 |\n| EleutherAI-gpt-neo-1.3B                    |   0.087 |                       0.087 | **0.155**               |                    0.097 |                                     0.117 |      0.087 |\n| EleutherAI-gpt-neo-125M                    |   0.019 |                       0.087 | 0.058                   |                    0.039 |                                     0.087 |      0.078 |\n| bert-base-multilingual-cased               |   0.078 |                       0.087 | 0.117                   |                    0.078 |                                     0.068 |      0.087 |\n| distilbert-base-multilingual-cased         |   0.097 |                       0.078 | 0.126                   |                    0.078 |                                     0.117 |      0.097 |\n| facebook-mbart-large-50                    |   0.117 |                       0.126 | 0.146                   |                    0.107 |                                     0.146 |      0.097 |\n| gpt2                                       |   0.097 |                       0.097 | 0.117                   |                    0.087 |                                     0.078 |      0.117 |\n| xlm-roberta-large                          |   0.058 |                       0.087 | 0.146                   |                    0.126 |                                     0.117 |      0.117 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# French"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.4   |                       0.46  |                   0.486 |                    0.449 |                                     0.52  | 0.502      |\n| EleutherAI-gpt-neo-1.3B                    |   0.379 |                       0.448 |                   0.441 |                    0.394 |                                     0.459 | 0.493      |\n| EleutherAI-gpt-neo-125M                    |   0.227 |                       0.359 |                   0.358 |                    0.352 |                                     0.413 | 0.462      |\n| bert-base-multilingual-cased               |   0.419 |                       0.412 |                   0.441 |                    0.494 |                                     0.513 | 0.555      |\n| distilbert-base-multilingual-cased         |   0.371 |                       0.461 |                   0.46  |                    0.508 |                                     0.54  | 0.546      |\n| facebook-mbart-large-50                    |   0.453 |                       0.475 |                   0.53  |                    0.504 |                                     0.541 | **0.568**  |\n| gpt2                                       |   0.323 |                       0.386 |                   0.4   |                    0.449 |                                     0.415 | 0.491      |\n| xlm-roberta-large                          |   0.434 |                       0.48  |                   0.502 |                    0.5   |                                     0.529 | 0.545      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.357 |                       0.413 |                   0.429 |                    0.405 |                                     0.468 | 0.460      |\n| EleutherAI-gpt-neo-1.3B                    |   0.294 |                       0.357 |                   0.373 |                    0.31  |                                     0.381 | 0.429      |\n| EleutherAI-gpt-neo-125M                    |   0.175 |                       0.294 |                   0.286 |                    0.294 |                                     0.341 | 0.389      |\n| bert-base-multilingual-cased               |   0.349 |                       0.333 |                   0.397 |                    0.46  |                                     0.484 | 0.540      |\n| distilbert-base-multilingual-cased         |   0.302 |                       0.421 |                   0.413 |                    0.476 |                                     0.484 | **0.563**  |\n| facebook-mbart-large-50                    |   0.381 |                       0.413 |                   0.452 |                    0.452 |                                     0.5   | 0.532      |\n| gpt2                                       |   0.286 |                       0.341 |                   0.341 |                    0.405 |                                     0.349 | 0.444      |\n| xlm-roberta-large                          |   0.381 |                       0.429 |                   0.444 |                    0.46  |                                     0.476 | 0.532      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.455 |                       0.52  | 0.562                   |                    0.505 |                                     0.584 |      0.552 |\n| EleutherAI-gpt-neo-1.3B                    |   0.536 |                       0.6   | 0.540                   |                    0.542 |                                     0.578 |      0.581 |\n| EleutherAI-gpt-neo-125M                    |   0.324 |                       0.463 | 0.480                   |                    0.44  |                                     0.524 |      0.57  |\n| bert-base-multilingual-cased               |   0.524 |                       0.538 | 0.495                   |                    0.532 |                                     0.545 |      0.571 |\n| distilbert-base-multilingual-cased         |   0.481 |                       0.51  | 0.520                   |                    0.545 |                                     0.61  |      0.53  |\n| facebook-mbart-large-50                    |   0.558 |                       0.559 | **0.640**               |                    0.57  |                                     0.589 |      0.609 |\n| gpt2                                       |   0.371 |                       0.443 | 0.483                   |                    0.505 |                                     0.512 |      0.549 |\n| xlm-roberta-large                          |   0.505 |                       0.545 | 0.577                   |                    0.547 |                                     0.594 |      0.558 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.071 | 0.071                       |                   0.071 |                    0.071 | **0.119**                                 |      0.071 |\n| EleutherAI-gpt-neo-1.3B                    |   0.024 | **0.119**                   |                   0.024 |                    0.071 | 0.024                                     |      0.071 |\n| EleutherAI-gpt-neo-125M                    |   0.024 | 0.024                       |                   0.024 |                    0.024 | 0.024                                     |      0.071 |\n| bert-base-multilingual-cased               |   0.048 | 0.071                       |                   0.048 |                    0.071 | 0.071                                     |      0.095 |\n| distilbert-base-multilingual-cased         |   0.071 | 0.071                       |                   0.095 |                    0.048 | 0.095                                     |      0.048 |\n| facebook-mbart-large-50                    |   0.024 | 0.095                       |                   0.071 |                    0.048 | 0.071                                     |      0.071 |\n| gpt2                                       |   0     | 0.024                       |                   0.048 |                    0     | 0.048                                     |      0.048 |\n| xlm-roberta-large                          |   0     | **0.119**                   |                   0.071 |                    0.095 | 0.095                                     |      0.071 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# German"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.559 |                       0.592 |                   0.567 |                    0.603 |                                     0.601 | 0.646      |\n| EleutherAI-gpt-neo-1.3B                    |   0.466 |                       0.572 |                   0.585 |                    0.593 |                                     0.603 | 0.647      |\n| EleutherAI-gpt-neo-125M                    |   0.409 |                       0.48  |                   0.509 |                    0.496 |                                     0.531 | 0.605      |\n| bert-base-multilingual-cased               |   0.5   |                       0.568 |                   0.591 |                    0.616 |                                     0.598 | 0.659      |\n| distilbert-base-multilingual-cased         |   0.516 |                       0.571 |                   0.542 |                    0.583 |                                     0.589 | 0.649      |\n| facebook-mbart-large-50                    |   0.585 |                       0.601 |                   0.566 |                    0.628 |                                     0.619 | 0.646      |\n| gpt2                                       |   0.475 |                       0.525 |                   0.485 |                    0.581 |                                     0.526 | 0.616      |\n| xlm-roberta-large                          |   0.565 |                       0.63  |                   0.587 |                    0.638 |                                     0.652 | **0.660**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.465 |                       0.535 |                   0.483 |                    0.494 |                                     0.529 | 0.599      |\n| EleutherAI-gpt-neo-1.3B                    |   0.355 |                       0.483 |                   0.5   |                    0.471 |                                     0.512 | 0.587      |\n| EleutherAI-gpt-neo-125M                    |   0.331 |                       0.384 |                   0.407 |                    0.401 |                                     0.442 | 0.535      |\n| bert-base-multilingual-cased               |   0.424 |                       0.488 |                   0.512 |                    0.547 |                                     0.57  | **0.645**  |\n| distilbert-base-multilingual-cased         |   0.424 |                       0.517 |                   0.471 |                    0.5   |                                     0.517 | **0.645**  |\n| facebook-mbart-large-50                    |   0.483 |                       0.547 |                   0.488 |                    0.535 |                                     0.552 | 0.599      |\n| gpt2                                       |   0.413 |                       0.453 |                   0.413 |                    0.523 |                                     0.442 | 0.587      |\n| xlm-roberta-large                          |   0.465 |                       0.57  |                   0.5   |                    0.564 |                                     0.593 | 0.610      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.702 |                       0.662 |                   0.686 | 0.773                    |                                     0.695 |      0.701 |\n| EleutherAI-gpt-neo-1.3B                    |   0.678 |                       0.703 |                   0.705 | **0.802**                |                                     0.733 |      0.721 |\n| EleutherAI-gpt-neo-125M                    |   0.533 |                       0.641 |                   0.68  | 0.651                    |                                     0.667 |      0.697 |\n| bert-base-multilingual-cased               |   0.608 |                       0.677 |                   0.698 | 0.707                    |                                     0.628 |      0.673 |\n| distilbert-base-multilingual-cased         |   0.658 |                       0.636 |                   0.638 | 0.699                    |                                     0.685 |      0.653 |\n| facebook-mbart-large-50                    |   0.741 |                       0.667 |                   0.672 | 0.760                    |                                     0.704 |      0.701 |\n| gpt2                                       |   0.559 |                       0.624 |                   0.587 | 0.652                    |                                     0.65  |      0.647 |\n| xlm-roberta-large                          |   0.721 |                       0.705 |                   0.711 | 0.735                    |                                     0.723 |      0.719 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.029 |                       0.029 |                   0.029 | 0.057                    |                                     0.057 |      0.029 |\n| EleutherAI-gpt-neo-1.3B                    |   0     |                       0.029 |                   0     | 0.057                    |                                     0.029 |      0.029 |\n| EleutherAI-gpt-neo-125M                    |   0     |                       0     |                   0.029 | 0.000                    |                                     0.029 |      0.029 |\n| bert-base-multilingual-cased               |   0     |                       0.029 |                   0.029 | 0.057                    |                                     0     |      0.029 |\n| distilbert-base-multilingual-cased         |   0.029 |                       0.029 |                   0     | 0.000                    |                                     0.029 |      0     |\n| facebook-mbart-large-50                    |   0.057 |                       0.029 |                   0.029 | 0.029                    |                                     0.057 |      0     |\n| gpt2                                       |   0     |                       0     |                   0.029 | **0.086**                |                                     0     |      0     |\n| xlm-roberta-large                          |   0.029 |                       0.057 |                   0.029 | 0.000                    |                                     0.057 |      0.029 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Italian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.57  |                       0.551 |                   0.552 |                    0.573 |                                     0.558 | 0.603      |\n| EleutherAI-gpt-neo-1.3B                    |   0.437 |                       0.538 |                   0.527 |                    0.537 |                                     0.547 | 0.607      |\n| EleutherAI-gpt-neo-125M                    |   0.3   |                       0.324 |                   0.442 |                    0.475 |                                     0.464 | 0.556      |\n| bert-base-multilingual-cased               |   0.479 |                       0.572 |                   0.566 |                    0.592 |                                     0.576 | 0.610      |\n| distilbert-base-multilingual-cased         |   0.476 |                       0.545 |                   0.564 |                    0.589 |                                     0.541 | 0.627      |\n| facebook-mbart-large-50                    |   0.533 |                       0.585 |                   0.59  |                    0.593 |                                     0.613 | **0.639**  |\n| gpt2                                       |   0.397 |                       0.485 |                   0.513 |                    0.543 |                                     0.477 | 0.535      |\n| xlm-roberta-large                          |   0.532 |                       0.587 |                   0.602 |                    0.598 |                                     0.587 | 0.633      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.478 |                       0.474 |                   0.474 |                    0.47  |                                     0.452 | 0.526      |\n| EleutherAI-gpt-neo-1.3B                    |   0.33  |                       0.426 |                   0.404 |                    0.426 |                                     0.443 | 0.526      |\n| EleutherAI-gpt-neo-125M                    |   0.243 |                       0.243 |                   0.348 |                    0.4   |                                     0.378 | 0.465      |\n| bert-base-multilingual-cased               |   0.413 |                       0.483 |                   0.491 |                    0.517 |                                     0.504 | 0.548      |\n| distilbert-base-multilingual-cased         |   0.409 |                       0.47  |                   0.487 |                    0.526 |                                     0.47  | **0.578**  |\n| facebook-mbart-large-50                    |   0.439 |                       0.487 |                   0.483 |                    0.478 |                                     0.526 | 0.565      |\n| gpt2                                       |   0.335 |                       0.417 |                   0.457 |                    0.491 |                                     0.387 | 0.496      |\n| xlm-roberta-large                          |   0.439 |                       0.491 |                   0.496 |                    0.491 |                                     0.491 | 0.543      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.705 |                       0.657 |                   0.661 | 0.735                    |                                     0.727 |      0.708 |\n| EleutherAI-gpt-neo-1.3B                    |   0.644 |                       0.731 |                   0.756 | 0.726                    |                                     0.713 |      0.716 |\n| EleutherAI-gpt-neo-125M                    |   0.392 |                       0.483 |                   0.606 | 0.586                    |                                     0.6   |      0.69  |\n| bert-base-multilingual-cased               |   0.569 |                       0.703 |                   0.669 | 0.692                    |                                     0.671 |      0.689 |\n| distilbert-base-multilingual-cased         |   0.57  |                       0.651 |                   0.671 | 0.669                    |                                     0.639 |      0.686 |\n| facebook-mbart-large-50                    |   0.678 |                       0.732 |                   0.76  | **0.780**                |                                     0.733 |      0.734 |\n| gpt2                                       |   0.487 |                       0.578 |                   0.587 | 0.608                    |                                     0.622 |      0.582 |\n| xlm-roberta-large                          |   0.673 |                       0.729 |                   0.765 | 0.764                    |                                     0.729 |      0.758 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   | title and 10 sentences   |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|:-------------------------|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.1   |                       0.15  | 0.167                   | 0.133                    |                                     0.1   | 0.117      |\n| EleutherAI-gpt-neo-1.3B                    |   0.05  |                       0.117 | **0.183**               | 0.150                    |                                     0.133 | 0.133      |\n| EleutherAI-gpt-neo-125M                    |   0     |                       0     | 0.033                   | 0.067                    |                                     0.033 | 0.133      |\n| bert-base-multilingual-cased               |   0.017 |                       0.117 | 0.067                   | **0.183**                |                                     0.117 | 0.133      |\n| distilbert-base-multilingual-cased         |   0.017 |                       0.117 | 0.117                   | 0.150                    |                                     0.083 | 0.133      |\n| facebook-mbart-large-50                    |   0.1   |                       0.167 | 0.150                   | 0.167                    |                                     0.167 | **0.183**  |\n| gpt2                                       |   0.033 |                       0.033 | 0.033                   | 0.067                    |                                     0.083 | 0.050      |\n| xlm-roberta-large                          |   0.1   |                       0.167 | **0.183**               | 0.133                    |                                     0.15  | **0.183**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Polish"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.507 |                       0.599 |                   0.617 |                    0.607 |                                     0.635 | 0.634      |\n| EleutherAI-gpt-neo-1.3B                    |   0.464 |                       0.515 |                   0.618 |                    0.586 |                                     0.618 | 0.623      |\n| EleutherAI-gpt-neo-125M                    |   0.453 |                       0.496 |                   0.516 |                    0.566 |                                     0.545 | 0.551      |\n| bert-base-multilingual-cased               |   0.571 |                       0.62  |                   0.652 |                    0.654 |                                     0.667 | 0.663      |\n| distilbert-base-multilingual-cased         |   0.558 |                       0.542 |                   0.592 |                    0.639 |                                     0.631 | 0.638      |\n| facebook-mbart-large-50                    |   0.565 |                       0.604 |                   0.676 |                    0.665 |                                     0.685 | 0.672      |\n| gpt2                                       |   0.517 |                       0.538 |                   0.591 |                    0.621 |                                     0.575 | 0.653      |\n| xlm-roberta-large                          |   0.569 |                       0.64  |                   0.634 |                    0.641 |                                     0.695 | **0.698**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.437 |                       0.549 |                   0.563 |                    0.51  |                                     0.544 | 0.573      |\n| EleutherAI-gpt-neo-1.3B                    |   0.393 |                       0.422 |                   0.539 |                    0.481 |                                     0.539 | 0.553      |\n| EleutherAI-gpt-neo-125M                    |   0.388 |                       0.427 |                   0.461 |                    0.519 |                                     0.471 | 0.471      |\n| bert-base-multilingual-cased               |   0.519 |                       0.563 |                   0.587 |                    0.597 |                                     0.597 | 0.636      |\n| distilbert-base-multilingual-cased         |   0.49  |                       0.5   |                   0.524 |                    0.558 |                                     0.568 | 0.621      |\n| facebook-mbart-large-50                    |   0.524 |                       0.544 |                   0.602 |                    0.573 |                                     0.607 | 0.617      |\n| gpt2                                       |   0.481 |                       0.481 |                   0.597 |                    0.568 |                                     0.51  | 0.617      |\n| xlm-roberta-large                          |   0.51  |                       0.587 |                   0.573 |                    0.563 |                                     0.626 | **0.650**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.604 |                       0.661 |                   0.682 | 0.750                    |                                     0.762 |      0.711 |\n| EleutherAI-gpt-neo-1.3B                    |   0.566 |                       0.659 |                   0.725 | 0.750                    |                                     0.725 |      0.713 |\n| EleutherAI-gpt-neo-125M                    |   0.544 |                       0.591 |                   0.586 | 0.622                    |                                     0.647 |      0.664 |\n| bert-base-multilingual-cased               |   0.633 |                       0.69  |                   0.733 | 0.724                    |                                     0.755 |      0.693 |\n| distilbert-base-multilingual-cased         |   0.647 |                       0.592 |                   0.679 | 0.747                    |                                     0.709 |      0.656 |\n| facebook-mbart-large-50                    |   0.614 |                       0.679 |                   0.77  | **0.792**                |                                     0.786 |      0.738 |\n| gpt2                                       |   0.559 |                       0.611 |                   0.586 | 0.684                    |                                     0.66  |      0.694 |\n| xlm-roberta-large                          |   0.644 |                       0.703 |                   0.711 | 0.744                    |                                     0.782 |      0.753 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.026 |                       0.026 |                   0.026 | 0.051                    |                                     0.051 | 0.051      |\n| EleutherAI-gpt-neo-1.3B                    |   0.026 |                       0     |                   0.051 | 0.051                    |                                     0.026 | 0.051      |\n| EleutherAI-gpt-neo-125M                    |   0     |                       0     |                   0.051 | 0.026                    |                                     0.026 | 0.026      |\n| bert-base-multilingual-cased               |   0.026 |                       0.051 |                   0.051 | 0.051                    |                                     0.051 | 0.051      |\n| distilbert-base-multilingual-cased         |   0.051 |                       0     |                   0.026 | 0.051                    |                                     0.051 | 0.026      |\n| facebook-mbart-large-50                    |   0.026 |                       0     |                   0.026 | **0.077**                |                                     0.026 | 0.051      |\n| gpt2                                       |   0     |                       0     |                   0     | 0.026                    |                                     0.026 | 0.051      |\n| xlm-roberta-large                          |   0.051 |                       0.026 |                   0.026 | 0.051                    |                                     0.026 | **0.077**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Russian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.419 |                       0.465 |                   0.474 |                    0.487 |                                     0.521 | **0.532**  |\n| EleutherAI-gpt-neo-1.3B                    |   0.319 |                       0.286 |                   0.377 |                    0.387 |                                     0.422 | 0.375      |\n| EleutherAI-gpt-neo-125M                    |   0.162 |                       0.22  |                   0.24  |                    0.217 |                                     0.137 | 0.150      |\n| bert-base-multilingual-cased               |   0.366 |                       0.436 |                   0.516 |                    0.471 |                                     0.468 | 0.478      |\n| distilbert-base-multilingual-cased         |   0.318 |                       0.407 |                   0.47  |                    0.5   |                                     0.468 | 0.503      |\n| facebook-mbart-large-50                    |   0.426 |                       0.435 |                   0.511 |                    0.49  |                                     0.526 | 0.519      |\n| gpt2                                       |   0.159 |                       0.107 |                   0.075 |                    0.095 |                                     0.143 | 0.217      |\n| xlm-roberta-large                          |   0.403 |                       0.446 |                   0.479 |                    0.455 |                                     0.507 | 0.443      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.36  |                       0.419 |                   0.419 |                    0.43  |                                     0.442 | 0.488      |\n| EleutherAI-gpt-neo-1.3B                    |   0.221 |                       0.198 |                   0.267 |                    0.279 |                                     0.314 | 0.279      |\n| EleutherAI-gpt-neo-125M                    |   0.105 |                       0.163 |                   0.174 |                    0.174 |                                     0.093 | 0.105      |\n| bert-base-multilingual-cased               |   0.302 |                       0.395 |                   0.465 |                    0.419 |                                     0.419 | 0.442      |\n| distilbert-base-multilingual-cased         |   0.244 |                       0.384 |                   0.407 |                    0.465 |                                     0.419 | **0.523**  |\n| facebook-mbart-large-50                    |   0.337 |                       0.349 |                   0.419 |                    0.407 |                                     0.419 | 0.465      |\n| gpt2                                       |   0.105 |                       0.07  |                   0.047 |                    0.058 |                                     0.093 | 0.163      |\n| xlm-roberta-large                          |   0.314 |                       0.36  |                   0.395 |                    0.384 |                                     0.442 | 0.384      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.5   |                       0.522 |                   0.545 |                    0.561 | 0.633                                     |      0.583 |\n| EleutherAI-gpt-neo-1.3B                    |   0.576 |                       0.515 |                   0.639 |                    0.632 | 0.643                                     |      0.571 |\n| EleutherAI-gpt-neo-125M                    |   0.36  |                       0.341 |                   0.385 |                    0.288 | 0.258                                     |      0.265 |\n| bert-base-multilingual-cased               |   0.464 |                       0.486 |                   0.58  |                    0.537 | 0.529                                     |      0.521 |\n| distilbert-base-multilingual-cased         |   0.457 |                       0.434 |                   0.556 |                    0.541 | 0.529                                     |      0.484 |\n| facebook-mbart-large-50                    |   0.58  |                       0.577 |                   0.655 |                    0.614 | **0.706**                                 |      0.588 |\n| gpt2                                       |   0.333 |                       0.231 |                   0.19  |                    0.263 | 0.308                                     |      0.326 |\n| xlm-roberta-large                          |   0.562 |                       0.585 |                   0.607 |                    0.559 | 0.594                                     |      0.524 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.158 |                       0.158 |                   0.132 |                    0.079 | **0.263**                                 |      0.237 |\n| EleutherAI-gpt-neo-1.3B                    |   0.053 |                       0.105 |                   0.079 |                    0.158 | 0.105                                     |      0.105 |\n| EleutherAI-gpt-neo-125M                    |   0.026 |                       0     |                   0.053 |                    0     | 0.026                                     |      0.026 |\n| bert-base-multilingual-cased               |   0.079 |                       0.105 |                   0.237 |                    0.158 | 0.184                                     |      0.237 |\n| distilbert-base-multilingual-cased         |   0.105 |                       0.132 |                   0.158 |                    0.158 | 0.211                                     |      0.158 |\n| facebook-mbart-large-50                    |   0.158 |                       0.211 |                   0.211 |                    0.184 | 0.211                                     |      0.184 |\n| gpt2                                       |   0     |                       0     |                   0     |                    0.026 | 0.053                                     |      0.026 |\n| xlm-roberta-large                          |   0.158 |                       0.184 |                   0.211 |                    0.105 | 0.211                                     |      0.158 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# All 6 Languages"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.64  |                       0.7   |                   0.684 | 0.699                    | 0.704                                     | 0.691      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.63  |                       0.703 |                   0.707 | 0.693                    | 0.695                                     | 0.698      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.53  |                       0.623 |                   0.637 | 0.634                    | 0.663                                     | 0.671      |\n| en         | bert-base-multilingual-cased               |   0.61  |                       0.677 |                   0.7   | 0.707                    | 0.684                                     | 0.693      |\n| en         | distilbert-base-multilingual-cased         |   0.596 |                       0.665 |                   0.661 | 0.672                    | 0.672                                     | 0.690      |\n| en         | facebook-mbart-large-50                    |   0.677 |                       0.717 |                   0.719 | **0.721**                | **0.721**                                 | 0.707      |\n| en         | gpt2                                       |   0.618 |                       0.693 |                   0.69  | 0.673                    | 0.692                                     | 0.696      |\n| en         | xlm-roberta-large                          |   0.66  |                       0.694 |                   0.718 | 0.715                    | 0.717                                     | 0.704      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.4   |                       0.46  |                   0.486 | 0.449                    | 0.520                                     | 0.502      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.379 |                       0.448 |                   0.441 | 0.394                    | 0.459                                     | 0.493      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.227 |                       0.359 |                   0.358 | 0.352                    | 0.413                                     | 0.462      |\n| fr         | bert-base-multilingual-cased               |   0.419 |                       0.412 |                   0.441 | 0.494                    | 0.513                                     | 0.555      |\n| fr         | distilbert-base-multilingual-cased         |   0.371 |                       0.461 |                   0.46  | 0.508                    | 0.540                                     | 0.546      |\n| fr         | facebook-mbart-large-50                    |   0.453 |                       0.475 |                   0.53  | 0.504                    | 0.541                                     | **0.568**  |\n| fr         | gpt2                                       |   0.323 |                       0.386 |                   0.4   | 0.449                    | 0.415                                     | 0.491      |\n| fr         | xlm-roberta-large                          |   0.434 |                       0.48  |                   0.502 | 0.500                    | 0.529                                     | 0.545      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.559 |                       0.592 |                   0.567 | 0.603                    | 0.601                                     | 0.646      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0.466 |                       0.572 |                   0.585 | 0.593                    | 0.603                                     | 0.647      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0.409 |                       0.48  |                   0.509 | 0.496                    | 0.531                                     | 0.605      |\n| ge         | bert-base-multilingual-cased               |   0.5   |                       0.568 |                   0.591 | 0.616                    | 0.598                                     | 0.659      |\n| ge         | distilbert-base-multilingual-cased         |   0.516 |                       0.571 |                   0.542 | 0.583                    | 0.589                                     | 0.649      |\n| ge         | facebook-mbart-large-50                    |   0.585 |                       0.601 |                   0.566 | 0.628                    | 0.619                                     | 0.646      |\n| ge         | gpt2                                       |   0.475 |                       0.525 |                   0.485 | 0.581                    | 0.526                                     | 0.616      |\n| ge         | xlm-roberta-large                          |   0.565 |                       0.63  |                   0.587 | 0.638                    | 0.652                                     | **0.660**  |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.57  |                       0.551 |                   0.552 | 0.573                    | 0.558                                     | 0.603      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.437 |                       0.538 |                   0.527 | 0.537                    | 0.547                                     | 0.607      |\n| it         | EleutherAI-gpt-neo-125M                    |   0.3   |                       0.324 |                   0.442 | 0.475                    | 0.464                                     | 0.556      |\n| it         | bert-base-multilingual-cased               |   0.479 |                       0.572 |                   0.566 | 0.592                    | 0.576                                     | 0.610      |\n| it         | distilbert-base-multilingual-cased         |   0.476 |                       0.545 |                   0.564 | 0.589                    | 0.541                                     | 0.627      |\n| it         | facebook-mbart-large-50                    |   0.533 |                       0.585 |                   0.59  | 0.593                    | 0.613                                     | **0.639**  |\n| it         | gpt2                                       |   0.397 |                       0.485 |                   0.513 | 0.543                    | 0.477                                     | 0.535      |\n| it         | xlm-roberta-large                          |   0.532 |                       0.587 |                   0.602 | 0.598                    | 0.587                                     | 0.633      |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.507 |                       0.599 |                   0.617 | 0.607                    | 0.635                                     | 0.634      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.464 |                       0.515 |                   0.618 | 0.586                    | 0.618                                     | 0.623      |\n| po         | EleutherAI-gpt-neo-125M                    |   0.453 |                       0.496 |                   0.516 | 0.566                    | 0.545                                     | 0.551      |\n| po         | bert-base-multilingual-cased               |   0.571 |                       0.62  |                   0.652 | 0.654                    | 0.667                                     | 0.663      |\n| po         | distilbert-base-multilingual-cased         |   0.558 |                       0.542 |                   0.592 | 0.639                    | 0.631                                     | 0.638      |\n| po         | facebook-mbart-large-50                    |   0.565 |                       0.604 |                   0.676 | 0.665                    | 0.685                                     | 0.672      |\n| po         | gpt2                                       |   0.517 |                       0.538 |                   0.591 | 0.621                    | 0.575                                     | 0.653      |\n| po         | xlm-roberta-large                          |   0.569 |                       0.64  |                   0.634 | 0.641                    | 0.695                                     | **0.698**  |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.419 |                       0.465 |                   0.474 | 0.487                    | 0.521                                     | **0.532**  |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.319 |                       0.286 |                   0.377 | 0.387                    | 0.422                                     | 0.375      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.162 |                       0.22  |                   0.24  | 0.217                    | 0.137                                     | 0.150      |\n| ru         | bert-base-multilingual-cased               |   0.366 |                       0.436 |                   0.516 | 0.471                    | 0.468                                     | 0.478      |\n| ru         | distilbert-base-multilingual-cased         |   0.318 |                       0.407 |                   0.47  | 0.500                    | 0.468                                     | 0.503      |\n| ru         | facebook-mbart-large-50                    |   0.426 |                       0.435 |                   0.511 | 0.490                    | 0.526                                     | 0.519      |\n| ru         | gpt2                                       |   0.159 |                       0.107 |                   0.075 | 0.095                    | 0.143                                     | 0.217      |\n| ru         | xlm-roberta-large                          |   0.403 |                       0.446 |                   0.479 | 0.455                    | 0.507                                     | 0.443      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.577 |                       0.638 |                   0.611 |                    0.643 | **0.667**                                 | 0.653      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.538 |                       0.631 |                   0.619 |                    0.604 | 0.619                                     | 0.633      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.45  |                       0.543 |                   0.562 |                    0.548 | 0.582                                     | 0.599      |\n| en         | bert-base-multilingual-cased               |   0.543 |                       0.606 |                   0.631 |                    0.65  | 0.628                                     | 0.655      |\n| en         | distilbert-base-multilingual-cased         |   0.528 |                       0.592 |                   0.579 |                    0.619 | 0.609                                     | 0.655      |\n| en         | facebook-mbart-large-50                    |   0.599 |                       0.653 |                   0.645 |                    0.653 | **0.667**                                 | **0.667**  |\n| en         | gpt2                                       |   0.548 |                       0.643 |                   0.658 |                    0.638 | 0.653                                     | 0.653      |\n| en         | xlm-roberta-large                          |   0.584 |                       0.636 |                   0.636 |                    0.643 | 0.663                                     | 0.653      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.357 |                       0.413 |                   0.429 |                    0.405 | 0.468                                     | 0.460      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.294 |                       0.357 |                   0.373 |                    0.31  | 0.381                                     | 0.429      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.175 |                       0.294 |                   0.286 |                    0.294 | 0.341                                     | 0.389      |\n| fr         | bert-base-multilingual-cased               |   0.349 |                       0.333 |                   0.397 |                    0.46  | 0.484                                     | 0.540      |\n| fr         | distilbert-base-multilingual-cased         |   0.302 |                       0.421 |                   0.413 |                    0.476 | 0.484                                     | **0.563**  |\n| fr         | facebook-mbart-large-50                    |   0.381 |                       0.413 |                   0.452 |                    0.452 | 0.500                                     | 0.532      |\n| fr         | gpt2                                       |   0.286 |                       0.341 |                   0.341 |                    0.405 | 0.349                                     | 0.444      |\n| fr         | xlm-roberta-large                          |   0.381 |                       0.429 |                   0.444 |                    0.46  | 0.476                                     | 0.532      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.465 |                       0.535 |                   0.483 |                    0.494 | 0.529                                     | 0.599      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0.355 |                       0.483 |                   0.5   |                    0.471 | 0.512                                     | 0.587      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0.331 |                       0.384 |                   0.407 |                    0.401 | 0.442                                     | 0.535      |\n| ge         | bert-base-multilingual-cased               |   0.424 |                       0.488 |                   0.512 |                    0.547 | 0.570                                     | **0.645**  |\n| ge         | distilbert-base-multilingual-cased         |   0.424 |                       0.517 |                   0.471 |                    0.5   | 0.517                                     | **0.645**  |\n| ge         | facebook-mbart-large-50                    |   0.483 |                       0.547 |                   0.488 |                    0.535 | 0.552                                     | 0.599      |\n| ge         | gpt2                                       |   0.413 |                       0.453 |                   0.413 |                    0.523 | 0.442                                     | 0.587      |\n| ge         | xlm-roberta-large                          |   0.465 |                       0.57  |                   0.5   |                    0.564 | 0.593                                     | 0.610      |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.478 |                       0.474 |                   0.474 |                    0.47  | 0.452                                     | 0.526      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.33  |                       0.426 |                   0.404 |                    0.426 | 0.443                                     | 0.526      |\n| it         | EleutherAI-gpt-neo-125M                    |   0.243 |                       0.243 |                   0.348 |                    0.4   | 0.378                                     | 0.465      |\n| it         | bert-base-multilingual-cased               |   0.413 |                       0.483 |                   0.491 |                    0.517 | 0.504                                     | 0.548      |\n| it         | distilbert-base-multilingual-cased         |   0.409 |                       0.47  |                   0.487 |                    0.526 | 0.470                                     | **0.578**  |\n| it         | facebook-mbart-large-50                    |   0.439 |                       0.487 |                   0.483 |                    0.478 | 0.526                                     | 0.565      |\n| it         | gpt2                                       |   0.335 |                       0.417 |                   0.457 |                    0.491 | 0.387                                     | 0.496      |\n| it         | xlm-roberta-large                          |   0.439 |                       0.491 |                   0.496 |                    0.491 | 0.491                                     | 0.543      |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.437 |                       0.549 |                   0.563 |                    0.51  | 0.544                                     | 0.573      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.393 |                       0.422 |                   0.539 |                    0.481 | 0.539                                     | 0.553      |\n| po         | EleutherAI-gpt-neo-125M                    |   0.388 |                       0.427 |                   0.461 |                    0.519 | 0.471                                     | 0.471      |\n| po         | bert-base-multilingual-cased               |   0.519 |                       0.563 |                   0.587 |                    0.597 | 0.597                                     | 0.636      |\n| po         | distilbert-base-multilingual-cased         |   0.49  |                       0.5   |                   0.524 |                    0.558 | 0.568                                     | 0.621      |\n| po         | facebook-mbart-large-50                    |   0.524 |                       0.544 |                   0.602 |                    0.573 | 0.607                                     | 0.617      |\n| po         | gpt2                                       |   0.481 |                       0.481 |                   0.597 |                    0.568 | 0.510                                     | 0.617      |\n| po         | xlm-roberta-large                          |   0.51  |                       0.587 |                   0.573 |                    0.563 | 0.626                                     | **0.650**  |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.36  |                       0.419 |                   0.419 |                    0.43  | 0.442                                     | 0.488      |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.221 |                       0.198 |                   0.267 |                    0.279 | 0.314                                     | 0.279      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.105 |                       0.163 |                   0.174 |                    0.174 | 0.093                                     | 0.105      |\n| ru         | bert-base-multilingual-cased               |   0.302 |                       0.395 |                   0.465 |                    0.419 | 0.419                                     | 0.442      |\n| ru         | distilbert-base-multilingual-cased         |   0.244 |                       0.384 |                   0.407 |                    0.465 | 0.419                                     | **0.523**  |\n| ru         | facebook-mbart-large-50                    |   0.337 |                       0.349 |                   0.419 |                    0.407 | 0.419                                     | 0.465      |\n| ru         | gpt2                                       |   0.105 |                       0.07  |                   0.047 |                    0.058 | 0.093                                     | 0.163      |\n| ru         | xlm-roberta-large                          |   0.314 |                       0.36  |                   0.395 |                    0.384 | 0.442                                     | 0.384      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title |   title and first paragraph | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   |   raw text |\n|:-----------|:-------------------------------------------|--------:|----------------------------:|:------------------------|:-------------------------|:------------------------------------------|-----------:|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.717 |                       0.774 | 0.776                   | 0.765                    | 0.744                                     |      0.734 |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.761 |                       0.794 | 0.824                   | 0.812                    | 0.793                                     |      0.778 |\n| en         | EleutherAI-gpt-neo-125M                    |   0.646 |                       0.73  | 0.735                   | 0.752                    | 0.770                                     |      0.763 |\n| en         | bert-base-multilingual-cased               |   0.696 |                       0.765 | 0.787                   | 0.776                    | 0.751                                     |      0.736 |\n| en         | distilbert-base-multilingual-cased         |   0.684 |                       0.759 | 0.769                   | 0.735                    | 0.750                                     |      0.728 |\n| en         | facebook-mbart-large-50                    |   0.778 |                       0.795 | 0.812                   | 0.804                    | 0.784                                     |      0.752 |\n| en         | gpt2                                       |   0.709 |                       0.751 | 0.725                   | 0.711                    | 0.736                                     |      0.746 |\n| en         | xlm-roberta-large                          |   0.759 |                       0.765 | **0.825**               | 0.804                    | 0.781                                     |      0.765 |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.455 |                       0.52  | 0.562                   | 0.505                    | 0.584                                     |      0.552 |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.536 |                       0.6   | 0.540                   | 0.542                    | 0.578                                     |      0.581 |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.324 |                       0.463 | 0.480                   | 0.440                    | 0.524                                     |      0.57  |\n| fr         | bert-base-multilingual-cased               |   0.524 |                       0.538 | 0.495                   | 0.532                    | 0.545                                     |      0.571 |\n| fr         | distilbert-base-multilingual-cased         |   0.481 |                       0.51  | 0.520                   | 0.545                    | 0.610                                     |      0.53  |\n| fr         | facebook-mbart-large-50                    |   0.558 |                       0.559 | **0.640**               | 0.570                    | 0.589                                     |      0.609 |\n| fr         | gpt2                                       |   0.371 |                       0.443 | 0.483                   | 0.505                    | 0.512                                     |      0.549 |\n| fr         | xlm-roberta-large                          |   0.505 |                       0.545 | 0.577                   | 0.547                    | 0.594                                     |      0.558 |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.702 |                       0.662 | 0.686                   | 0.773                    | 0.695                                     |      0.701 |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0.678 |                       0.703 | 0.705                   | **0.802**                | 0.733                                     |      0.721 |\n| ge         | EleutherAI-gpt-neo-125M                    |   0.533 |                       0.641 | 0.680                   | 0.651                    | 0.667                                     |      0.697 |\n| ge         | bert-base-multilingual-cased               |   0.608 |                       0.677 | 0.698                   | 0.707                    | 0.628                                     |      0.673 |\n| ge         | distilbert-base-multilingual-cased         |   0.658 |                       0.636 | 0.638                   | 0.699                    | 0.685                                     |      0.653 |\n| ge         | facebook-mbart-large-50                    |   0.741 |                       0.667 | 0.672                   | 0.760                    | 0.704                                     |      0.701 |\n| ge         | gpt2                                       |   0.559 |                       0.624 | 0.587                   | 0.652                    | 0.650                                     |      0.647 |\n| ge         | xlm-roberta-large                          |   0.721 |                       0.705 | 0.711                   | 0.735                    | 0.723                                     |      0.719 |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.705 |                       0.657 | 0.661                   | 0.735                    | 0.727                                     |      0.708 |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.644 |                       0.731 | 0.756                   | 0.726                    | 0.713                                     |      0.716 |\n| it         | EleutherAI-gpt-neo-125M                    |   0.392 |                       0.483 | 0.606                   | 0.586                    | 0.600                                     |      0.69  |\n| it         | bert-base-multilingual-cased               |   0.569 |                       0.703 | 0.669                   | 0.692                    | 0.671                                     |      0.689 |\n| it         | distilbert-base-multilingual-cased         |   0.57  |                       0.651 | 0.671                   | 0.669                    | 0.639                                     |      0.686 |\n| it         | facebook-mbart-large-50                    |   0.678 |                       0.732 | 0.760                   | **0.780**                | 0.733                                     |      0.734 |\n| it         | gpt2                                       |   0.487 |                       0.578 | 0.587                   | 0.608                    | 0.622                                     |      0.582 |\n| it         | xlm-roberta-large                          |   0.673 |                       0.729 | 0.765                   | 0.764                    | 0.729                                     |      0.758 |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.604 |                       0.661 | 0.682                   | 0.750                    | 0.762                                     |      0.711 |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.566 |                       0.659 | 0.725                   | 0.750                    | 0.725                                     |      0.713 |\n| po         | EleutherAI-gpt-neo-125M                    |   0.544 |                       0.591 | 0.586                   | 0.622                    | 0.647                                     |      0.664 |\n| po         | bert-base-multilingual-cased               |   0.633 |                       0.69  | 0.733                   | 0.724                    | 0.755                                     |      0.693 |\n| po         | distilbert-base-multilingual-cased         |   0.647 |                       0.592 | 0.679                   | 0.747                    | 0.709                                     |      0.656 |\n| po         | facebook-mbart-large-50                    |   0.614 |                       0.679 | 0.770                   | **0.792**                | 0.786                                     |      0.738 |\n| po         | gpt2                                       |   0.559 |                       0.611 | 0.586                   | 0.684                    | 0.660                                     |      0.694 |\n| po         | xlm-roberta-large                          |   0.644 |                       0.703 | 0.711                   | 0.744                    | 0.782                                     |      0.753 |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.5   |                       0.522 | 0.545                   | 0.561                    | 0.633                                     |      0.583 |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.576 |                       0.515 | 0.639                   | 0.632                    | 0.643                                     |      0.571 |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.36  |                       0.341 | 0.385                   | 0.288                    | 0.258                                     |      0.265 |\n| ru         | bert-base-multilingual-cased               |   0.464 |                       0.486 | 0.580                   | 0.537                    | 0.529                                     |      0.521 |\n| ru         | distilbert-base-multilingual-cased         |   0.457 |                       0.434 | 0.556                   | 0.541                    | 0.529                                     |      0.484 |\n| ru         | facebook-mbart-large-50                    |   0.58  |                       0.577 | 0.655                   | 0.614                    | **0.706**                                 |      0.588 |\n| ru         | gpt2                                       |   0.333 |                       0.231 | 0.190                   | 0.263                    | 0.308                                     |      0.326 |\n| ru         | xlm-roberta-large                          |   0.562 |                       0.585 | 0.607                   | 0.559                    | 0.594                                     |      0.524 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.087 | 0.136                       | 0.126                   | 0.068                    | 0.078                                     | 0.058      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.087 | 0.087                       | **0.155**               | 0.097                    | 0.117                                     | 0.087      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.019 | 0.087                       | 0.058                   | 0.039                    | 0.087                                     | 0.078      |\n| en         | bert-base-multilingual-cased               |   0.078 | 0.087                       | 0.117                   | 0.078                    | 0.068                                     | 0.087      |\n| en         | distilbert-base-multilingual-cased         |   0.097 | 0.078                       | 0.126                   | 0.078                    | 0.117                                     | 0.097      |\n| en         | facebook-mbart-large-50                    |   0.117 | 0.126                       | 0.146                   | 0.107                    | 0.146                                     | 0.097      |\n| en         | gpt2                                       |   0.097 | 0.097                       | 0.117                   | 0.087                    | 0.078                                     | 0.117      |\n| en         | xlm-roberta-large                          |   0.058 | 0.087                       | 0.146                   | 0.126                    | 0.117                                     | 0.117      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.071 | 0.071                       | 0.071                   | 0.071                    | **0.119**                                 | 0.071      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.024 | **0.119**                   | 0.024                   | 0.071                    | 0.024                                     | 0.071      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.024 | 0.024                       | 0.024                   | 0.024                    | 0.024                                     | 0.071      |\n| fr         | bert-base-multilingual-cased               |   0.048 | 0.071                       | 0.048                   | 0.071                    | 0.071                                     | 0.095      |\n| fr         | distilbert-base-multilingual-cased         |   0.071 | 0.071                       | 0.095                   | 0.048                    | 0.095                                     | 0.048      |\n| fr         | facebook-mbart-large-50                    |   0.024 | 0.095                       | 0.071                   | 0.048                    | 0.071                                     | 0.071      |\n| fr         | gpt2                                       |   0     | 0.024                       | 0.048                   | 0.000                    | 0.048                                     | 0.048      |\n| fr         | xlm-roberta-large                          |   0     | **0.119**                   | 0.071                   | 0.095                    | 0.095                                     | 0.071      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.029 | 0.029                       | 0.029                   | 0.057                    | 0.057                                     | 0.029      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0     | 0.029                       | 0.000                   | 0.057                    | 0.029                                     | 0.029      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0     | 0.000                       | 0.029                   | 0.000                    | 0.029                                     | 0.029      |\n| ge         | bert-base-multilingual-cased               |   0     | 0.029                       | 0.029                   | 0.057                    | 0.000                                     | 0.029      |\n| ge         | distilbert-base-multilingual-cased         |   0.029 | 0.029                       | 0.000                   | 0.000                    | 0.029                                     | 0.000      |\n| ge         | facebook-mbart-large-50                    |   0.057 | 0.029                       | 0.029                   | 0.029                    | 0.057                                     | 0.000      |\n| ge         | gpt2                                       |   0     | 0.000                       | 0.029                   | **0.086**                | 0.000                                     | 0.000      |\n| ge         | xlm-roberta-large                          |   0.029 | 0.057                       | 0.029                   | 0.000                    | 0.057                                     | 0.029      |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.1   | 0.150                       | 0.167                   | 0.133                    | 0.100                                     | 0.117      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.05  | 0.117                       | **0.183**               | 0.150                    | 0.133                                     | 0.133      |\n| it         | EleutherAI-gpt-neo-125M                    |   0     | 0.000                       | 0.033                   | 0.067                    | 0.033                                     | 0.133      |\n| it         | bert-base-multilingual-cased               |   0.017 | 0.117                       | 0.067                   | **0.183**                | 0.117                                     | 0.133      |\n| it         | distilbert-base-multilingual-cased         |   0.017 | 0.117                       | 0.117                   | 0.150                    | 0.083                                     | 0.133      |\n| it         | facebook-mbart-large-50                    |   0.1   | 0.167                       | 0.150                   | 0.167                    | 0.167                                     | **0.183**  |\n| it         | gpt2                                       |   0.033 | 0.033                       | 0.033                   | 0.067                    | 0.083                                     | 0.050      |\n| it         | xlm-roberta-large                          |   0.1   | 0.167                       | **0.183**               | 0.133                    | 0.150                                     | **0.183**  |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.026 | 0.026                       | 0.026                   | 0.051                    | 0.051                                     | 0.051      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.026 | 0.000                       | 0.051                   | 0.051                    | 0.026                                     | 0.051      |\n| po         | EleutherAI-gpt-neo-125M                    |   0     | 0.000                       | 0.051                   | 0.026                    | 0.026                                     | 0.026      |\n| po         | bert-base-multilingual-cased               |   0.026 | 0.051                       | 0.051                   | 0.051                    | 0.051                                     | 0.051      |\n| po         | distilbert-base-multilingual-cased         |   0.051 | 0.000                       | 0.026                   | 0.051                    | 0.051                                     | 0.026      |\n| po         | facebook-mbart-large-50                    |   0.026 | 0.000                       | 0.026                   | **0.077**                | 0.026                                     | 0.051      |\n| po         | gpt2                                       |   0     | 0.000                       | 0.000                   | 0.026                    | 0.026                                     | 0.051      |\n| po         | xlm-roberta-large                          |   0.051 | 0.026                       | 0.026                   | 0.051                    | 0.026                                     | **0.077**  |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.158 | 0.158                       | 0.132                   | 0.079                    | **0.263**                                 | 0.237      |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.053 | 0.105                       | 0.079                   | 0.158                    | 0.105                                     | 0.105      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.026 | 0.000                       | 0.053                   | 0.000                    | 0.026                                     | 0.026      |\n| ru         | bert-base-multilingual-cased               |   0.079 | 0.105                       | 0.237                   | 0.158                    | 0.184                                     | 0.237      |\n| ru         | distilbert-base-multilingual-cased         |   0.105 | 0.132                       | 0.158                   | 0.158                    | 0.211                                     | 0.158      |\n| ru         | facebook-mbart-large-50                    |   0.158 | 0.211                       | 0.211                   | 0.184                    | 0.211                                     | 0.184      |\n| ru         | gpt2                                       |   0     | 0.000                       | 0.000                   | 0.026                    | 0.053                                     | 0.026      |\n| ru         | xlm-roberta-large                          |   0.158 | 0.184                       | 0.211                   | 0.105                    | 0.211                                     | 0.158      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    }
   ],
   "source": [
    "display_metrics_and_write_to_file(df=results_mean_pred_df, grouping_criterion=['model_name'], output_dir='per_model_name_tables_mean_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# English"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.64  |                       0.7   |                   0.684 |                    0.699 | 0.696                                     |      0.679 |\n| EleutherAI-gpt-neo-1.3B                    |   0.63  |                       0.703 |                   0.707 |                    0.693 | 0.692                                     |      0.692 |\n| EleutherAI-gpt-neo-125M                    |   0.53  |                       0.623 |                   0.637 |                    0.634 | 0.649                                     |      0.654 |\n| bert-base-multilingual-cased               |   0.61  |                       0.677 |                   0.7   |                    0.707 | 0.682                                     |      0.688 |\n| distilbert-base-multilingual-cased         |   0.596 |                       0.665 |                   0.661 |                    0.672 | 0.662                                     |      0.683 |\n| facebook-mbart-large-50                    |   0.677 |                       0.717 |                   0.719 |                    0.721 | **0.723**                                 |      0.701 |\n| gpt2                                       |   0.618 |                       0.693 |                   0.69  |                    0.673 | 0.696                                     |      0.691 |\n| xlm-roberta-large                          |   0.66  |                       0.694 |                   0.718 |                    0.715 | 0.710                                     |      0.687 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.577 |                       0.638 |                   0.611 |                    0.643 | 0.643                                     |      0.621 |\n| EleutherAI-gpt-neo-1.3B                    |   0.538 |                       0.631 |                   0.619 |                    0.604 | 0.604                                     |      0.611 |\n| EleutherAI-gpt-neo-125M                    |   0.45  |                       0.543 |                   0.562 |                    0.548 | 0.555                                     |      0.562 |\n| bert-base-multilingual-cased               |   0.543 |                       0.606 |                   0.631 |                    0.65  | 0.619                                     |      0.621 |\n| distilbert-base-multilingual-cased         |   0.528 |                       0.592 |                   0.579 |                    0.619 | 0.589                                     |      0.623 |\n| facebook-mbart-large-50                    |   0.599 |                       0.653 |                   0.645 |                    0.653 | **0.660**                                 |      0.643 |\n| gpt2                                       |   0.548 |                       0.643 |                   0.658 |                    0.638 | 0.645                                     |      0.631 |\n| xlm-roberta-large                          |   0.584 |                       0.636 |                   0.636 |                    0.643 | 0.648                                     |      0.621 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.717 |                       0.774 | 0.776                   |                    0.765 |                                     0.758 |      0.749 |\n| EleutherAI-gpt-neo-1.3B                    |   0.761 |                       0.794 | 0.824                   |                    0.812 |                                     0.81  |      0.796 |\n| EleutherAI-gpt-neo-125M                    |   0.646 |                       0.73  | 0.735                   |                    0.752 |                                     0.78  |      0.782 |\n| bert-base-multilingual-cased               |   0.696 |                       0.765 | 0.787                   |                    0.776 |                                     0.76  |      0.772 |\n| distilbert-base-multilingual-cased         |   0.684 |                       0.759 | 0.769                   |                    0.735 |                                     0.755 |      0.754 |\n| facebook-mbart-large-50                    |   0.778 |                       0.795 | 0.812                   |                    0.804 |                                     0.799 |      0.771 |\n| gpt2                                       |   0.709 |                       0.751 | 0.725                   |                    0.711 |                                     0.754 |      0.763 |\n| xlm-roberta-large                          |   0.759 |                       0.765 | **0.825**               |                    0.804 |                                     0.784 |      0.77  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.087 |                       0.136 | 0.126                   |                    0.068 |                                     0.097 |      0.058 |\n| EleutherAI-gpt-neo-1.3B                    |   0.087 |                       0.087 | **0.155**               |                    0.097 |                                     0.107 |      0.087 |\n| EleutherAI-gpt-neo-125M                    |   0.019 |                       0.087 | 0.058                   |                    0.039 |                                     0.078 |      0.107 |\n| bert-base-multilingual-cased               |   0.078 |                       0.087 | 0.117                   |                    0.078 |                                     0.078 |      0.117 |\n| distilbert-base-multilingual-cased         |   0.097 |                       0.078 | 0.126                   |                    0.078 |                                     0.117 |      0.117 |\n| facebook-mbart-large-50                    |   0.117 |                       0.126 | 0.146                   |                    0.107 |                                     0.126 |      0.126 |\n| gpt2                                       |   0.097 |                       0.097 | 0.117                   |                    0.087 |                                     0.087 |      0.117 |\n| xlm-roberta-large                          |   0.058 |                       0.087 | 0.146                   |                    0.126 |                                     0.107 |      0.107 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# French"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.4   |                       0.46  |                   0.486 |                    0.449 |                                     0.513 | 0.491      |\n| EleutherAI-gpt-neo-1.3B                    |   0.379 |                       0.448 |                   0.441 |                    0.394 |                                     0.433 | 0.483      |\n| EleutherAI-gpt-neo-125M                    |   0.227 |                       0.359 |                   0.358 |                    0.352 |                                     0.414 | 0.459      |\n| bert-base-multilingual-cased               |   0.419 |                       0.412 |                   0.441 |                    0.494 |                                     0.517 | 0.498      |\n| distilbert-base-multilingual-cased         |   0.371 |                       0.461 |                   0.46  |                    0.508 |                                     0.532 | **0.544**  |\n| facebook-mbart-large-50                    |   0.453 |                       0.475 |                   0.53  |                    0.504 |                                     0.534 | 0.522      |\n| gpt2                                       |   0.323 |                       0.386 |                   0.4   |                    0.449 |                                     0.4   | 0.495      |\n| xlm-roberta-large                          |   0.434 |                       0.48  |                   0.502 |                    0.5   |                                     0.529 | 0.513      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.357 |                       0.413 |                   0.429 |                    0.405 |                                     0.46  | 0.429      |\n| EleutherAI-gpt-neo-1.3B                    |   0.294 |                       0.357 |                   0.373 |                    0.31  |                                     0.349 | 0.405      |\n| EleutherAI-gpt-neo-125M                    |   0.175 |                       0.294 |                   0.286 |                    0.294 |                                     0.333 | 0.357      |\n| bert-base-multilingual-cased               |   0.349 |                       0.333 |                   0.397 |                    0.46  |                                     0.476 | 0.437      |\n| distilbert-base-multilingual-cased         |   0.302 |                       0.421 |                   0.413 |                    0.476 |                                     0.468 | **0.516**  |\n| facebook-mbart-large-50                    |   0.381 |                       0.413 |                   0.452 |                    0.452 |                                     0.492 | 0.468      |\n| gpt2                                       |   0.286 |                       0.341 |                   0.341 |                    0.405 |                                     0.333 | 0.421      |\n| xlm-roberta-large                          |   0.381 |                       0.429 |                   0.444 |                    0.46  |                                     0.476 | 0.460      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.455 |                       0.52  |                   0.562 |                    0.505 |                                     0.58  | 0.574      |\n| EleutherAI-gpt-neo-1.3B                    |   0.536 |                       0.6   |                   0.54  |                    0.542 |                                     0.571 | 0.600      |\n| EleutherAI-gpt-neo-125M                    |   0.324 |                       0.463 |                   0.48  |                    0.44  |                                     0.545 | **0.643**  |\n| bert-base-multilingual-cased               |   0.524 |                       0.538 |                   0.495 |                    0.532 |                                     0.566 | 0.579      |\n| distilbert-base-multilingual-cased         |   0.481 |                       0.51  |                   0.52  |                    0.545 |                                     0.615 | 0.575      |\n| facebook-mbart-large-50                    |   0.558 |                       0.559 |                   0.64  |                    0.57  |                                     0.585 | 0.590      |\n| gpt2                                       |   0.371 |                       0.443 |                   0.483 |                    0.505 |                                     0.5   | 0.602      |\n| xlm-roberta-large                          |   0.505 |                       0.545 |                   0.577 |                    0.547 |                                     0.594 | 0.580      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.071 | 0.071                       |                   0.071 |                    0.071 | **0.119**                                 |      0.095 |\n| EleutherAI-gpt-neo-1.3B                    |   0.024 | **0.119**                   |                   0.024 |                    0.071 | 0.024                                     |      0.071 |\n| EleutherAI-gpt-neo-125M                    |   0.024 | 0.024                       |                   0.024 |                    0.024 | 0.024                                     |      0.071 |\n| bert-base-multilingual-cased               |   0.048 | 0.071                       |                   0.048 |                    0.071 | 0.071                                     |      0.095 |\n| distilbert-base-multilingual-cased         |   0.071 | 0.071                       |                   0.095 |                    0.048 | 0.095                                     |      0.095 |\n| facebook-mbart-large-50                    |   0.024 | 0.095                       |                   0.071 |                    0.048 | 0.071                                     |      0.071 |\n| gpt2                                       |   0     | 0.024                       |                   0.048 |                    0     | 0.048                                     |      0.048 |\n| xlm-roberta-large                          |   0     | **0.119**                   |                   0.071 |                    0.095 | 0.095                                     |      0.048 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# German"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.559 |                       0.592 |                   0.567 |                    0.603 |                                     0.581 | 0.636      |\n| EleutherAI-gpt-neo-1.3B                    |   0.466 |                       0.572 |                   0.585 |                    0.593 |                                     0.582 | 0.636      |\n| EleutherAI-gpt-neo-125M                    |   0.409 |                       0.48  |                   0.509 |                    0.496 |                                     0.518 | 0.592      |\n| bert-base-multilingual-cased               |   0.5   |                       0.568 |                   0.591 |                    0.616 |                                     0.583 | **0.654**  |\n| distilbert-base-multilingual-cased         |   0.516 |                       0.571 |                   0.542 |                    0.583 |                                     0.576 | 0.638      |\n| facebook-mbart-large-50                    |   0.585 |                       0.601 |                   0.566 |                    0.628 |                                     0.616 | 0.645      |\n| gpt2                                       |   0.475 |                       0.525 |                   0.485 |                    0.581 |                                     0.512 | 0.618      |\n| xlm-roberta-large                          |   0.565 |                       0.63  |                   0.587 |                    0.638 |                                     0.623 | 0.636      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.465 |                       0.535 |                   0.483 |                    0.494 |                                     0.5   | 0.570      |\n| EleutherAI-gpt-neo-1.3B                    |   0.355 |                       0.483 |                   0.5   |                    0.471 |                                     0.483 | 0.558      |\n| EleutherAI-gpt-neo-125M                    |   0.331 |                       0.384 |                   0.407 |                    0.401 |                                     0.419 | 0.494      |\n| bert-base-multilingual-cased               |   0.424 |                       0.488 |                   0.512 |                    0.547 |                                     0.541 | **0.616**  |\n| distilbert-base-multilingual-cased         |   0.424 |                       0.517 |                   0.471 |                    0.5   |                                     0.494 | 0.599      |\n| facebook-mbart-large-50                    |   0.483 |                       0.547 |                   0.488 |                    0.535 |                                     0.541 | 0.564      |\n| gpt2                                       |   0.413 |                       0.453 |                   0.413 |                    0.523 |                                     0.419 | 0.570      |\n| xlm-roberta-large                          |   0.465 |                       0.57  |                   0.5   |                    0.564 |                                     0.552 | 0.564      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.702 |                       0.662 |                   0.686 | 0.773                    |                                     0.694 |      0.721 |\n| EleutherAI-gpt-neo-1.3B                    |   0.678 |                       0.703 |                   0.705 | **0.802**                |                                     0.735 |      0.738 |\n| EleutherAI-gpt-neo-125M                    |   0.533 |                       0.641 |                   0.68  | 0.651                    |                                     0.679 |      0.739 |\n| bert-base-multilingual-cased               |   0.608 |                       0.677 |                   0.698 | 0.707                    |                                     0.633 |      0.697 |\n| distilbert-base-multilingual-cased         |   0.658 |                       0.636 |                   0.638 | 0.699                    |                                     0.691 |      0.682 |\n| facebook-mbart-large-50                    |   0.741 |                       0.667 |                   0.672 | 0.760                    |                                     0.715 |      0.752 |\n| gpt2                                       |   0.559 |                       0.624 |                   0.587 | 0.652                    |                                     0.661 |      0.676 |\n| xlm-roberta-large                          |   0.721 |                       0.705 |                   0.711 | 0.735                    |                                     0.714 |      0.729 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.029 |                       0.029 |                   0.029 | 0.057                    |                                     0.057 |      0.029 |\n| EleutherAI-gpt-neo-1.3B                    |   0     |                       0.029 |                   0     | 0.057                    |                                     0.029 |      0.029 |\n| EleutherAI-gpt-neo-125M                    |   0     |                       0     |                   0.029 | 0.000                    |                                     0.029 |      0.029 |\n| bert-base-multilingual-cased               |   0     |                       0.029 |                   0.029 | 0.057                    |                                     0     |      0.029 |\n| distilbert-base-multilingual-cased         |   0.029 |                       0.029 |                   0     | 0.000                    |                                     0.029 |      0     |\n| facebook-mbart-large-50                    |   0.057 |                       0.029 |                   0.029 | 0.029                    |                                     0.057 |      0     |\n| gpt2                                       |   0     |                       0     |                   0.029 | **0.086**                |                                     0     |      0     |\n| xlm-roberta-large                          |   0.029 |                       0.057 |                   0.029 | 0.000                    |                                     0.057 |      0.029 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Italian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.57  |                       0.551 |                   0.552 |                    0.573 |                                     0.542 | 0.573      |\n| EleutherAI-gpt-neo-1.3B                    |   0.437 |                       0.538 |                   0.527 |                    0.537 |                                     0.541 | 0.575      |\n| EleutherAI-gpt-neo-125M                    |   0.3   |                       0.324 |                   0.442 |                    0.475 |                                     0.459 | 0.509      |\n| bert-base-multilingual-cased               |   0.479 |                       0.572 |                   0.566 |                    0.592 |                                     0.574 | 0.585      |\n| distilbert-base-multilingual-cased         |   0.476 |                       0.545 |                   0.564 |                    0.589 |                                     0.535 | 0.589      |\n| facebook-mbart-large-50                    |   0.533 |                       0.585 |                   0.59  |                    0.593 |                                     0.593 | **0.627**  |\n| gpt2                                       |   0.397 |                       0.485 |                   0.513 |                    0.543 |                                     0.469 | 0.525      |\n| xlm-roberta-large                          |   0.532 |                       0.587 |                   0.602 |                    0.598 |                                     0.57  | 0.573      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.478 |                       0.474 |                   0.474 | 0.470                    |                                     0.435 |      0.478 |\n| EleutherAI-gpt-neo-1.3B                    |   0.33  |                       0.426 |                   0.404 | 0.426                    |                                     0.435 |      0.474 |\n| EleutherAI-gpt-neo-125M                    |   0.243 |                       0.243 |                   0.348 | 0.400                    |                                     0.37  |      0.409 |\n| bert-base-multilingual-cased               |   0.413 |                       0.483 |                   0.491 | 0.517                    |                                     0.496 |      0.496 |\n| distilbert-base-multilingual-cased         |   0.409 |                       0.47  |                   0.487 | **0.526**                |                                     0.461 |      0.517 |\n| facebook-mbart-large-50                    |   0.439 |                       0.487 |                   0.483 | 0.478                    |                                     0.5   |      0.522 |\n| gpt2                                       |   0.335 |                       0.417 |                   0.457 | 0.491                    |                                     0.374 |      0.461 |\n| xlm-roberta-large                          |   0.439 |                       0.491 |                   0.496 | 0.491                    |                                     0.47  |      0.461 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.705 |                       0.657 |                   0.661 |                    0.735 |                                     0.719 | 0.714      |\n| EleutherAI-gpt-neo-1.3B                    |   0.644 |                       0.731 |                   0.756 |                    0.726 |                                     0.714 | 0.732      |\n| EleutherAI-gpt-neo-125M                    |   0.392 |                       0.483 |                   0.606 |                    0.586 |                                     0.607 | 0.676      |\n| bert-base-multilingual-cased               |   0.569 |                       0.703 |                   0.669 |                    0.692 |                                     0.683 | 0.713      |\n| distilbert-base-multilingual-cased         |   0.57  |                       0.651 |                   0.671 |                    0.669 |                                     0.639 | 0.684      |\n| facebook-mbart-large-50                    |   0.678 |                       0.732 |                   0.76  |                    0.78  |                                     0.728 | **0.784**  |\n| gpt2                                       |   0.487 |                       0.578 |                   0.587 |                    0.608 |                                     0.628 | 0.609      |\n| xlm-roberta-large                          |   0.673 |                       0.729 |                   0.765 |                    0.764 |                                     0.725 | 0.757      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.1   |                       0.15  |                   0.167 |                    0.133 |                                     0.1   | 0.117      |\n| EleutherAI-gpt-neo-1.3B                    |   0.05  |                       0.117 |                   0.183 |                    0.15  |                                     0.133 | 0.167      |\n| EleutherAI-gpt-neo-125M                    |   0     |                       0     |                   0.033 |                    0.067 |                                     0.033 | 0.117      |\n| bert-base-multilingual-cased               |   0.017 |                       0.117 |                   0.067 |                    0.183 |                                     0.117 | 0.133      |\n| distilbert-base-multilingual-cased         |   0.017 |                       0.117 |                   0.117 |                    0.15  |                                     0.083 | 0.100      |\n| facebook-mbart-large-50                    |   0.1   |                       0.167 |                   0.15  |                    0.167 |                                     0.167 | **0.217**  |\n| gpt2                                       |   0.033 |                       0.033 |                   0.033 |                    0.067 |                                     0.083 | 0.067      |\n| xlm-roberta-large                          |   0.1   |                       0.167 |                   0.183 |                    0.133 |                                     0.15  | 0.183      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Polish"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.507 |                       0.599 |                   0.617 |                    0.607 | 0.623                                     |      0.607 |\n| EleutherAI-gpt-neo-1.3B                    |   0.464 |                       0.515 |                   0.618 |                    0.586 | 0.618                                     |      0.604 |\n| EleutherAI-gpt-neo-125M                    |   0.453 |                       0.496 |                   0.516 |                    0.566 | 0.522                                     |      0.511 |\n| bert-base-multilingual-cased               |   0.571 |                       0.62  |                   0.652 |                    0.654 | 0.646                                     |      0.624 |\n| distilbert-base-multilingual-cased         |   0.558 |                       0.542 |                   0.592 |                    0.639 | 0.623                                     |      0.604 |\n| facebook-mbart-large-50                    |   0.565 |                       0.604 |                   0.676 |                    0.665 | **0.680**                                 |      0.652 |\n| gpt2                                       |   0.517 |                       0.538 |                   0.591 |                    0.621 | 0.568                                     |      0.59  |\n| xlm-roberta-large                          |   0.569 |                       0.64  |                   0.634 |                    0.641 | 0.667                                     |      0.665 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.437 |                       0.549 | 0.563                   |                    0.51  |                                     0.529 |      0.524 |\n| EleutherAI-gpt-neo-1.3B                    |   0.393 |                       0.422 | 0.539                   |                    0.481 |                                     0.529 |      0.515 |\n| EleutherAI-gpt-neo-125M                    |   0.388 |                       0.427 | 0.461                   |                    0.519 |                                     0.432 |      0.408 |\n| bert-base-multilingual-cased               |   0.519 |                       0.563 | 0.587                   |                    0.597 |                                     0.568 |      0.563 |\n| distilbert-base-multilingual-cased         |   0.49  |                       0.5   | 0.524                   |                    0.558 |                                     0.553 |      0.549 |\n| facebook-mbart-large-50                    |   0.524 |                       0.544 | **0.602**               |                    0.573 |                                     0.597 |      0.573 |\n| gpt2                                       |   0.481 |                       0.481 | 0.597                   |                    0.568 |                                     0.495 |      0.51  |\n| xlm-roberta-large                          |   0.51  |                       0.587 | 0.573                   |                    0.563 |                                     0.587 |      0.592 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.604 |                       0.661 |                   0.682 | 0.750                    |                                     0.757 |      0.72  |\n| EleutherAI-gpt-neo-1.3B                    |   0.566 |                       0.659 |                   0.725 | 0.750                    |                                     0.741 |      0.731 |\n| EleutherAI-gpt-neo-125M                    |   0.544 |                       0.591 |                   0.586 | 0.622                    |                                     0.659 |      0.683 |\n| bert-base-multilingual-cased               |   0.633 |                       0.69  |                   0.733 | 0.724                    |                                     0.75  |      0.699 |\n| distilbert-base-multilingual-cased         |   0.647 |                       0.592 |                   0.679 | 0.747                    |                                     0.713 |      0.673 |\n| facebook-mbart-large-50                    |   0.614 |                       0.679 |                   0.77  | **0.792**                |                                     0.788 |      0.756 |\n| gpt2                                       |   0.559 |                       0.611 |                   0.586 | 0.684                    |                                     0.667 |      0.7   |\n| xlm-roberta-large                          |   0.644 |                       0.703 |                   0.711 | 0.744                    |                                     0.771 |      0.758 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.026 |                       0.026 |                   0.026 | 0.051                    |                                     0.051 |      0.051 |\n| EleutherAI-gpt-neo-1.3B                    |   0.026 |                       0     |                   0.051 | 0.051                    |                                     0.026 |      0.051 |\n| EleutherAI-gpt-neo-125M                    |   0     |                       0     |                   0.051 | 0.026                    |                                     0.026 |      0.026 |\n| bert-base-multilingual-cased               |   0.026 |                       0.051 |                   0.051 | 0.051                    |                                     0.051 |      0.051 |\n| distilbert-base-multilingual-cased         |   0.051 |                       0     |                   0.026 | 0.051                    |                                     0.051 |      0.026 |\n| facebook-mbart-large-50                    |   0.026 |                       0     |                   0.026 | **0.077**                |                                     0.026 |      0.026 |\n| gpt2                                       |   0     |                       0     |                   0     | 0.026                    |                                     0.026 |      0.026 |\n| xlm-roberta-large                          |   0.051 |                       0.026 |                   0.026 | 0.051                    |                                     0.026 |      0.026 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Russian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.419 |                       0.465 |                   0.474 |                    0.487 |                                     0.521 | **0.530**  |\n| EleutherAI-gpt-neo-1.3B                    |   0.319 |                       0.286 |                   0.377 |                    0.387 |                                     0.409 | 0.374      |\n| EleutherAI-gpt-neo-125M                    |   0.162 |                       0.22  |                   0.24  |                    0.217 |                                     0.122 | 0.151      |\n| bert-base-multilingual-cased               |   0.366 |                       0.436 |                   0.516 |                    0.471 |                                     0.447 | 0.497      |\n| distilbert-base-multilingual-cased         |   0.318 |                       0.407 |                   0.47  |                    0.5   |                                     0.461 | 0.488      |\n| facebook-mbart-large-50                    |   0.426 |                       0.435 |                   0.511 |                    0.49  |                                     0.519 | 0.507      |\n| gpt2                                       |   0.159 |                       0.107 |                   0.075 |                    0.095 |                                     0.126 | 0.215      |\n| xlm-roberta-large                          |   0.403 |                       0.446 |                   0.479 |                    0.455 |                                     0.486 | 0.455      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.36  |                       0.419 |                   0.419 |                    0.43  |                                     0.442 | 0.465      |\n| EleutherAI-gpt-neo-1.3B                    |   0.221 |                       0.198 |                   0.267 |                    0.279 |                                     0.302 | 0.267      |\n| EleutherAI-gpt-neo-125M                    |   0.105 |                       0.163 |                   0.174 |                    0.174 |                                     0.081 | 0.105      |\n| bert-base-multilingual-cased               |   0.302 |                       0.395 |                   0.465 |                    0.419 |                                     0.395 | 0.442      |\n| distilbert-base-multilingual-cased         |   0.244 |                       0.384 |                   0.407 |                    0.465 |                                     0.407 | **0.477**  |\n| facebook-mbart-large-50                    |   0.337 |                       0.349 |                   0.419 |                    0.407 |                                     0.407 | 0.430      |\n| gpt2                                       |   0.105 |                       0.07  |                   0.047 |                    0.058 |                                     0.081 | 0.151      |\n| xlm-roberta-large                          |   0.314 |                       0.36  |                   0.395 |                    0.384 |                                     0.419 | 0.384      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.5   |                       0.522 |                   0.545 |                    0.561 | 0.633                                     |      0.615 |\n| EleutherAI-gpt-neo-1.3B                    |   0.576 |                       0.515 |                   0.639 |                    0.632 | 0.634                                     |      0.622 |\n| EleutherAI-gpt-neo-125M                    |   0.36  |                       0.341 |                   0.385 |                    0.288 | 0.241                                     |      0.273 |\n| bert-base-multilingual-cased               |   0.464 |                       0.486 |                   0.58  |                    0.537 | 0.515                                     |      0.567 |\n| distilbert-base-multilingual-cased         |   0.457 |                       0.434 |                   0.556 |                    0.541 | 0.530                                     |      0.5   |\n| facebook-mbart-large-50                    |   0.58  |                       0.577 |                   0.655 |                    0.614 | **0.714**                                 |      0.617 |\n| gpt2                                       |   0.333 |                       0.231 |                   0.19  |                    0.263 | 0.280                                     |      0.371 |\n| xlm-roberta-large                          |   0.562 |                       0.585 |                   0.607 |                    0.559 | 0.581                                     |      0.559 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.158 |                       0.158 |                   0.132 |                    0.079 | **0.263**                                 |      0.211 |\n| EleutherAI-gpt-neo-1.3B                    |   0.053 |                       0.105 |                   0.079 |                    0.158 | 0.105                                     |      0.132 |\n| EleutherAI-gpt-neo-125M                    |   0.026 |                       0     |                   0.053 |                    0     | 0.026                                     |      0.026 |\n| bert-base-multilingual-cased               |   0.079 |                       0.105 |                   0.237 |                    0.158 | 0.158                                     |      0.237 |\n| distilbert-base-multilingual-cased         |   0.105 |                       0.132 |                   0.158 |                    0.158 | 0.184                                     |      0.184 |\n| facebook-mbart-large-50                    |   0.158 |                       0.211 |                   0.211 |                    0.184 | 0.184                                     |      0.184 |\n| gpt2                                       |   0     |                       0     |                   0     |                    0.026 | 0.053                                     |      0.026 |\n| xlm-roberta-large                          |   0.158 |                       0.184 |                   0.211 |                    0.105 | 0.184                                     |      0.158 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# All 6 Languages"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.64  |                       0.7   |                   0.684 |                    0.699 | 0.696                                     | 0.679      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.63  |                       0.703 |                   0.707 |                    0.693 | 0.692                                     | 0.692      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.53  |                       0.623 |                   0.637 |                    0.634 | 0.649                                     | 0.654      |\n| en         | bert-base-multilingual-cased               |   0.61  |                       0.677 |                   0.7   |                    0.707 | 0.682                                     | 0.688      |\n| en         | distilbert-base-multilingual-cased         |   0.596 |                       0.665 |                   0.661 |                    0.672 | 0.662                                     | 0.683      |\n| en         | facebook-mbart-large-50                    |   0.677 |                       0.717 |                   0.719 |                    0.721 | **0.723**                                 | 0.701      |\n| en         | gpt2                                       |   0.618 |                       0.693 |                   0.69  |                    0.673 | 0.696                                     | 0.691      |\n| en         | xlm-roberta-large                          |   0.66  |                       0.694 |                   0.718 |                    0.715 | 0.710                                     | 0.687      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.4   |                       0.46  |                   0.486 |                    0.449 | 0.513                                     | 0.491      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.379 |                       0.448 |                   0.441 |                    0.394 | 0.433                                     | 0.483      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.227 |                       0.359 |                   0.358 |                    0.352 | 0.414                                     | 0.459      |\n| fr         | bert-base-multilingual-cased               |   0.419 |                       0.412 |                   0.441 |                    0.494 | 0.517                                     | 0.498      |\n| fr         | distilbert-base-multilingual-cased         |   0.371 |                       0.461 |                   0.46  |                    0.508 | 0.532                                     | **0.544**  |\n| fr         | facebook-mbart-large-50                    |   0.453 |                       0.475 |                   0.53  |                    0.504 | 0.534                                     | 0.522      |\n| fr         | gpt2                                       |   0.323 |                       0.386 |                   0.4   |                    0.449 | 0.400                                     | 0.495      |\n| fr         | xlm-roberta-large                          |   0.434 |                       0.48  |                   0.502 |                    0.5   | 0.529                                     | 0.513      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.559 |                       0.592 |                   0.567 |                    0.603 | 0.581                                     | 0.636      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0.466 |                       0.572 |                   0.585 |                    0.593 | 0.582                                     | 0.636      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0.409 |                       0.48  |                   0.509 |                    0.496 | 0.518                                     | 0.592      |\n| ge         | bert-base-multilingual-cased               |   0.5   |                       0.568 |                   0.591 |                    0.616 | 0.583                                     | **0.654**  |\n| ge         | distilbert-base-multilingual-cased         |   0.516 |                       0.571 |                   0.542 |                    0.583 | 0.576                                     | 0.638      |\n| ge         | facebook-mbart-large-50                    |   0.585 |                       0.601 |                   0.566 |                    0.628 | 0.616                                     | 0.645      |\n| ge         | gpt2                                       |   0.475 |                       0.525 |                   0.485 |                    0.581 | 0.512                                     | 0.618      |\n| ge         | xlm-roberta-large                          |   0.565 |                       0.63  |                   0.587 |                    0.638 | 0.623                                     | 0.636      |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.57  |                       0.551 |                   0.552 |                    0.573 | 0.542                                     | 0.573      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.437 |                       0.538 |                   0.527 |                    0.537 | 0.541                                     | 0.575      |\n| it         | EleutherAI-gpt-neo-125M                    |   0.3   |                       0.324 |                   0.442 |                    0.475 | 0.459                                     | 0.509      |\n| it         | bert-base-multilingual-cased               |   0.479 |                       0.572 |                   0.566 |                    0.592 | 0.574                                     | 0.585      |\n| it         | distilbert-base-multilingual-cased         |   0.476 |                       0.545 |                   0.564 |                    0.589 | 0.535                                     | 0.589      |\n| it         | facebook-mbart-large-50                    |   0.533 |                       0.585 |                   0.59  |                    0.593 | 0.593                                     | **0.627**  |\n| it         | gpt2                                       |   0.397 |                       0.485 |                   0.513 |                    0.543 | 0.469                                     | 0.525      |\n| it         | xlm-roberta-large                          |   0.532 |                       0.587 |                   0.602 |                    0.598 | 0.570                                     | 0.573      |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.507 |                       0.599 |                   0.617 |                    0.607 | 0.623                                     | 0.607      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.464 |                       0.515 |                   0.618 |                    0.586 | 0.618                                     | 0.604      |\n| po         | EleutherAI-gpt-neo-125M                    |   0.453 |                       0.496 |                   0.516 |                    0.566 | 0.522                                     | 0.511      |\n| po         | bert-base-multilingual-cased               |   0.571 |                       0.62  |                   0.652 |                    0.654 | 0.646                                     | 0.624      |\n| po         | distilbert-base-multilingual-cased         |   0.558 |                       0.542 |                   0.592 |                    0.639 | 0.623                                     | 0.604      |\n| po         | facebook-mbart-large-50                    |   0.565 |                       0.604 |                   0.676 |                    0.665 | **0.680**                                 | 0.652      |\n| po         | gpt2                                       |   0.517 |                       0.538 |                   0.591 |                    0.621 | 0.568                                     | 0.590      |\n| po         | xlm-roberta-large                          |   0.569 |                       0.64  |                   0.634 |                    0.641 | 0.667                                     | 0.665      |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.419 |                       0.465 |                   0.474 |                    0.487 | 0.521                                     | **0.530**  |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.319 |                       0.286 |                   0.377 |                    0.387 | 0.409                                     | 0.374      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.162 |                       0.22  |                   0.24  |                    0.217 | 0.122                                     | 0.151      |\n| ru         | bert-base-multilingual-cased               |   0.366 |                       0.436 |                   0.516 |                    0.471 | 0.447                                     | 0.497      |\n| ru         | distilbert-base-multilingual-cased         |   0.318 |                       0.407 |                   0.47  |                    0.5   | 0.461                                     | 0.488      |\n| ru         | facebook-mbart-large-50                    |   0.426 |                       0.435 |                   0.511 |                    0.49  | 0.519                                     | 0.507      |\n| ru         | gpt2                                       |   0.159 |                       0.107 |                   0.075 |                    0.095 | 0.126                                     | 0.215      |\n| ru         | xlm-roberta-large                          |   0.403 |                       0.446 |                   0.479 |                    0.455 | 0.486                                     | 0.455      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title |   title and first paragraph | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|----------------------------:|:------------------------|:-------------------------|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.577 |                       0.638 | 0.611                   | 0.643                    | 0.643                                     | 0.621      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.538 |                       0.631 | 0.619                   | 0.604                    | 0.604                                     | 0.611      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.45  |                       0.543 | 0.562                   | 0.548                    | 0.555                                     | 0.562      |\n| en         | bert-base-multilingual-cased               |   0.543 |                       0.606 | 0.631                   | 0.650                    | 0.619                                     | 0.621      |\n| en         | distilbert-base-multilingual-cased         |   0.528 |                       0.592 | 0.579                   | 0.619                    | 0.589                                     | 0.623      |\n| en         | facebook-mbart-large-50                    |   0.599 |                       0.653 | 0.645                   | 0.653                    | **0.660**                                 | 0.643      |\n| en         | gpt2                                       |   0.548 |                       0.643 | 0.658                   | 0.638                    | 0.645                                     | 0.631      |\n| en         | xlm-roberta-large                          |   0.584 |                       0.636 | 0.636                   | 0.643                    | 0.648                                     | 0.621      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.357 |                       0.413 | 0.429                   | 0.405                    | 0.460                                     | 0.429      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.294 |                       0.357 | 0.373                   | 0.310                    | 0.349                                     | 0.405      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.175 |                       0.294 | 0.286                   | 0.294                    | 0.333                                     | 0.357      |\n| fr         | bert-base-multilingual-cased               |   0.349 |                       0.333 | 0.397                   | 0.460                    | 0.476                                     | 0.437      |\n| fr         | distilbert-base-multilingual-cased         |   0.302 |                       0.421 | 0.413                   | 0.476                    | 0.468                                     | **0.516**  |\n| fr         | facebook-mbart-large-50                    |   0.381 |                       0.413 | 0.452                   | 0.452                    | 0.492                                     | 0.468      |\n| fr         | gpt2                                       |   0.286 |                       0.341 | 0.341                   | 0.405                    | 0.333                                     | 0.421      |\n| fr         | xlm-roberta-large                          |   0.381 |                       0.429 | 0.444                   | 0.460                    | 0.476                                     | 0.460      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.465 |                       0.535 | 0.483                   | 0.494                    | 0.500                                     | 0.570      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0.355 |                       0.483 | 0.500                   | 0.471                    | 0.483                                     | 0.558      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0.331 |                       0.384 | 0.407                   | 0.401                    | 0.419                                     | 0.494      |\n| ge         | bert-base-multilingual-cased               |   0.424 |                       0.488 | 0.512                   | 0.547                    | 0.541                                     | **0.616**  |\n| ge         | distilbert-base-multilingual-cased         |   0.424 |                       0.517 | 0.471                   | 0.500                    | 0.494                                     | 0.599      |\n| ge         | facebook-mbart-large-50                    |   0.483 |                       0.547 | 0.488                   | 0.535                    | 0.541                                     | 0.564      |\n| ge         | gpt2                                       |   0.413 |                       0.453 | 0.413                   | 0.523                    | 0.419                                     | 0.570      |\n| ge         | xlm-roberta-large                          |   0.465 |                       0.57  | 0.500                   | 0.564                    | 0.552                                     | 0.564      |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.478 |                       0.474 | 0.474                   | 0.470                    | 0.435                                     | 0.478      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.33  |                       0.426 | 0.404                   | 0.426                    | 0.435                                     | 0.474      |\n| it         | EleutherAI-gpt-neo-125M                    |   0.243 |                       0.243 | 0.348                   | 0.400                    | 0.370                                     | 0.409      |\n| it         | bert-base-multilingual-cased               |   0.413 |                       0.483 | 0.491                   | 0.517                    | 0.496                                     | 0.496      |\n| it         | distilbert-base-multilingual-cased         |   0.409 |                       0.47  | 0.487                   | **0.526**                | 0.461                                     | 0.517      |\n| it         | facebook-mbart-large-50                    |   0.439 |                       0.487 | 0.483                   | 0.478                    | 0.500                                     | 0.522      |\n| it         | gpt2                                       |   0.335 |                       0.417 | 0.457                   | 0.491                    | 0.374                                     | 0.461      |\n| it         | xlm-roberta-large                          |   0.439 |                       0.491 | 0.496                   | 0.491                    | 0.470                                     | 0.461      |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.437 |                       0.549 | 0.563                   | 0.510                    | 0.529                                     | 0.524      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.393 |                       0.422 | 0.539                   | 0.481                    | 0.529                                     | 0.515      |\n| po         | EleutherAI-gpt-neo-125M                    |   0.388 |                       0.427 | 0.461                   | 0.519                    | 0.432                                     | 0.408      |\n| po         | bert-base-multilingual-cased               |   0.519 |                       0.563 | 0.587                   | 0.597                    | 0.568                                     | 0.563      |\n| po         | distilbert-base-multilingual-cased         |   0.49  |                       0.5   | 0.524                   | 0.558                    | 0.553                                     | 0.549      |\n| po         | facebook-mbart-large-50                    |   0.524 |                       0.544 | **0.602**               | 0.573                    | 0.597                                     | 0.573      |\n| po         | gpt2                                       |   0.481 |                       0.481 | 0.597                   | 0.568                    | 0.495                                     | 0.510      |\n| po         | xlm-roberta-large                          |   0.51  |                       0.587 | 0.573                   | 0.563                    | 0.587                                     | 0.592      |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.36  |                       0.419 | 0.419                   | 0.430                    | 0.442                                     | 0.465      |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.221 |                       0.198 | 0.267                   | 0.279                    | 0.302                                     | 0.267      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.105 |                       0.163 | 0.174                   | 0.174                    | 0.081                                     | 0.105      |\n| ru         | bert-base-multilingual-cased               |   0.302 |                       0.395 | 0.465                   | 0.419                    | 0.395                                     | 0.442      |\n| ru         | distilbert-base-multilingual-cased         |   0.244 |                       0.384 | 0.407                   | 0.465                    | 0.407                                     | **0.477**  |\n| ru         | facebook-mbart-large-50                    |   0.337 |                       0.349 | 0.419                   | 0.407                    | 0.407                                     | 0.430      |\n| ru         | gpt2                                       |   0.105 |                       0.07  | 0.047                   | 0.058                    | 0.081                                     | 0.151      |\n| ru         | xlm-roberta-large                          |   0.314 |                       0.36  | 0.395                   | 0.384                    | 0.419                                     | 0.384      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title |   title and first paragraph | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|----------------------------:|:------------------------|:-------------------------|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.717 |                       0.774 | 0.776                   | 0.765                    | 0.758                                     | 0.749      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.761 |                       0.794 | 0.824                   | 0.812                    | 0.810                                     | 0.796      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.646 |                       0.73  | 0.735                   | 0.752                    | 0.780                                     | 0.782      |\n| en         | bert-base-multilingual-cased               |   0.696 |                       0.765 | 0.787                   | 0.776                    | 0.760                                     | 0.772      |\n| en         | distilbert-base-multilingual-cased         |   0.684 |                       0.759 | 0.769                   | 0.735                    | 0.755                                     | 0.754      |\n| en         | facebook-mbart-large-50                    |   0.778 |                       0.795 | 0.812                   | 0.804                    | 0.799                                     | 0.771      |\n| en         | gpt2                                       |   0.709 |                       0.751 | 0.725                   | 0.711                    | 0.754                                     | 0.763      |\n| en         | xlm-roberta-large                          |   0.759 |                       0.765 | **0.825**               | 0.804                    | 0.784                                     | 0.770      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.455 |                       0.52  | 0.562                   | 0.505                    | 0.580                                     | 0.574      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.536 |                       0.6   | 0.540                   | 0.542                    | 0.571                                     | 0.600      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.324 |                       0.463 | 0.480                   | 0.440                    | 0.545                                     | **0.643**  |\n| fr         | bert-base-multilingual-cased               |   0.524 |                       0.538 | 0.495                   | 0.532                    | 0.566                                     | 0.579      |\n| fr         | distilbert-base-multilingual-cased         |   0.481 |                       0.51  | 0.520                   | 0.545                    | 0.615                                     | 0.575      |\n| fr         | facebook-mbart-large-50                    |   0.558 |                       0.559 | 0.640                   | 0.570                    | 0.585                                     | 0.590      |\n| fr         | gpt2                                       |   0.371 |                       0.443 | 0.483                   | 0.505                    | 0.500                                     | 0.602      |\n| fr         | xlm-roberta-large                          |   0.505 |                       0.545 | 0.577                   | 0.547                    | 0.594                                     | 0.580      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.702 |                       0.662 | 0.686                   | 0.773                    | 0.694                                     | 0.721      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0.678 |                       0.703 | 0.705                   | **0.802**                | 0.735                                     | 0.738      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0.533 |                       0.641 | 0.680                   | 0.651                    | 0.679                                     | 0.739      |\n| ge         | bert-base-multilingual-cased               |   0.608 |                       0.677 | 0.698                   | 0.707                    | 0.633                                     | 0.697      |\n| ge         | distilbert-base-multilingual-cased         |   0.658 |                       0.636 | 0.638                   | 0.699                    | 0.691                                     | 0.682      |\n| ge         | facebook-mbart-large-50                    |   0.741 |                       0.667 | 0.672                   | 0.760                    | 0.715                                     | 0.752      |\n| ge         | gpt2                                       |   0.559 |                       0.624 | 0.587                   | 0.652                    | 0.661                                     | 0.676      |\n| ge         | xlm-roberta-large                          |   0.721 |                       0.705 | 0.711                   | 0.735                    | 0.714                                     | 0.729      |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.705 |                       0.657 | 0.661                   | 0.735                    | 0.719                                     | 0.714      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.644 |                       0.731 | 0.756                   | 0.726                    | 0.714                                     | 0.732      |\n| it         | EleutherAI-gpt-neo-125M                    |   0.392 |                       0.483 | 0.606                   | 0.586                    | 0.607                                     | 0.676      |\n| it         | bert-base-multilingual-cased               |   0.569 |                       0.703 | 0.669                   | 0.692                    | 0.683                                     | 0.713      |\n| it         | distilbert-base-multilingual-cased         |   0.57  |                       0.651 | 0.671                   | 0.669                    | 0.639                                     | 0.684      |\n| it         | facebook-mbart-large-50                    |   0.678 |                       0.732 | 0.760                   | 0.780                    | 0.728                                     | **0.784**  |\n| it         | gpt2                                       |   0.487 |                       0.578 | 0.587                   | 0.608                    | 0.628                                     | 0.609      |\n| it         | xlm-roberta-large                          |   0.673 |                       0.729 | 0.765                   | 0.764                    | 0.725                                     | 0.757      |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.604 |                       0.661 | 0.682                   | 0.750                    | 0.757                                     | 0.720      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.566 |                       0.659 | 0.725                   | 0.750                    | 0.741                                     | 0.731      |\n| po         | EleutherAI-gpt-neo-125M                    |   0.544 |                       0.591 | 0.586                   | 0.622                    | 0.659                                     | 0.683      |\n| po         | bert-base-multilingual-cased               |   0.633 |                       0.69  | 0.733                   | 0.724                    | 0.750                                     | 0.699      |\n| po         | distilbert-base-multilingual-cased         |   0.647 |                       0.592 | 0.679                   | 0.747                    | 0.713                                     | 0.673      |\n| po         | facebook-mbart-large-50                    |   0.614 |                       0.679 | 0.770                   | **0.792**                | 0.788                                     | 0.756      |\n| po         | gpt2                                       |   0.559 |                       0.611 | 0.586                   | 0.684                    | 0.667                                     | 0.700      |\n| po         | xlm-roberta-large                          |   0.644 |                       0.703 | 0.711                   | 0.744                    | 0.771                                     | 0.758      |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.5   |                       0.522 | 0.545                   | 0.561                    | 0.633                                     | 0.615      |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.576 |                       0.515 | 0.639                   | 0.632                    | 0.634                                     | 0.622      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.36  |                       0.341 | 0.385                   | 0.288                    | 0.241                                     | 0.273      |\n| ru         | bert-base-multilingual-cased               |   0.464 |                       0.486 | 0.580                   | 0.537                    | 0.515                                     | 0.567      |\n| ru         | distilbert-base-multilingual-cased         |   0.457 |                       0.434 | 0.556                   | 0.541                    | 0.530                                     | 0.500      |\n| ru         | facebook-mbart-large-50                    |   0.58  |                       0.577 | 0.655                   | 0.614                    | **0.714**                                 | 0.617      |\n| ru         | gpt2                                       |   0.333 |                       0.231 | 0.190                   | 0.263                    | 0.280                                     | 0.371      |\n| ru         | xlm-roberta-large                          |   0.562 |                       0.585 | 0.607                   | 0.559                    | 0.581                                     | 0.559      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                                 |   title | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-------------------------------------------|--------:|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:-----------|\n| en         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.087 | 0.136                       | 0.126                   | 0.068                    | 0.097                                     | 0.058      |\n| en         | EleutherAI-gpt-neo-1.3B                    |   0.087 | 0.087                       | **0.155**               | 0.097                    | 0.107                                     | 0.087      |\n| en         | EleutherAI-gpt-neo-125M                    |   0.019 | 0.087                       | 0.058                   | 0.039                    | 0.078                                     | 0.107      |\n| en         | bert-base-multilingual-cased               |   0.078 | 0.087                       | 0.117                   | 0.078                    | 0.078                                     | 0.117      |\n| en         | distilbert-base-multilingual-cased         |   0.097 | 0.078                       | 0.126                   | 0.078                    | 0.117                                     | 0.117      |\n| en         | facebook-mbart-large-50                    |   0.117 | 0.126                       | 0.146                   | 0.107                    | 0.126                                     | 0.126      |\n| en         | gpt2                                       |   0.097 | 0.097                       | 0.117                   | 0.087                    | 0.087                                     | 0.117      |\n| en         | xlm-roberta-large                          |   0.058 | 0.087                       | 0.146                   | 0.126                    | 0.107                                     | 0.107      |\n| fr         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.071 | 0.071                       | 0.071                   | 0.071                    | **0.119**                                 | 0.095      |\n| fr         | EleutherAI-gpt-neo-1.3B                    |   0.024 | **0.119**                   | 0.024                   | 0.071                    | 0.024                                     | 0.071      |\n| fr         | EleutherAI-gpt-neo-125M                    |   0.024 | 0.024                       | 0.024                   | 0.024                    | 0.024                                     | 0.071      |\n| fr         | bert-base-multilingual-cased               |   0.048 | 0.071                       | 0.048                   | 0.071                    | 0.071                                     | 0.095      |\n| fr         | distilbert-base-multilingual-cased         |   0.071 | 0.071                       | 0.095                   | 0.048                    | 0.095                                     | 0.095      |\n| fr         | facebook-mbart-large-50                    |   0.024 | 0.095                       | 0.071                   | 0.048                    | 0.071                                     | 0.071      |\n| fr         | gpt2                                       |   0     | 0.024                       | 0.048                   | 0.000                    | 0.048                                     | 0.048      |\n| fr         | xlm-roberta-large                          |   0     | **0.119**                   | 0.071                   | 0.095                    | 0.095                                     | 0.048      |\n| ge         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.029 | 0.029                       | 0.029                   | 0.057                    | 0.057                                     | 0.029      |\n| ge         | EleutherAI-gpt-neo-1.3B                    |   0     | 0.029                       | 0.000                   | 0.057                    | 0.029                                     | 0.029      |\n| ge         | EleutherAI-gpt-neo-125M                    |   0     | 0.000                       | 0.029                   | 0.000                    | 0.029                                     | 0.029      |\n| ge         | bert-base-multilingual-cased               |   0     | 0.029                       | 0.029                   | 0.057                    | 0.000                                     | 0.029      |\n| ge         | distilbert-base-multilingual-cased         |   0.029 | 0.029                       | 0.000                   | 0.000                    | 0.029                                     | 0.000      |\n| ge         | facebook-mbart-large-50                    |   0.057 | 0.029                       | 0.029                   | 0.029                    | 0.057                                     | 0.000      |\n| ge         | gpt2                                       |   0     | 0.000                       | 0.029                   | **0.086**                | 0.000                                     | 0.000      |\n| ge         | xlm-roberta-large                          |   0.029 | 0.057                       | 0.029                   | 0.000                    | 0.057                                     | 0.029      |\n| it         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.1   | 0.150                       | 0.167                   | 0.133                    | 0.100                                     | 0.117      |\n| it         | EleutherAI-gpt-neo-1.3B                    |   0.05  | 0.117                       | 0.183                   | 0.150                    | 0.133                                     | 0.167      |\n| it         | EleutherAI-gpt-neo-125M                    |   0     | 0.000                       | 0.033                   | 0.067                    | 0.033                                     | 0.117      |\n| it         | bert-base-multilingual-cased               |   0.017 | 0.117                       | 0.067                   | 0.183                    | 0.117                                     | 0.133      |\n| it         | distilbert-base-multilingual-cased         |   0.017 | 0.117                       | 0.117                   | 0.150                    | 0.083                                     | 0.100      |\n| it         | facebook-mbart-large-50                    |   0.1   | 0.167                       | 0.150                   | 0.167                    | 0.167                                     | **0.217**  |\n| it         | gpt2                                       |   0.033 | 0.033                       | 0.033                   | 0.067                    | 0.083                                     | 0.067      |\n| it         | xlm-roberta-large                          |   0.1   | 0.167                       | 0.183                   | 0.133                    | 0.150                                     | 0.183      |\n| po         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.026 | 0.026                       | 0.026                   | 0.051                    | 0.051                                     | 0.051      |\n| po         | EleutherAI-gpt-neo-1.3B                    |   0.026 | 0.000                       | 0.051                   | 0.051                    | 0.026                                     | 0.051      |\n| po         | EleutherAI-gpt-neo-125M                    |   0     | 0.000                       | 0.051                   | 0.026                    | 0.026                                     | 0.026      |\n| po         | bert-base-multilingual-cased               |   0.026 | 0.051                       | 0.051                   | 0.051                    | 0.051                                     | 0.051      |\n| po         | distilbert-base-multilingual-cased         |   0.051 | 0.000                       | 0.026                   | 0.051                    | 0.051                                     | 0.026      |\n| po         | facebook-mbart-large-50                    |   0.026 | 0.000                       | 0.026                   | **0.077**                | 0.026                                     | 0.026      |\n| po         | gpt2                                       |   0     | 0.000                       | 0.000                   | 0.026                    | 0.026                                     | 0.026      |\n| po         | xlm-roberta-large                          |   0.051 | 0.026                       | 0.026                   | 0.051                    | 0.026                                     | 0.026      |\n| ru         | AshtonIsNotHere-xlm-roberta-long-base-4096 |   0.158 | 0.158                       | 0.132                   | 0.079                    | **0.263**                                 | 0.211      |\n| ru         | EleutherAI-gpt-neo-1.3B                    |   0.053 | 0.105                       | 0.079                   | 0.158                    | 0.105                                     | 0.132      |\n| ru         | EleutherAI-gpt-neo-125M                    |   0.026 | 0.000                       | 0.053                   | 0.000                    | 0.026                                     | 0.026      |\n| ru         | bert-base-multilingual-cased               |   0.079 | 0.105                       | 0.237                   | 0.158                    | 0.158                                     | 0.237      |\n| ru         | distilbert-base-multilingual-cased         |   0.105 | 0.132                       | 0.158                   | 0.158                    | 0.184                                     | 0.184      |\n| ru         | facebook-mbart-large-50                    |   0.158 | 0.211                       | 0.211                   | 0.184                    | 0.184                                     | 0.184      |\n| ru         | gpt2                                       |   0     | 0.000                       | 0.000                   | 0.026                    | 0.053                                     | 0.026      |\n| ru         | xlm-roberta-large                          |   0.158 | 0.184                       | 0.211                   | 0.105                    | 0.184                                     | 0.158      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25445/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    }
   ],
   "source": [
    "display_metrics_and_write_to_file(df=results_majority_vote_pred_df, grouping_criterion=['model_name'], output_dir='per_model_name_tables_majority_voting')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
