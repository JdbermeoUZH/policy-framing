\begin{tabular}{llllllll}
\toprule
language &                                 model\_name & title & title and first paragraph & title and 5 sentences & title and 10 sentences & title and first sentence each paragraph &  raw text \\
\midrule
      en & AshtonIsNotHere-xlm-roberta-long-base-4096 & 0.577 &                     0.638 &                 0.611 &                  0.643 &                               **0.667** &     0.653 \\
      en &                    EleutherAI-gpt-neo-1.3B & 0.538 &                     0.631 &                 0.619 &                  0.604 &                                   0.619 &     0.633 \\
      en &                    EleutherAI-gpt-neo-125M & 0.450 &                     0.543 &                 0.562 &                  0.548 &                                   0.582 &     0.599 \\
      en &               bert-base-multilingual-cased & 0.543 &                     0.606 &                 0.631 &                  0.650 &                                   0.628 &     0.655 \\
      en &         distilbert-base-multilingual-cased & 0.528 &                     0.592 &                 0.579 &                  0.619 &                                   0.609 &     0.655 \\
      en &                    facebook-mbart-large-50 & 0.599 &                     0.653 &                 0.645 &                  0.653 &                               **0.667** & **0.667** \\
      en &                                       gpt2 & 0.548 &                     0.643 &                 0.658 &                  0.638 &                                   0.653 &     0.653 \\
      en &                          xlm-roberta-large & 0.584 &                     0.636 &                 0.636 &                  0.643 &                                   0.663 &     0.653 \\
      fr & AshtonIsNotHere-xlm-roberta-long-base-4096 & 0.357 &                     0.413 &                 0.429 &                  0.405 &                                   0.468 &     0.460 \\
      fr &                    EleutherAI-gpt-neo-1.3B & 0.294 &                     0.357 &                 0.373 &                  0.310 &                                   0.381 &     0.429 \\
      fr &                    EleutherAI-gpt-neo-125M & 0.175 &                     0.294 &                 0.286 &                  0.294 &                                   0.341 &     0.389 \\
      fr &               bert-base-multilingual-cased & 0.349 &                     0.333 &                 0.397 &                  0.460 &                                   0.484 &     0.540 \\
      fr &         distilbert-base-multilingual-cased & 0.302 &                     0.421 &                 0.413 &                  0.476 &                                   0.484 & **0.563** \\
      fr &                    facebook-mbart-large-50 & 0.381 &                     0.413 &                 0.452 &                  0.452 &                                   0.500 &     0.532 \\
      fr &                                       gpt2 & 0.286 &                     0.341 &                 0.341 &                  0.405 &                                   0.349 &     0.444 \\
      fr &                          xlm-roberta-large & 0.381 &                     0.429 &                 0.444 &                  0.460 &                                   0.476 &     0.532 \\
      ge & AshtonIsNotHere-xlm-roberta-long-base-4096 & 0.465 &                     0.535 &                 0.483 &                  0.494 &                                   0.529 &     0.599 \\
      ge &                    EleutherAI-gpt-neo-1.3B & 0.355 &                     0.483 &                 0.500 &                  0.471 &                                   0.512 &     0.587 \\
      ge &                    EleutherAI-gpt-neo-125M & 0.331 &                     0.384 &                 0.407 &                  0.401 &                                   0.442 &     0.535 \\
      ge &               bert-base-multilingual-cased & 0.424 &                     0.488 &                 0.512 &                  0.547 &                                   0.570 & **0.645** \\
      ge &         distilbert-base-multilingual-cased & 0.424 &                     0.517 &                 0.471 &                  0.500 &                                   0.517 & **0.645** \\
      ge &                    facebook-mbart-large-50 & 0.483 &                     0.547 &                 0.488 &                  0.535 &                                   0.552 &     0.599 \\
      ge &                                       gpt2 & 0.413 &                     0.453 &                 0.413 &                  0.523 &                                   0.442 &     0.587 \\
      ge &                          xlm-roberta-large & 0.465 &                     0.570 &                 0.500 &                  0.564 &                                   0.593 &     0.610 \\
      it & AshtonIsNotHere-xlm-roberta-long-base-4096 & 0.478 &                     0.474 &                 0.474 &                  0.470 &                                   0.452 &     0.526 \\
      it &                    EleutherAI-gpt-neo-1.3B & 0.330 &                     0.426 &                 0.404 &                  0.426 &                                   0.443 &     0.526 \\
      it &                    EleutherAI-gpt-neo-125M & 0.243 &                     0.243 &                 0.348 &                  0.400 &                                   0.378 &     0.465 \\
      it &               bert-base-multilingual-cased & 0.413 &                     0.483 &                 0.491 &                  0.517 &                                   0.504 &     0.548 \\
      it &         distilbert-base-multilingual-cased & 0.409 &                     0.470 &                 0.487 &                  0.526 &                                   0.470 & **0.578** \\
      it &                    facebook-mbart-large-50 & 0.439 &                     0.487 &                 0.483 &                  0.478 &                                   0.526 &     0.565 \\
      it &                                       gpt2 & 0.335 &                     0.417 &                 0.457 &                  0.491 &                                   0.387 &     0.496 \\
      it &                          xlm-roberta-large & 0.439 &                     0.491 &                 0.496 &                  0.491 &                                   0.491 &     0.543 \\
      po & AshtonIsNotHere-xlm-roberta-long-base-4096 & 0.437 &                     0.549 &                 0.563 &                  0.510 &                                   0.544 &     0.573 \\
      po &                    EleutherAI-gpt-neo-1.3B & 0.393 &                     0.422 &                 0.539 &                  0.481 &                                   0.539 &     0.553 \\
      po &                    EleutherAI-gpt-neo-125M & 0.388 &                     0.427 &                 0.461 &                  0.519 &                                   0.471 &     0.471 \\
      po &               bert-base-multilingual-cased & 0.519 &                     0.563 &                 0.587 &                  0.597 &                                   0.597 &     0.636 \\
      po &         distilbert-base-multilingual-cased & 0.490 &                     0.500 &                 0.524 &                  0.558 &                                   0.568 &     0.621 \\
      po &                    facebook-mbart-large-50 & 0.524 &                     0.544 &                 0.602 &                  0.573 &                                   0.607 &     0.617 \\
      po &                                       gpt2 & 0.481 &                     0.481 &                 0.597 &                  0.568 &                                   0.510 &     0.617 \\
      po &                          xlm-roberta-large & 0.510 &                     0.587 &                 0.573 &                  0.563 &                                   0.626 & **0.650** \\
      ru & AshtonIsNotHere-xlm-roberta-long-base-4096 & 0.360 &                     0.419 &                 0.419 &                  0.430 &                                   0.442 &     0.488 \\
      ru &                    EleutherAI-gpt-neo-1.3B & 0.221 &                     0.198 &                 0.267 &                  0.279 &                                   0.314 &     0.279 \\
      ru &                    EleutherAI-gpt-neo-125M & 0.105 &                     0.163 &                 0.174 &                  0.174 &                                   0.093 &     0.105 \\
      ru &               bert-base-multilingual-cased & 0.302 &                     0.395 &                 0.465 &                  0.419 &                                   0.419 &     0.442 \\
      ru &         distilbert-base-multilingual-cased & 0.244 &                     0.384 &                 0.407 &                  0.465 &                                   0.419 & **0.523** \\
      ru &                    facebook-mbart-large-50 & 0.337 &                     0.349 &                 0.419 &                  0.407 &                                   0.419 &     0.465 \\
      ru &                                       gpt2 & 0.105 &                     0.070 &                 0.047 &                  0.058 &                                   0.093 &     0.163 \\
      ru &                          xlm-roberta-large & 0.314 &                     0.360 &                 0.395 &                  0.384 &                                   0.442 &     0.384 \\
\bottomrule
\end{tabular}
