{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7fd1c8-16ff-43f2-a049-a27d37ea2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3de210-17ae-42af-a361-fff5a0f66735",
   "metadata": {},
   "source": [
    "# Group types of models (experiment type and model type) and pick best performing in terms of f1-score per unit of analysis and report them in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd704f0c-6f9a-47d2-b05b-150230e363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filepaths = glob.glob('./logged_performance_per_model/*/*raw*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['./logged_performance_per_model/distilbert-base-multilingual-cased/truncated_raw_truncated_single_instance_distilbert-base-multilingual-cased-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/truncated_raw_truncated_single_instance_distilbert-base-multilingual-cased-raw_text_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/truncated_raw_truncated_single_instance_distilbert-base-multilingual-cased-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/truncated_raw_truncated_single_instance_distilbert-base-multilingual-cased-title_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/truncated_raw_truncated_single_instance_distilbert-base-multilingual-cased-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/truncated_raw_truncated_single_instance_distilbert-base-multilingual-cased-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/truncated_raw_truncated_single_instance_bert-base-multilingual-cased-raw_text_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/truncated_raw_truncated_single_instance_bert-base-multilingual-cased-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/truncated_raw_truncated_single_instance_bert-base-multilingual-cased-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/truncated_raw_truncated_single_instance_bert-base-multilingual-cased-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/truncated_raw_truncated_single_instance_bert-base-multilingual-cased-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/truncated_raw_truncated_single_instance_bert-base-multilingual-cased-title_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-1.3B-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-1.3B-title_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-1.3B-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-1.3B-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-1.3B-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-1.3B-raw_text_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/truncated_raw_truncated_single_instance_facebook_mbart-large-50-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/truncated_raw_truncated_single_instance_facebook_mbart-large-50-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/truncated_raw_truncated_single_instance_facebook_mbart-large-50-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/truncated_raw_truncated_single_instance_facebook_mbart-large-50-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/truncated_raw_truncated_single_instance_facebook_mbart-large-50-raw_text_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/truncated_raw_truncated_single_instance_facebook_mbart-large-50-title_metrics.csv',\n './logged_performance_per_model/gpt2/truncated_raw_truncated_single_instance_gpt2-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/gpt2/truncated_raw_truncated_single_instance_gpt2-title_metrics.csv',\n './logged_performance_per_model/gpt2/truncated_raw_truncated_single_instance_gpt2-raw_text_metrics.csv',\n './logged_performance_per_model/gpt2/truncated_raw_truncated_single_instance_gpt2-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/gpt2/truncated_raw_truncated_single_instance_gpt2-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/gpt2/truncated_raw_truncated_single_instance_gpt2-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-125M-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-125M-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-125M-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-125M-title_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-125M-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/truncated_raw_truncated_single_instance_EleutherAI_gpt-neo-125M-raw_text_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/truncated_raw_truncated_single_instance_xlm-roberta-large-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/truncated_raw_truncated_single_instance_xlm-roberta-large-raw_text_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/truncated_raw_truncated_single_instance_xlm-roberta-large-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/truncated_raw_truncated_single_instance_xlm-roberta-large-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/truncated_raw_truncated_single_instance_xlm-roberta-large-title_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/truncated_raw_truncated_single_instance_xlm-roberta-large-title_and_5_sentences_metrics.csv']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filepaths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                          f1_micro  \\\nlanguage model_name              unit_of_analysis                                    \nen       EleutherAI-gpt-neo-1.3B raw_text                                 0.693042   \n                                 title                                    0.646067   \n                                 title_and_10_sentences                   0.682451   \n                                 title_and_5_sentences                    0.668524   \n                                 title_and_first_paragraph                0.686981   \n...                                                                            ...   \nru       xlm-roberta-large       title                                    0.463768   \n                                 title_and_10_sentences                   0.533333   \n                                 title_and_5_sentences                    0.463768   \n                                 title_and_first_paragraph                0.488889   \n                                 title_and_first_sentence_each_paragraph  0.520548   \n\n                                                                          precision_micro  \\\nlanguage model_name              unit_of_analysis                                           \nen       EleutherAI-gpt-neo-1.3B raw_text                                        0.783951   \n                                 title                                           0.759076   \n                                 title_and_10_sentences                          0.792880   \n                                 title_and_5_sentences                           0.776699   \n                                 title_and_first_paragraph                       0.792332   \n...                                                                                   ...   \nru       xlm-roberta-large       title                                           0.615385   \n                                 title_and_10_sentences                          0.625000   \n                                 title_and_5_sentences                           0.615385   \n                                 title_and_first_paragraph                       0.673469   \n                                 title_and_first_sentence_each_paragraph         0.633333   \n\n                                                                          recall_micro  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.621027   \n                                 title                                        0.562347   \n                                 title_and_10_sentences                       0.599022   \n                                 title_and_5_sentences                        0.586797   \n                                 title_and_first_paragraph                    0.606357   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.372093   \n                                 title_and_10_sentences                       0.465116   \n                                 title_and_5_sentences                        0.372093   \n                                 title_and_first_paragraph                    0.383721   \n                                 title_and_first_sentence_each_paragraph      0.441860   \n\n                                                                          f1_macro  \\\nlanguage model_name              unit_of_analysis                                    \nen       EleutherAI-gpt-neo-1.3B raw_text                                 0.507779   \n                                 title                                    0.443437   \n                                 title_and_10_sentences                   0.498518   \n                                 title_and_5_sentences                    0.451412   \n                                 title_and_first_paragraph                0.493932   \n...                                                                            ...   \nru       xlm-roberta-large       title                                    0.304746   \n                                 title_and_10_sentences                   0.436164   \n                                 title_and_5_sentences                    0.311980   \n                                 title_and_first_paragraph                0.383344   \n                                 title_and_first_sentence_each_paragraph  0.388683   \n\n                                                                          precision_macro  \\\nlanguage model_name              unit_of_analysis                                           \nen       EleutherAI-gpt-neo-1.3B raw_text                                        0.652437   \n                                 title                                           0.575110   \n                                 title_and_10_sentences                          0.665394   \n                                 title_and_5_sentences                           0.578119   \n                                 title_and_first_paragraph                       0.662291   \n...                                                                                   ...   \nru       xlm-roberta-large       title                                           0.346726   \n                                 title_and_10_sentences                          0.511738   \n                                 title_and_5_sentences                           0.356132   \n                                 title_and_first_paragraph                       0.525935   \n                                 title_and_first_sentence_each_paragraph         0.440306   \n\n                                                                          recall_macro  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.445927   \n                                 title                                        0.388387   \n                                 title_and_10_sentences                       0.425943   \n                                 title_and_5_sentences                        0.391092   \n                                 title_and_first_paragraph                    0.425312   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.293027   \n                                 title_and_10_sentences                       0.409099   \n                                 title_and_5_sentences                        0.300425   \n                                 title_and_first_paragraph                    0.341752   \n                                 title_and_first_sentence_each_paragraph      0.365986   \n\n                                                                          accuracy  \nlanguage model_name              unit_of_analysis                                   \nen       EleutherAI-gpt-neo-1.3B raw_text                                 0.097087  \n                                 title                                    0.058252  \n                                 title_and_10_sentences                   0.077670  \n                                 title_and_5_sentences                    0.116505  \n                                 title_and_first_paragraph                0.135922  \n...                                                                            ...  \nru       xlm-roberta-large       title                                    0.236842  \n                                 title_and_10_sentences                   0.210526  \n                                 title_and_5_sentences                    0.210526  \n                                 title_and_first_paragraph                0.210526  \n                                 title_and_first_sentence_each_paragraph  0.210526  \n\n[252 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>f1_micro</th>\n      <th>precision_micro</th>\n      <th>recall_micro</th>\n      <th>f1_macro</th>\n      <th>precision_macro</th>\n      <th>recall_macro</th>\n      <th>accuracy</th>\n    </tr>\n    <tr>\n      <th>language</th>\n      <th>model_name</th>\n      <th>unit_of_analysis</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">en</th>\n      <th rowspan=\"5\" valign=\"top\">EleutherAI-gpt-neo-1.3B</th>\n      <th>raw_text</th>\n      <td>0.693042</td>\n      <td>0.783951</td>\n      <td>0.621027</td>\n      <td>0.507779</td>\n      <td>0.652437</td>\n      <td>0.445927</td>\n      <td>0.097087</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>0.646067</td>\n      <td>0.759076</td>\n      <td>0.562347</td>\n      <td>0.443437</td>\n      <td>0.575110</td>\n      <td>0.388387</td>\n      <td>0.058252</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.682451</td>\n      <td>0.792880</td>\n      <td>0.599022</td>\n      <td>0.498518</td>\n      <td>0.665394</td>\n      <td>0.425943</td>\n      <td>0.077670</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.668524</td>\n      <td>0.776699</td>\n      <td>0.586797</td>\n      <td>0.451412</td>\n      <td>0.578119</td>\n      <td>0.391092</td>\n      <td>0.116505</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.686981</td>\n      <td>0.792332</td>\n      <td>0.606357</td>\n      <td>0.493932</td>\n      <td>0.662291</td>\n      <td>0.425312</td>\n      <td>0.135922</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">ru</th>\n      <th rowspan=\"5\" valign=\"top\">xlm-roberta-large</th>\n      <th>title</th>\n      <td>0.463768</td>\n      <td>0.615385</td>\n      <td>0.372093</td>\n      <td>0.304746</td>\n      <td>0.346726</td>\n      <td>0.293027</td>\n      <td>0.236842</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.533333</td>\n      <td>0.625000</td>\n      <td>0.465116</td>\n      <td>0.436164</td>\n      <td>0.511738</td>\n      <td>0.409099</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.463768</td>\n      <td>0.615385</td>\n      <td>0.372093</td>\n      <td>0.311980</td>\n      <td>0.356132</td>\n      <td>0.300425</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.488889</td>\n      <td>0.673469</td>\n      <td>0.383721</td>\n      <td>0.383344</td>\n      <td>0.525935</td>\n      <td>0.341752</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>title_and_first_sentence_each_paragraph</th>\n      <td>0.520548</td>\n      <td>0.633333</td>\n      <td>0.441860</td>\n      <td>0.388683</td>\n      <td>0.440306</td>\n      <td>0.365986</td>\n      <td>0.210526</td>\n    </tr>\n  </tbody>\n</table>\n<p>252 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_list = []\n",
    "for results_filepath in results_filepaths:\n",
    "    model_name = results_filepath.split('/')[-2]\n",
    "    results_df_i = pd.read_csv(results_filepath)\n",
    "    results_df_i['model_name'] = model_name\n",
    "    dfs_list.append(results_df_i)\n",
    "\n",
    "results_df = pd.concat(dfs_list).set_index(['language', 'model_name', 'unit_of_analysis']).sort_index()\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "results_df.to_csv('performance_of_models.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6a452c68-3d2e-43a1-b820-a7688df2cd33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the tables to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c97b164-c359-40ea-b974-a0dc65c1db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_table(df, metric, index_cols=['model_name'], display_=True):\n",
    "    report_table = df.reset_index().copy()\n",
    "    report_table['result'] = report_table[f'{metric}'].map(lambda x: f'{x:.3f}')\n",
    "    report_table['col_title'] = report_table.unit_of_analysis.str.split('_').str.join(' ') \n",
    "    report_table['col_title'] = pd.Categorical(\n",
    "        report_table.col_title,\n",
    "        categories=['title', 'title and first paragraph', 'title and 5 sentences', 'title and 10 sentences',\n",
    "                    'title and first sentence each paragraph', 'raw text'],\n",
    "        ordered=True)\n",
    "    report_table = report_table[index_cols + ['col_title', 'result']]\\\n",
    "        .pivot_table(index=index_cols, columns=['col_title'], values=['result'], aggfunc='first', fill_value=0)\\\n",
    "        .droplevel(0, axis=1)\n",
    "\n",
    "    report_table.columns.names = [None]\n",
    "\n",
    "    # Highlight best scoring models according to their average\n",
    "    mean_perf_arr = report_table.applymap(lambda x: float(str(x).split(' ')[0])).to_numpy()\n",
    "    highlight_mask = mean_perf_arr == mean_perf_arr.max()\n",
    "    report_table_arr = report_table.to_numpy()  # Note it passes the array by reference\n",
    "    report_table_arr[highlight_mask] = '**' + report_table_arr[highlight_mask] + '**'\n",
    "\n",
    "    if display_:\n",
    "        display(Markdown(report_table.to_markdown()))\n",
    "    \n",
    "    return report_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.646 | 0.687                       |                   0.669 |                    0.682 |                                     0.709 |      0.693 |\n| EleutherAI-gpt-neo-125M            |   0.573 | 0.647                       |                   0.642 |                    0.636 |                                     0.649 |      0.631 |\n| bert-base-multilingual-cased       |   0.619 | 0.690                       |                   0.676 |                    0.689 |                                     0.688 |      0.711 |\n| distilbert-base-multilingual-cased |   0.592 | 0.662                       |                   0.685 |                    0.686 |                                     0.684 |      0.684 |\n| facebook-mbart-large-50            |   0.666 | **0.734**                   |                   0.731 |                    0.718 |                                     0.708 |      0.711 |\n| gpt2                               |   0.625 | 0.664                       |                   0.678 |                    0.66  |                                     0.68  |      0.654 |\n| xlm-roberta-large                  |   0.659 | 0.710                       |                   0.721 |                    0.71  |                                     0.709 |      0.7   |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                    title title and first paragraph  \\\nmodel_name                                                            \nEleutherAI-gpt-neo-1.3B             0.646                     0.687   \nEleutherAI-gpt-neo-125M             0.573                     0.647   \nbert-base-multilingual-cased        0.619                     0.690   \ndistilbert-base-multilingual-cased  0.592                     0.662   \nfacebook-mbart-large-50             0.666                 **0.734**   \ngpt2                                0.625                     0.664   \nxlm-roberta-large                   0.659                     0.710   \n\n                                   title and 5 sentences  \\\nmodel_name                                                 \nEleutherAI-gpt-neo-1.3B                            0.669   \nEleutherAI-gpt-neo-125M                            0.642   \nbert-base-multilingual-cased                       0.676   \ndistilbert-base-multilingual-cased                 0.685   \nfacebook-mbart-large-50                            0.731   \ngpt2                                               0.678   \nxlm-roberta-large                                  0.721   \n\n                                   title and 10 sentences  \\\nmodel_name                                                  \nEleutherAI-gpt-neo-1.3B                             0.682   \nEleutherAI-gpt-neo-125M                             0.636   \nbert-base-multilingual-cased                        0.689   \ndistilbert-base-multilingual-cased                  0.686   \nfacebook-mbart-large-50                             0.718   \ngpt2                                                0.660   \nxlm-roberta-large                                   0.710   \n\n                                   title and first sentence each paragraph  \\\nmodel_name                                                                   \nEleutherAI-gpt-neo-1.3B                                              0.709   \nEleutherAI-gpt-neo-125M                                              0.649   \nbert-base-multilingual-cased                                         0.688   \ndistilbert-base-multilingual-cased                                   0.684   \nfacebook-mbart-large-50                                              0.708   \ngpt2                                                                 0.680   \nxlm-roberta-large                                                    0.709   \n\n                                   raw text  \nmodel_name                                   \nEleutherAI-gpt-neo-1.3B               0.693  \nEleutherAI-gpt-neo-125M               0.631  \nbert-base-multilingual-cased          0.711  \ndistilbert-base-multilingual-cased    0.684  \nfacebook-mbart-large-50               0.711  \ngpt2                                  0.654  \nxlm-roberta-large                     0.700  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>title and first paragraph</th>\n      <th>title and 5 sentences</th>\n      <th>title and 10 sentences</th>\n      <th>title and first sentence each paragraph</th>\n      <th>raw text</th>\n    </tr>\n    <tr>\n      <th>model_name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>EleutherAI-gpt-neo-1.3B</th>\n      <td>0.646</td>\n      <td>0.687</td>\n      <td>0.669</td>\n      <td>0.682</td>\n      <td>0.709</td>\n      <td>0.693</td>\n    </tr>\n    <tr>\n      <th>EleutherAI-gpt-neo-125M</th>\n      <td>0.573</td>\n      <td>0.647</td>\n      <td>0.642</td>\n      <td>0.636</td>\n      <td>0.649</td>\n      <td>0.631</td>\n    </tr>\n    <tr>\n      <th>bert-base-multilingual-cased</th>\n      <td>0.619</td>\n      <td>0.690</td>\n      <td>0.676</td>\n      <td>0.689</td>\n      <td>0.688</td>\n      <td>0.711</td>\n    </tr>\n    <tr>\n      <th>distilbert-base-multilingual-cased</th>\n      <td>0.592</td>\n      <td>0.662</td>\n      <td>0.685</td>\n      <td>0.686</td>\n      <td>0.684</td>\n      <td>0.684</td>\n    </tr>\n    <tr>\n      <th>facebook-mbart-large-50</th>\n      <td>0.666</td>\n      <td>**0.734**</td>\n      <td>0.731</td>\n      <td>0.718</td>\n      <td>0.708</td>\n      <td>0.711</td>\n    </tr>\n    <tr>\n      <th>gpt2</th>\n      <td>0.625</td>\n      <td>0.664</td>\n      <td>0.678</td>\n      <td>0.660</td>\n      <td>0.680</td>\n      <td>0.654</td>\n    </tr>\n    <tr>\n      <th>xlm-roberta-large</th>\n      <td>0.659</td>\n      <td>0.710</td>\n      <td>0.721</td>\n      <td>0.710</td>\n      <td>0.709</td>\n      <td>0.700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_performance_table(df=results_df.loc['en'], metric='f1_micro', index_cols=['model_name'], display_=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6f14e8da-1acf-4114-9cfd-eeaa97261f68",
   "metadata": {},
   "source": [
    "### Generate tables for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea51df3-87ed-4ace-ad56-25d235b5cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_report = ['f1_micro', 'recall_micro', 'precision_micro', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1650fa17-5ccc-439a-9818-d61bf369ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'en': 'English', 'it': 'Italian', 'fr': 'French', 'po': 'Polish', 'ru': 'Russian', 'ge': 'German'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                          f1_micro  \\\nlanguage model_name              unit_of_analysis                                    \nen       EleutherAI-gpt-neo-1.3B raw_text                                 0.693042   \n                                 title                                    0.646067   \n                                 title_and_10_sentences                   0.682451   \n                                 title_and_5_sentences                    0.668524   \n                                 title_and_first_paragraph                0.686981   \n...                                                                            ...   \nru       xlm-roberta-large       title                                    0.463768   \n                                 title_and_10_sentences                   0.533333   \n                                 title_and_5_sentences                    0.463768   \n                                 title_and_first_paragraph                0.488889   \n                                 title_and_first_sentence_each_paragraph  0.520548   \n\n                                                                          precision_micro  \\\nlanguage model_name              unit_of_analysis                                           \nen       EleutherAI-gpt-neo-1.3B raw_text                                        0.783951   \n                                 title                                           0.759076   \n                                 title_and_10_sentences                          0.792880   \n                                 title_and_5_sentences                           0.776699   \n                                 title_and_first_paragraph                       0.792332   \n...                                                                                   ...   \nru       xlm-roberta-large       title                                           0.615385   \n                                 title_and_10_sentences                          0.625000   \n                                 title_and_5_sentences                           0.615385   \n                                 title_and_first_paragraph                       0.673469   \n                                 title_and_first_sentence_each_paragraph         0.633333   \n\n                                                                          recall_micro  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.621027   \n                                 title                                        0.562347   \n                                 title_and_10_sentences                       0.599022   \n                                 title_and_5_sentences                        0.586797   \n                                 title_and_first_paragraph                    0.606357   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.372093   \n                                 title_and_10_sentences                       0.465116   \n                                 title_and_5_sentences                        0.372093   \n                                 title_and_first_paragraph                    0.383721   \n                                 title_and_first_sentence_each_paragraph      0.441860   \n\n                                                                          f1_macro  \\\nlanguage model_name              unit_of_analysis                                    \nen       EleutherAI-gpt-neo-1.3B raw_text                                 0.507779   \n                                 title                                    0.443437   \n                                 title_and_10_sentences                   0.498518   \n                                 title_and_5_sentences                    0.451412   \n                                 title_and_first_paragraph                0.493932   \n...                                                                            ...   \nru       xlm-roberta-large       title                                    0.304746   \n                                 title_and_10_sentences                   0.436164   \n                                 title_and_5_sentences                    0.311980   \n                                 title_and_first_paragraph                0.383344   \n                                 title_and_first_sentence_each_paragraph  0.388683   \n\n                                                                          precision_macro  \\\nlanguage model_name              unit_of_analysis                                           \nen       EleutherAI-gpt-neo-1.3B raw_text                                        0.652437   \n                                 title                                           0.575110   \n                                 title_and_10_sentences                          0.665394   \n                                 title_and_5_sentences                           0.578119   \n                                 title_and_first_paragraph                       0.662291   \n...                                                                                   ...   \nru       xlm-roberta-large       title                                           0.346726   \n                                 title_and_10_sentences                          0.511738   \n                                 title_and_5_sentences                           0.356132   \n                                 title_and_first_paragraph                       0.525935   \n                                 title_and_first_sentence_each_paragraph         0.440306   \n\n                                                                          recall_macro  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.445927   \n                                 title                                        0.388387   \n                                 title_and_10_sentences                       0.425943   \n                                 title_and_5_sentences                        0.391092   \n                                 title_and_first_paragraph                    0.425312   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.293027   \n                                 title_and_10_sentences                       0.409099   \n                                 title_and_5_sentences                        0.300425   \n                                 title_and_first_paragraph                    0.341752   \n                                 title_and_first_sentence_each_paragraph      0.365986   \n\n                                                                          accuracy  \nlanguage model_name              unit_of_analysis                                   \nen       EleutherAI-gpt-neo-1.3B raw_text                                 0.097087  \n                                 title                                    0.058252  \n                                 title_and_10_sentences                   0.077670  \n                                 title_and_5_sentences                    0.116505  \n                                 title_and_first_paragraph                0.135922  \n...                                                                            ...  \nru       xlm-roberta-large       title                                    0.236842  \n                                 title_and_10_sentences                   0.210526  \n                                 title_and_5_sentences                    0.210526  \n                                 title_and_first_paragraph                0.210526  \n                                 title_and_first_sentence_each_paragraph  0.210526  \n\n[252 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>f1_micro</th>\n      <th>precision_micro</th>\n      <th>recall_micro</th>\n      <th>f1_macro</th>\n      <th>precision_macro</th>\n      <th>recall_macro</th>\n      <th>accuracy</th>\n    </tr>\n    <tr>\n      <th>language</th>\n      <th>model_name</th>\n      <th>unit_of_analysis</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">en</th>\n      <th rowspan=\"5\" valign=\"top\">EleutherAI-gpt-neo-1.3B</th>\n      <th>raw_text</th>\n      <td>0.693042</td>\n      <td>0.783951</td>\n      <td>0.621027</td>\n      <td>0.507779</td>\n      <td>0.652437</td>\n      <td>0.445927</td>\n      <td>0.097087</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>0.646067</td>\n      <td>0.759076</td>\n      <td>0.562347</td>\n      <td>0.443437</td>\n      <td>0.575110</td>\n      <td>0.388387</td>\n      <td>0.058252</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.682451</td>\n      <td>0.792880</td>\n      <td>0.599022</td>\n      <td>0.498518</td>\n      <td>0.665394</td>\n      <td>0.425943</td>\n      <td>0.077670</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.668524</td>\n      <td>0.776699</td>\n      <td>0.586797</td>\n      <td>0.451412</td>\n      <td>0.578119</td>\n      <td>0.391092</td>\n      <td>0.116505</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.686981</td>\n      <td>0.792332</td>\n      <td>0.606357</td>\n      <td>0.493932</td>\n      <td>0.662291</td>\n      <td>0.425312</td>\n      <td>0.135922</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">ru</th>\n      <th rowspan=\"5\" valign=\"top\">xlm-roberta-large</th>\n      <th>title</th>\n      <td>0.463768</td>\n      <td>0.615385</td>\n      <td>0.372093</td>\n      <td>0.304746</td>\n      <td>0.346726</td>\n      <td>0.293027</td>\n      <td>0.236842</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.533333</td>\n      <td>0.625000</td>\n      <td>0.465116</td>\n      <td>0.436164</td>\n      <td>0.511738</td>\n      <td>0.409099</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.463768</td>\n      <td>0.615385</td>\n      <td>0.372093</td>\n      <td>0.311980</td>\n      <td>0.356132</td>\n      <td>0.300425</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.488889</td>\n      <td>0.673469</td>\n      <td>0.383721</td>\n      <td>0.383344</td>\n      <td>0.525935</td>\n      <td>0.341752</td>\n      <td>0.210526</td>\n    </tr>\n    <tr>\n      <th>title_and_first_sentence_each_paragraph</th>\n      <td>0.520548</td>\n      <td>0.633333</td>\n      <td>0.441860</td>\n      <td>0.388683</td>\n      <td>0.440306</td>\n      <td>0.365986</td>\n      <td>0.210526</td>\n    </tr>\n  </tbody>\n</table>\n<p>252 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143ecba4-ffb9-45e9-869c-87d16caea017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_metrics_and_write_to_file(df, grouping_criterion, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    report_tables_dfs_dict = {metric: [] for metric in metrics_to_report}\n",
    "\n",
    "    for language, results_df in df.groupby(level=0):\n",
    "        display(Markdown(f'# {language_dict[language]}'))\n",
    "        \n",
    "        for metric in metrics_to_report:\n",
    "            os.makedirs(os.path.join(output_dir, metric), exist_ok=True)\n",
    "\n",
    "            output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "            output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "            output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "            os.makedirs(output_dir_markdown, exist_ok=True)\n",
    "            os.makedirs(output_dir_latex, exist_ok=True)\n",
    "            os.makedirs(output_dir_csv, exist_ok=True)\n",
    "\n",
    "            display(Markdown(f'## {metric}'))\n",
    "\n",
    "            report_table = display_performance_table(df=results_df, index_cols=grouping_criterion, metric=metric, display_=True)\n",
    "\n",
    "            # Export as markdown\n",
    "            markdown_file = open(os.path.join(output_dir_markdown, f\"{language_dict[language]}_{metric}.md\"), \"w\")\n",
    "            report_table.reset_index().to_markdown(markdown_file, index=False)\n",
    "            markdown_file.close()\n",
    "\n",
    "            # Export as latex table\n",
    "            latex_file = open(os.path.join(output_dir_latex, f\"{language_dict[language]}_{metric}.tex\"), \"w\")\n",
    "            report_table.reset_index().to_latex(latex_file, index=False)\n",
    "            latex_file.close()\n",
    "\n",
    "            # Export as csv\n",
    "            report_table.to_csv(os.path.join(output_dir_csv, f\"{language_dict[language]}_{metric}.csv\"))\n",
    "\n",
    "            # Stack all languages into single table\n",
    "            report_table['language'] = language\n",
    "            report_table = report_table.reset_index().set_index(['language'] + grouping_criterion)\n",
    "\n",
    "            report_tables_dfs_dict[metric].append(report_table)\n",
    "\n",
    "    # Report or store unified table\n",
    "    display(Markdown(f'# All 6 Languages'))\n",
    "    for metric in metrics_to_report:\n",
    "        display(Markdown(f'## {metric}'))\n",
    "        multi_language_report_table_metric = pd.concat(report_tables_dfs_dict[metric])\n",
    "        display(Markdown(multi_language_report_table_metric.reset_index().to_markdown(index=False)))\n",
    "\n",
    "        output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "        output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "        output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "        # Export as markdown\n",
    "        markdown_file = open(os.path.join(output_dir_markdown, f\"all_6_languages_{metric}.md\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_markdown(markdown_file, index=False)\n",
    "        markdown_file.close()\n",
    "\n",
    "        # Export as latex table\n",
    "        latex_file = open(os.path.join(output_dir_latex, f\"all_6_languages_{metric}.tex\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n",
    "        latex_file.close()\n",
    "\n",
    "        # Export as csv\n",
    "        multi_language_report_table_metric.to_csv(os.path.join(output_dir_csv, f\"all_6_languages_{metric}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d754-49ba-4c74-8220-2e144f624044",
   "metadata": {},
   "source": [
    "# Per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5abb824-f93a-44fd-8203-a0b84224b0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# English"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.646 | 0.687                       |                   0.669 |                    0.682 |                                     0.709 |      0.693 |\n| EleutherAI-gpt-neo-125M            |   0.573 | 0.647                       |                   0.642 |                    0.636 |                                     0.649 |      0.631 |\n| bert-base-multilingual-cased       |   0.619 | 0.690                       |                   0.676 |                    0.689 |                                     0.688 |      0.711 |\n| distilbert-base-multilingual-cased |   0.592 | 0.662                       |                   0.685 |                    0.686 |                                     0.684 |      0.684 |\n| facebook-mbart-large-50            |   0.666 | **0.734**                   |                   0.731 |                    0.718 |                                     0.708 |      0.711 |\n| gpt2                               |   0.625 | 0.664                       |                   0.678 |                    0.66  |                                     0.68  |      0.654 |\n| xlm-roberta-large                  |   0.659 | 0.710                       |                   0.721 |                    0.71  |                                     0.709 |      0.7   |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.562 | 0.606                       |                   0.587 |                    0.599 |                                     0.645 |      0.621 |\n| EleutherAI-gpt-neo-125M            |   0.491 | 0.572                       |                   0.567 |                    0.553 |                                     0.577 |      0.57  |\n| bert-base-multilingual-cased       |   0.545 | 0.626                       |                   0.599 |                    0.638 |                                     0.66  |      0.65  |\n| distilbert-base-multilingual-cased |   0.513 | 0.592                       |                   0.611 |                    0.621 |                                     0.623 |      0.614 |\n| facebook-mbart-large-50            |   0.587 | **0.680**                   |                   0.665 |                    0.643 |                                     0.65  |      0.655 |\n| gpt2                               |   0.565 | 0.621                       |                   0.655 |                    0.601 |                                     0.645 |      0.592 |\n| xlm-roberta-large                  |   0.579 | 0.636                       |                   0.645 |                    0.641 |                                     0.653 |      0.626 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.759 |                       0.792 | 0.777                   |                    0.793 |                                     0.786 |      0.784 |\n| EleutherAI-gpt-neo-125M            |   0.686 |                       0.745 | 0.739                   |                    0.748 |                                     0.742 |      0.706 |\n| bert-base-multilingual-cased       |   0.715 |                       0.769 | 0.775                   |                    0.748 |                                     0.718 |      0.785 |\n| distilbert-base-multilingual-cased |   0.7   |                       0.752 | 0.779                   |                    0.765 |                                     0.757 |      0.772 |\n| facebook-mbart-large-50            |   0.769 |                       0.797 | 0.812                   |                    0.812 |                                     0.778 |      0.777 |\n| gpt2                               |   0.7   |                       0.713 | 0.702                   |                    0.732 |                                     0.719 |      0.731 |\n| xlm-roberta-large                  |   0.765 |                       0.805 | **0.817**               |                    0.796 |                                     0.776 |      0.795 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph | title and 5 sentences   |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|----------------------------:|:------------------------|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.058 |                       0.136 | 0.117                   |                    0.078 |                                     0.146 |      0.097 |\n| EleutherAI-gpt-neo-125M            |   0.068 |                       0.087 | 0.107                   |                    0.097 |                                     0.078 |      0.019 |\n| bert-base-multilingual-cased       |   0.097 |                       0.136 | 0.126                   |                    0.097 |                                     0.126 |      0.117 |\n| distilbert-base-multilingual-cased |   0.087 |                       0.097 | 0.117                   |                    0.058 |                                     0.068 |      0.087 |\n| facebook-mbart-large-50            |   0.097 |                       0.126 | **0.155**               |                    0.126 |                                     0.136 |      0.117 |\n| gpt2                               |   0.049 |                       0.078 | 0.087                   |                    0.068 |                                     0.058 |      0.039 |\n| xlm-roberta-large                  |   0.049 |                       0.117 | **0.155**               |                    0.126 |                                     0.068 |      0.097 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# French"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.368 |                       0.454 |                   0.452 |                    0.429 |                                     0.486 | 0.500      |\n| EleutherAI-gpt-neo-125M            |   0.317 |                       0.314 |                   0.378 |                    0.396 |                                     0.439 | 0.338      |\n| bert-base-multilingual-cased       |   0.429 |                       0.421 |                   0.475 |                    0.492 |                                     0.545 | **0.549**  |\n| distilbert-base-multilingual-cased |   0.377 |                       0.426 |                   0.459 |                    0.538 |                                     0.538 | 0.496      |\n| facebook-mbart-large-50            |   0.429 |                       0.498 |                   0.489 |                    0.498 |                                     0.513 | 0.509      |\n| gpt2                               |   0.356 |                       0.387 |                   0.41  |                    0.369 |                                     0.471 | 0.517      |\n| xlm-roberta-large                  |   0.475 |                       0.484 |                   0.489 |                    0.533 |                                     0.526 | 0.498      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.278 |                       0.373 |                   0.389 | 0.357                    |                                     0.429 | 0.444      |\n| EleutherAI-gpt-neo-125M            |   0.262 |                       0.238 |                   0.333 | 0.357                    |                                     0.357 | 0.270      |\n| bert-base-multilingual-cased       |   0.357 |                       0.357 |                   0.413 | 0.460                    |                                     0.5   | **0.508**  |\n| distilbert-base-multilingual-cased |   0.317 |                       0.389 |                   0.421 | 0.500                    |                                     0.476 | 0.452      |\n| facebook-mbart-large-50            |   0.357 |                       0.437 |                   0.429 | 0.452                    |                                     0.46  | 0.444      |\n| gpt2                               |   0.31  |                       0.325 |                   0.341 | 0.302                    |                                     0.413 | 0.476      |\n| xlm-roberta-large                  |   0.413 |                       0.429 |                   0.444 | **0.508**                |                                     0.484 | 0.452      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.547 |                       0.58  |                   0.538 |                    0.536 | 0.562                                     |      0.571 |\n| EleutherAI-gpt-neo-125M            |   0.402 |                       0.462 |                   0.438 |                    0.446 | 0.570                                     |      0.453 |\n| bert-base-multilingual-cased       |   0.536 |                       0.511 |                   0.559 |                    0.527 | 0.600                                     |      0.598 |\n| distilbert-base-multilingual-cased |   0.465 |                       0.471 |                   0.505 |                    0.583 | **0.619**                                 |      0.548 |\n| facebook-mbart-large-50            |   0.536 |                       0.579 |                   0.568 |                    0.553 | 0.580                                     |      0.596 |\n| gpt2                               |   0.419 |                       0.477 |                   0.512 |                    0.475 | 0.547                                     |      0.566 |\n| xlm-roberta-large                  |   0.559 |                       0.557 |                   0.544 |                    0.561 | 0.575                                     |      0.553 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.024 |                       0.095 |                   0.071 |                    0.048 | 0.071                                     |      0.071 |\n| EleutherAI-gpt-neo-125M            |   0.071 |                       0.024 |                   0.048 |                    0.071 | 0.048                                     |      0     |\n| bert-base-multilingual-cased       |   0.048 |                       0.095 |                   0.071 |                    0.071 | **0.167**                                 |      0.048 |\n| distilbert-base-multilingual-cased |   0.048 |                       0.048 |                   0.048 |                    0.048 | 0.048                                     |      0.071 |\n| facebook-mbart-large-50            |   0     |                       0.095 |                   0.024 |                    0.024 | 0.095                                     |      0.071 |\n| gpt2                               |   0     |                       0.071 |                   0.024 |                    0.024 | 0.071                                     |      0.071 |\n| xlm-roberta-large                  |   0.095 |                       0.071 |                   0.071 |                    0.071 | 0.095                                     |      0.095 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# German"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.502 |                       0.546 |                   0.567 |                    0.578 |                                     0.573 | 0.568      |\n| EleutherAI-gpt-neo-125M            |   0.395 |                       0.462 |                   0.468 |                    0.486 |                                     0.507 | 0.452      |\n| bert-base-multilingual-cased       |   0.488 |                       0.599 |                   0.587 |                    0.602 |                                     0.587 | 0.617      |\n| distilbert-base-multilingual-cased |   0.483 |                       0.551 |                   0.561 |                    0.578 |                                     0.632 | 0.587      |\n| facebook-mbart-large-50            |   0.602 |                       0.625 |                   0.598 |                    0.647 |                                     0.604 | **0.693**  |\n| gpt2                               |   0.462 |                       0.474 |                   0.469 |                    0.554 |                                     0.583 | 0.563      |\n| xlm-roberta-large                  |   0.566 |                       0.595 |                   0.609 |                    0.634 |                                     0.622 | 0.645      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.39  |                       0.448 |                   0.477 |                    0.453 |                                     0.477 | 0.471      |\n| EleutherAI-gpt-neo-125M            |   0.297 |                       0.384 |                   0.366 |                    0.401 |                                     0.413 | 0.355      |\n| bert-base-multilingual-cased       |   0.413 |                       0.535 |                   0.488 |                    0.523 |                                     0.541 | 0.535      |\n| distilbert-base-multilingual-cased |   0.401 |                       0.471 |                   0.494 |                    0.517 |                                     0.558 | 0.512      |\n| facebook-mbart-large-50            |   0.523 |                       0.581 |                   0.541 |                    0.576 |                                     0.541 | **0.616**  |\n| gpt2                               |   0.401 |                       0.395 |                   0.424 |                    0.459 |                                     0.5   | 0.483      |\n| xlm-roberta-large                  |   0.471 |                       0.535 |                   0.529 |                    0.558 |                                     0.541 | 0.576      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences | title and 10 sentences   |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|:-------------------------|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.705 |                       0.7   |                   0.701 | **0.796**                |                                     0.719 |      0.717 |\n| EleutherAI-gpt-neo-125M            |   0.593 |                       0.579 |                   0.649 | 0.616                    |                                     0.657 |      0.622 |\n| bert-base-multilingual-cased       |   0.597 |                       0.681 |                   0.737 | 0.709                    |                                     0.641 |      0.73  |\n| distilbert-base-multilingual-cased |   0.605 |                       0.664 |                   0.649 | 0.654                    |                                     0.727 |      0.688 |\n| facebook-mbart-large-50            |   0.709 |                       0.676 |                   0.669 | 0.739                    |                                     0.684 |      0.791 |\n| gpt2                               |   0.543 |                       0.591 |                   0.525 | 0.699                    |                                     0.699 |      0.675 |\n| xlm-roberta-large                  |   0.711 |                       0.672 |                   0.717 | 0.733                    |                                     0.732 |      0.733 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0     | 0.029                       |                   0     |                    0.057 |                                     0.029 |      0.057 |\n| EleutherAI-gpt-neo-125M            |   0     | 0.000                       |                   0     |                    0     |                                     0     |      0     |\n| bert-base-multilingual-cased       |   0.029 | 0.029                       |                   0.086 |                    0     |                                     0     |      0     |\n| distilbert-base-multilingual-cased |   0     | 0.029                       |                   0     |                    0     |                                     0.029 |      0.029 |\n| facebook-mbart-large-50            |   0.057 | **0.114**                   |                   0.029 |                    0.029 |                                     0.029 |      0.086 |\n| gpt2                               |   0.029 | 0.000                       |                   0     |                    0     |                                     0.029 |      0     |\n| xlm-roberta-large                  |   0.057 | 0.086                       |                   0.029 |                    0.029 |                                     0.057 |      0.029 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Italian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.492 |                       0.522 |                   0.555 |                    0.54  |                                     0.538 | 0.603      |\n| EleutherAI-gpt-neo-125M            |   0.353 |                       0.471 |                   0.45  |                    0.481 |                                     0.524 | 0.450      |\n| bert-base-multilingual-cased       |   0.492 |                       0.562 |                   0.56  |                    0.61  |                                     0.601 | 0.607      |\n| distilbert-base-multilingual-cased |   0.458 |                       0.495 |                   0.54  |                    0.585 |                                     0.527 | 0.602      |\n| facebook-mbart-large-50            |   0.545 |                       0.571 |                   0.596 |                    0.599 |                                     0.621 | **0.655**  |\n| gpt2                               |   0.409 |                       0.47  |                   0.491 |                    0.523 |                                     0.533 | 0.545      |\n| xlm-roberta-large                  |   0.565 |                       0.604 |                   0.608 |                    0.603 |                                     0.586 | **0.655**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.396 |                       0.413 |                   0.452 |                    0.426 |                                     0.426 | 0.496      |\n| EleutherAI-gpt-neo-125M            |   0.265 |                       0.391 |                   0.37  |                    0.387 |                                     0.426 | 0.352      |\n| bert-base-multilingual-cased       |   0.426 |                       0.474 |                   0.474 |                    0.53  |                                     0.548 | 0.513      |\n| distilbert-base-multilingual-cased |   0.383 |                       0.413 |                   0.47  |                    0.496 |                                     0.465 | 0.500      |\n| facebook-mbart-large-50            |   0.448 |                       0.474 |                   0.504 |                    0.513 |                                     0.53  | **0.565**  |\n| gpt2                               |   0.322 |                       0.391 |                   0.43  |                    0.443 |                                     0.457 | 0.483      |\n| xlm-roberta-large                  |   0.47  |                       0.491 |                   0.509 |                    0.496 |                                     0.504 | **0.565**  |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.65  | 0.709                       |                   0.717 |                    0.737 |                                     0.731 |      0.77  |\n| EleutherAI-gpt-neo-125M            |   0.526 | 0.592                       |                   0.574 |                    0.636 |                                     0.681 |      0.623 |\n| bert-base-multilingual-cased       |   0.583 | 0.690                       |                   0.686 |                    0.718 |                                     0.667 |      0.742 |\n| distilbert-base-multilingual-cased |   0.571 | 0.617                       |                   0.635 |                    0.713 |                                     0.608 |      0.757 |\n| facebook-mbart-large-50            |   0.696 | 0.717                       |                   0.73  |                    0.72  |                                     0.748 |      0.778 |\n| gpt2                               |   0.561 | 0.588                       |                   0.572 |                    0.637 |                                     0.64  |      0.627 |\n| xlm-roberta-large                  |   0.711 | **0.785**                   |                   0.755 |                    0.77  |                                     0.699 |      0.778 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.05  |                       0.133 |                   0.117 |                    0.183 |                                     0.133 | 0.200      |\n| EleutherAI-gpt-neo-125M            |   0.017 |                       0.067 |                   0.033 |                    0.05  |                                     0.067 | 0.067      |\n| bert-base-multilingual-cased       |   0.083 |                       0.117 |                   0.1   |                    0.167 |                                     0.1   | **0.267**  |\n| distilbert-base-multilingual-cased |   0.017 |                       0.1   |                   0.117 |                    0.1   |                                     0.033 | 0.117      |\n| facebook-mbart-large-50            |   0.117 |                       0.117 |                   0.133 |                    0.083 |                                     0.133 | 0.183      |\n| gpt2                               |   0.05  |                       0.083 |                   0.05  |                    0.083 |                                     0.1   | 0.050      |\n| xlm-roberta-large                  |   0.117 |                       0.2   |                   0.167 |                    0.15  |                                     0.117 | 0.200      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Polish"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.463 |                       0.521 |                   0.585 |                    0.603 |                                     0.622 | 0.599      |\n| EleutherAI-gpt-neo-125M            |   0.448 |                       0.475 |                   0.503 |                    0.499 |                                     0.553 | 0.488      |\n| bert-base-multilingual-cased       |   0.578 |                       0.59  |                   0.636 |                    0.64  |                                     0.656 | 0.625      |\n| distilbert-base-multilingual-cased |   0.5   |                       0.6   |                   0.617 |                    0.647 |                                     0.593 | 0.620      |\n| facebook-mbart-large-50            |   0.572 |                       0.597 |                   0.652 |                    0.657 |                                     0.701 | **0.727**  |\n| gpt2                               |   0.522 |                       0.548 |                   0.579 |                    0.558 |                                     0.575 | 0.634      |\n| xlm-roberta-large                  |   0.591 |                       0.63  |                   0.658 |                    0.667 |                                     0.622 | 0.667      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.379 |                       0.442 |                   0.5   |                    0.51  |                                     0.524 | 0.500      |\n| EleutherAI-gpt-neo-125M            |   0.374 |                       0.398 |                   0.437 |                    0.413 |                                     0.466 | 0.408      |\n| bert-base-multilingual-cased       |   0.539 |                       0.534 |                   0.573 |                    0.587 |                                     0.583 | 0.539      |\n| distilbert-base-multilingual-cased |   0.442 |                       0.553 |                   0.539 |                    0.583 |                                     0.524 | 0.539      |\n| facebook-mbart-large-50            |   0.51  |                       0.524 |                   0.587 |                    0.578 |                                     0.626 | **0.665**  |\n| gpt2                               |   0.456 |                       0.51  |                   0.558 |                    0.5   |                                     0.5   | 0.587      |\n| xlm-roberta-large                  |   0.544 |                       0.578 |                   0.607 |                    0.602 |                                     0.519 | 0.583      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.595 |                       0.636 |                   0.705 |                    0.739 |                                     0.766 | 0.746      |\n| EleutherAI-gpt-neo-125M            |   0.558 |                       0.59  |                   0.592 |                    0.63  |                                     0.681 | 0.609      |\n| bert-base-multilingual-cased       |   0.624 |                       0.659 |                   0.715 |                    0.703 |                                     0.75  | 0.745      |\n| distilbert-base-multilingual-cased |   0.576 |                       0.655 |                   0.721 |                    0.727 |                                     0.684 | 0.730      |\n| facebook-mbart-large-50            |   0.652 |                       0.692 |                   0.733 |                    0.763 |                                     0.796 | **0.801**  |\n| gpt2                               |   0.61  |                       0.593 |                   0.602 |                    0.632 |                                     0.678 | 0.688      |\n| xlm-roberta-large                  |   0.647 |                       0.692 |                   0.718 |                    0.747 |                                     0.775 | 0.779      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph | raw text   |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|:-----------|\n| EleutherAI-gpt-neo-1.3B            |   0.026 |                       0     |                   0.026 |                    0.026 |                                     0.051 | 0.051      |\n| EleutherAI-gpt-neo-125M            |   0     |                       0     |                   0     |                    0.051 |                                     0     | 0.026      |\n| bert-base-multilingual-cased       |   0     |                       0.026 |                   0.051 |                    0.026 |                                     0.051 | **0.103**  |\n| distilbert-base-multilingual-cased |   0     |                       0     |                   0.051 |                    0.026 |                                     0.051 | 0.026      |\n| facebook-mbart-large-50            |   0     |                       0.026 |                   0.051 |                    0.077 |                                     0.051 | 0.051      |\n| gpt2                               |   0     |                       0.026 |                   0     |                    0.026 |                                     0.051 | 0.026      |\n| xlm-roberta-large                  |   0     |                       0.051 |                   0.026 |                    0.026 |                                     0.077 | 0.077      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Russian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.296 |                       0.308 |                   0.397 |                    0.381 | 0.371                                     |      0.298 |\n| EleutherAI-gpt-neo-125M            |   0.172 |                       0.169 |                   0.203 |                    0.252 | 0.192                                     |      0.125 |\n| bert-base-multilingual-cased       |   0.386 |                       0.426 |                   0.487 |                    0.447 | 0.515                                     |      0.464 |\n| distilbert-base-multilingual-cased |   0.34  |                       0.372 |                   0.497 |                    0.497 | 0.455                                     |      0.517 |\n| facebook-mbart-large-50            |   0.424 |                       0.441 |                   0.446 |                    0.521 | **0.573**                                 |      0.529 |\n| gpt2                               |   0.061 |                       0.073 |                   0.062 |                    0.039 | 0.191                                     |      0.171 |\n| xlm-roberta-large                  |   0.464 |                       0.489 |                   0.464 |                    0.533 | 0.521                                     |      0.553 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         |   title |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-----------------------------------|--------:|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| EleutherAI-gpt-neo-1.3B            |   0.186 |                       0.209 |                   0.291 |                    0.279 | 0.267                                     |      0.209 |\n| EleutherAI-gpt-neo-125M            |   0.116 |                       0.128 |                   0.14  |                    0.198 | 0.140                                     |      0.093 |\n| bert-base-multilingual-cased       |   0.314 |                       0.384 |                   0.43  |                    0.419 | 0.488                                     |      0.407 |\n| distilbert-base-multilingual-cased |   0.291 |                       0.337 |                   0.442 |                    0.442 | 0.407                                     |      0.453 |\n| facebook-mbart-large-50            |   0.326 |                       0.349 |                   0.36  |                    0.442 | **0.500**                                 |      0.43  |\n| gpt2                               |   0.035 |                       0.047 |                   0.035 |                    0.023 | 0.128                                     |      0.105 |\n| xlm-roberta-large                  |   0.372 |                       0.384 |                   0.372 |                    0.465 | 0.442                                     |      0.488 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title     |   title and first paragraph |   title and 5 sentences |   title and 10 sentences |   title and first sentence each paragraph |   raw text |\n|:-----------------------------------|:----------|----------------------------:|------------------------:|-------------------------:|------------------------------------------:|-----------:|\n| EleutherAI-gpt-neo-1.3B            | **0.727** |                       0.581 |                   0.625 |                    0.6   |                                     0.605 |      0.514 |\n| EleutherAI-gpt-neo-125M            | 0.333     |                       0.25  |                   0.375 |                    0.347 |                                     0.308 |      0.19  |\n| bert-base-multilingual-cased       | 0.500     |                       0.478 |                   0.561 |                    0.48  |                                     0.545 |      0.538 |\n| distilbert-base-multilingual-cased | 0.410     |                       0.414 |                   0.567 |                    0.567 |                                     0.515 |      0.6   |\n| facebook-mbart-large-50            | 0.609     |                       0.6   |                   0.585 |                    0.633 |                                     0.672 |      0.685 |\n| gpt2                               | 0.231     |                       0.174 |                   0.3   |                    0.118 |                                     0.379 |      0.474 |\n| xlm-roberta-large                  | 0.615     |                       0.673 |                   0.615 |                    0.625 |                                     0.633 |      0.636 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title     |   title and first paragraph |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   |   raw text |\n|:-----------------------------------|:----------|----------------------------:|------------------------:|-------------------------:|:------------------------------------------|-----------:|\n| EleutherAI-gpt-neo-1.3B            | 0.053     |                       0.079 |                   0.079 |                    0.158 | 0.158                                     |      0.053 |\n| EleutherAI-gpt-neo-125M            | 0.026     |                       0.026 |                   0.053 |                    0.053 | 0.105                                     |      0.026 |\n| bert-base-multilingual-cased       | 0.105     |                       0.132 |                   0.211 |                    0.158 | **0.237**                                 |      0.211 |\n| distilbert-base-multilingual-cased | 0.105     |                       0.158 |                   0.132 |                    0.211 | 0.132                                     |      0.184 |\n| facebook-mbart-large-50            | 0.158     |                       0.211 |                   0.132 |                    0.211 | 0.158                                     |      0.184 |\n| gpt2                               | 0.000     |                       0     |                   0.026 |                    0     | 0.079                                     |      0.026 |\n| xlm-roberta-large                  | **0.237** |                       0.211 |                   0.211 |                    0.211 | 0.211                                     |      0.211 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# All 6 Languages"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         |   title | title and first paragraph   |   title and 5 sentences |   title and 10 sentences | title and first sentence each paragraph   | raw text   |\n|:-----------|:-----------------------------------|--------:|:----------------------------|------------------------:|-------------------------:|:------------------------------------------|:-----------|\n| en         | EleutherAI-gpt-neo-1.3B            |   0.646 | 0.687                       |                   0.669 |                    0.682 | 0.709                                     | 0.693      |\n| en         | EleutherAI-gpt-neo-125M            |   0.573 | 0.647                       |                   0.642 |                    0.636 | 0.649                                     | 0.631      |\n| en         | bert-base-multilingual-cased       |   0.619 | 0.690                       |                   0.676 |                    0.689 | 0.688                                     | 0.711      |\n| en         | distilbert-base-multilingual-cased |   0.592 | 0.662                       |                   0.685 |                    0.686 | 0.684                                     | 0.684      |\n| en         | facebook-mbart-large-50            |   0.666 | **0.734**                   |                   0.731 |                    0.718 | 0.708                                     | 0.711      |\n| en         | gpt2                               |   0.625 | 0.664                       |                   0.678 |                    0.66  | 0.680                                     | 0.654      |\n| en         | xlm-roberta-large                  |   0.659 | 0.710                       |                   0.721 |                    0.71  | 0.709                                     | 0.700      |\n| fr         | EleutherAI-gpt-neo-1.3B            |   0.368 | 0.454                       |                   0.452 |                    0.429 | 0.486                                     | 0.500      |\n| fr         | EleutherAI-gpt-neo-125M            |   0.317 | 0.314                       |                   0.378 |                    0.396 | 0.439                                     | 0.338      |\n| fr         | bert-base-multilingual-cased       |   0.429 | 0.421                       |                   0.475 |                    0.492 | 0.545                                     | **0.549**  |\n| fr         | distilbert-base-multilingual-cased |   0.377 | 0.426                       |                   0.459 |                    0.538 | 0.538                                     | 0.496      |\n| fr         | facebook-mbart-large-50            |   0.429 | 0.498                       |                   0.489 |                    0.498 | 0.513                                     | 0.509      |\n| fr         | gpt2                               |   0.356 | 0.387                       |                   0.41  |                    0.369 | 0.471                                     | 0.517      |\n| fr         | xlm-roberta-large                  |   0.475 | 0.484                       |                   0.489 |                    0.533 | 0.526                                     | 0.498      |\n| ge         | EleutherAI-gpt-neo-1.3B            |   0.502 | 0.546                       |                   0.567 |                    0.578 | 0.573                                     | 0.568      |\n| ge         | EleutherAI-gpt-neo-125M            |   0.395 | 0.462                       |                   0.468 |                    0.486 | 0.507                                     | 0.452      |\n| ge         | bert-base-multilingual-cased       |   0.488 | 0.599                       |                   0.587 |                    0.602 | 0.587                                     | 0.617      |\n| ge         | distilbert-base-multilingual-cased |   0.483 | 0.551                       |                   0.561 |                    0.578 | 0.632                                     | 0.587      |\n| ge         | facebook-mbart-large-50            |   0.602 | 0.625                       |                   0.598 |                    0.647 | 0.604                                     | **0.693**  |\n| ge         | gpt2                               |   0.462 | 0.474                       |                   0.469 |                    0.554 | 0.583                                     | 0.563      |\n| ge         | xlm-roberta-large                  |   0.566 | 0.595                       |                   0.609 |                    0.634 | 0.622                                     | 0.645      |\n| it         | EleutherAI-gpt-neo-1.3B            |   0.492 | 0.522                       |                   0.555 |                    0.54  | 0.538                                     | 0.603      |\n| it         | EleutherAI-gpt-neo-125M            |   0.353 | 0.471                       |                   0.45  |                    0.481 | 0.524                                     | 0.450      |\n| it         | bert-base-multilingual-cased       |   0.492 | 0.562                       |                   0.56  |                    0.61  | 0.601                                     | 0.607      |\n| it         | distilbert-base-multilingual-cased |   0.458 | 0.495                       |                   0.54  |                    0.585 | 0.527                                     | 0.602      |\n| it         | facebook-mbart-large-50            |   0.545 | 0.571                       |                   0.596 |                    0.599 | 0.621                                     | **0.655**  |\n| it         | gpt2                               |   0.409 | 0.470                       |                   0.491 |                    0.523 | 0.533                                     | 0.545      |\n| it         | xlm-roberta-large                  |   0.565 | 0.604                       |                   0.608 |                    0.603 | 0.586                                     | **0.655**  |\n| po         | EleutherAI-gpt-neo-1.3B            |   0.463 | 0.521                       |                   0.585 |                    0.603 | 0.622                                     | 0.599      |\n| po         | EleutherAI-gpt-neo-125M            |   0.448 | 0.475                       |                   0.503 |                    0.499 | 0.553                                     | 0.488      |\n| po         | bert-base-multilingual-cased       |   0.578 | 0.590                       |                   0.636 |                    0.64  | 0.656                                     | 0.625      |\n| po         | distilbert-base-multilingual-cased |   0.5   | 0.600                       |                   0.617 |                    0.647 | 0.593                                     | 0.620      |\n| po         | facebook-mbart-large-50            |   0.572 | 0.597                       |                   0.652 |                    0.657 | 0.701                                     | **0.727**  |\n| po         | gpt2                               |   0.522 | 0.548                       |                   0.579 |                    0.558 | 0.575                                     | 0.634      |\n| po         | xlm-roberta-large                  |   0.591 | 0.630                       |                   0.658 |                    0.667 | 0.622                                     | 0.667      |\n| ru         | EleutherAI-gpt-neo-1.3B            |   0.296 | 0.308                       |                   0.397 |                    0.381 | 0.371                                     | 0.298      |\n| ru         | EleutherAI-gpt-neo-125M            |   0.172 | 0.169                       |                   0.203 |                    0.252 | 0.192                                     | 0.125      |\n| ru         | bert-base-multilingual-cased       |   0.386 | 0.426                       |                   0.487 |                    0.447 | 0.515                                     | 0.464      |\n| ru         | distilbert-base-multilingual-cased |   0.34  | 0.372                       |                   0.497 |                    0.497 | 0.455                                     | 0.517      |\n| ru         | facebook-mbart-large-50            |   0.424 | 0.441                       |                   0.446 |                    0.521 | **0.573**                                 | 0.529      |\n| ru         | gpt2                               |   0.061 | 0.073                       |                   0.062 |                    0.039 | 0.191                                     | 0.171      |\n| ru         | xlm-roberta-large                  |   0.464 | 0.489                       |                   0.464 |                    0.533 | 0.521                                     | 0.553      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         |   title | title and first paragraph   |   title and 5 sentences | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-----------------------------------|--------:|:----------------------------|------------------------:|:-------------------------|:------------------------------------------|:-----------|\n| en         | EleutherAI-gpt-neo-1.3B            |   0.562 | 0.606                       |                   0.587 | 0.599                    | 0.645                                     | 0.621      |\n| en         | EleutherAI-gpt-neo-125M            |   0.491 | 0.572                       |                   0.567 | 0.553                    | 0.577                                     | 0.570      |\n| en         | bert-base-multilingual-cased       |   0.545 | 0.626                       |                   0.599 | 0.638                    | 0.660                                     | 0.650      |\n| en         | distilbert-base-multilingual-cased |   0.513 | 0.592                       |                   0.611 | 0.621                    | 0.623                                     | 0.614      |\n| en         | facebook-mbart-large-50            |   0.587 | **0.680**                   |                   0.665 | 0.643                    | 0.650                                     | 0.655      |\n| en         | gpt2                               |   0.565 | 0.621                       |                   0.655 | 0.601                    | 0.645                                     | 0.592      |\n| en         | xlm-roberta-large                  |   0.579 | 0.636                       |                   0.645 | 0.641                    | 0.653                                     | 0.626      |\n| fr         | EleutherAI-gpt-neo-1.3B            |   0.278 | 0.373                       |                   0.389 | 0.357                    | 0.429                                     | 0.444      |\n| fr         | EleutherAI-gpt-neo-125M            |   0.262 | 0.238                       |                   0.333 | 0.357                    | 0.357                                     | 0.270      |\n| fr         | bert-base-multilingual-cased       |   0.357 | 0.357                       |                   0.413 | 0.460                    | 0.500                                     | **0.508**  |\n| fr         | distilbert-base-multilingual-cased |   0.317 | 0.389                       |                   0.421 | 0.500                    | 0.476                                     | 0.452      |\n| fr         | facebook-mbart-large-50            |   0.357 | 0.437                       |                   0.429 | 0.452                    | 0.460                                     | 0.444      |\n| fr         | gpt2                               |   0.31  | 0.325                       |                   0.341 | 0.302                    | 0.413                                     | 0.476      |\n| fr         | xlm-roberta-large                  |   0.413 | 0.429                       |                   0.444 | **0.508**                | 0.484                                     | 0.452      |\n| ge         | EleutherAI-gpt-neo-1.3B            |   0.39  | 0.448                       |                   0.477 | 0.453                    | 0.477                                     | 0.471      |\n| ge         | EleutherAI-gpt-neo-125M            |   0.297 | 0.384                       |                   0.366 | 0.401                    | 0.413                                     | 0.355      |\n| ge         | bert-base-multilingual-cased       |   0.413 | 0.535                       |                   0.488 | 0.523                    | 0.541                                     | 0.535      |\n| ge         | distilbert-base-multilingual-cased |   0.401 | 0.471                       |                   0.494 | 0.517                    | 0.558                                     | 0.512      |\n| ge         | facebook-mbart-large-50            |   0.523 | 0.581                       |                   0.541 | 0.576                    | 0.541                                     | **0.616**  |\n| ge         | gpt2                               |   0.401 | 0.395                       |                   0.424 | 0.459                    | 0.500                                     | 0.483      |\n| ge         | xlm-roberta-large                  |   0.471 | 0.535                       |                   0.529 | 0.558                    | 0.541                                     | 0.576      |\n| it         | EleutherAI-gpt-neo-1.3B            |   0.396 | 0.413                       |                   0.452 | 0.426                    | 0.426                                     | 0.496      |\n| it         | EleutherAI-gpt-neo-125M            |   0.265 | 0.391                       |                   0.37  | 0.387                    | 0.426                                     | 0.352      |\n| it         | bert-base-multilingual-cased       |   0.426 | 0.474                       |                   0.474 | 0.530                    | 0.548                                     | 0.513      |\n| it         | distilbert-base-multilingual-cased |   0.383 | 0.413                       |                   0.47  | 0.496                    | 0.465                                     | 0.500      |\n| it         | facebook-mbart-large-50            |   0.448 | 0.474                       |                   0.504 | 0.513                    | 0.530                                     | **0.565**  |\n| it         | gpt2                               |   0.322 | 0.391                       |                   0.43  | 0.443                    | 0.457                                     | 0.483      |\n| it         | xlm-roberta-large                  |   0.47  | 0.491                       |                   0.509 | 0.496                    | 0.504                                     | **0.565**  |\n| po         | EleutherAI-gpt-neo-1.3B            |   0.379 | 0.442                       |                   0.5   | 0.510                    | 0.524                                     | 0.500      |\n| po         | EleutherAI-gpt-neo-125M            |   0.374 | 0.398                       |                   0.437 | 0.413                    | 0.466                                     | 0.408      |\n| po         | bert-base-multilingual-cased       |   0.539 | 0.534                       |                   0.573 | 0.587                    | 0.583                                     | 0.539      |\n| po         | distilbert-base-multilingual-cased |   0.442 | 0.553                       |                   0.539 | 0.583                    | 0.524                                     | 0.539      |\n| po         | facebook-mbart-large-50            |   0.51  | 0.524                       |                   0.587 | 0.578                    | 0.626                                     | **0.665**  |\n| po         | gpt2                               |   0.456 | 0.510                       |                   0.558 | 0.500                    | 0.500                                     | 0.587      |\n| po         | xlm-roberta-large                  |   0.544 | 0.578                       |                   0.607 | 0.602                    | 0.519                                     | 0.583      |\n| ru         | EleutherAI-gpt-neo-1.3B            |   0.186 | 0.209                       |                   0.291 | 0.279                    | 0.267                                     | 0.209      |\n| ru         | EleutherAI-gpt-neo-125M            |   0.116 | 0.128                       |                   0.14  | 0.198                    | 0.140                                     | 0.093      |\n| ru         | bert-base-multilingual-cased       |   0.314 | 0.384                       |                   0.43  | 0.419                    | 0.488                                     | 0.407      |\n| ru         | distilbert-base-multilingual-cased |   0.291 | 0.337                       |                   0.442 | 0.442                    | 0.407                                     | 0.453      |\n| ru         | facebook-mbart-large-50            |   0.326 | 0.349                       |                   0.36  | 0.442                    | **0.500**                                 | 0.430      |\n| ru         | gpt2                               |   0.035 | 0.047                       |                   0.035 | 0.023                    | 0.128                                     | 0.105      |\n| ru         | xlm-roberta-large                  |   0.372 | 0.384                       |                   0.372 | 0.465                    | 0.442                                     | 0.488      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision_micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title     | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text   |\n|:-----------|:-----------------------------------|:----------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:-----------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.759     | 0.792                       | 0.777                   | 0.793                    | 0.786                                     | 0.784      |\n| en         | EleutherAI-gpt-neo-125M            | 0.686     | 0.745                       | 0.739                   | 0.748                    | 0.742                                     | 0.706      |\n| en         | bert-base-multilingual-cased       | 0.715     | 0.769                       | 0.775                   | 0.748                    | 0.718                                     | 0.785      |\n| en         | distilbert-base-multilingual-cased | 0.700     | 0.752                       | 0.779                   | 0.765                    | 0.757                                     | 0.772      |\n| en         | facebook-mbart-large-50            | 0.769     | 0.797                       | 0.812                   | 0.812                    | 0.778                                     | 0.777      |\n| en         | gpt2                               | 0.700     | 0.713                       | 0.702                   | 0.732                    | 0.719                                     | 0.731      |\n| en         | xlm-roberta-large                  | 0.765     | 0.805                       | **0.817**               | 0.796                    | 0.776                                     | 0.795      |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.547     | 0.580                       | 0.538                   | 0.536                    | 0.562                                     | 0.571      |\n| fr         | EleutherAI-gpt-neo-125M            | 0.402     | 0.462                       | 0.438                   | 0.446                    | 0.570                                     | 0.453      |\n| fr         | bert-base-multilingual-cased       | 0.536     | 0.511                       | 0.559                   | 0.527                    | 0.600                                     | 0.598      |\n| fr         | distilbert-base-multilingual-cased | 0.465     | 0.471                       | 0.505                   | 0.583                    | **0.619**                                 | 0.548      |\n| fr         | facebook-mbart-large-50            | 0.536     | 0.579                       | 0.568                   | 0.553                    | 0.580                                     | 0.596      |\n| fr         | gpt2                               | 0.419     | 0.477                       | 0.512                   | 0.475                    | 0.547                                     | 0.566      |\n| fr         | xlm-roberta-large                  | 0.559     | 0.557                       | 0.544                   | 0.561                    | 0.575                                     | 0.553      |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.705     | 0.700                       | 0.701                   | **0.796**                | 0.719                                     | 0.717      |\n| ge         | EleutherAI-gpt-neo-125M            | 0.593     | 0.579                       | 0.649                   | 0.616                    | 0.657                                     | 0.622      |\n| ge         | bert-base-multilingual-cased       | 0.597     | 0.681                       | 0.737                   | 0.709                    | 0.641                                     | 0.730      |\n| ge         | distilbert-base-multilingual-cased | 0.605     | 0.664                       | 0.649                   | 0.654                    | 0.727                                     | 0.688      |\n| ge         | facebook-mbart-large-50            | 0.709     | 0.676                       | 0.669                   | 0.739                    | 0.684                                     | 0.791      |\n| ge         | gpt2                               | 0.543     | 0.591                       | 0.525                   | 0.699                    | 0.699                                     | 0.675      |\n| ge         | xlm-roberta-large                  | 0.711     | 0.672                       | 0.717                   | 0.733                    | 0.732                                     | 0.733      |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.650     | 0.709                       | 0.717                   | 0.737                    | 0.731                                     | 0.770      |\n| it         | EleutherAI-gpt-neo-125M            | 0.526     | 0.592                       | 0.574                   | 0.636                    | 0.681                                     | 0.623      |\n| it         | bert-base-multilingual-cased       | 0.583     | 0.690                       | 0.686                   | 0.718                    | 0.667                                     | 0.742      |\n| it         | distilbert-base-multilingual-cased | 0.571     | 0.617                       | 0.635                   | 0.713                    | 0.608                                     | 0.757      |\n| it         | facebook-mbart-large-50            | 0.696     | 0.717                       | 0.730                   | 0.720                    | 0.748                                     | 0.778      |\n| it         | gpt2                               | 0.561     | 0.588                       | 0.572                   | 0.637                    | 0.640                                     | 0.627      |\n| it         | xlm-roberta-large                  | 0.711     | **0.785**                   | 0.755                   | 0.770                    | 0.699                                     | 0.778      |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.595     | 0.636                       | 0.705                   | 0.739                    | 0.766                                     | 0.746      |\n| po         | EleutherAI-gpt-neo-125M            | 0.558     | 0.590                       | 0.592                   | 0.630                    | 0.681                                     | 0.609      |\n| po         | bert-base-multilingual-cased       | 0.624     | 0.659                       | 0.715                   | 0.703                    | 0.750                                     | 0.745      |\n| po         | distilbert-base-multilingual-cased | 0.576     | 0.655                       | 0.721                   | 0.727                    | 0.684                                     | 0.730      |\n| po         | facebook-mbart-large-50            | 0.652     | 0.692                       | 0.733                   | 0.763                    | 0.796                                     | **0.801**  |\n| po         | gpt2                               | 0.610     | 0.593                       | 0.602                   | 0.632                    | 0.678                                     | 0.688      |\n| po         | xlm-roberta-large                  | 0.647     | 0.692                       | 0.718                   | 0.747                    | 0.775                                     | 0.779      |\n| ru         | EleutherAI-gpt-neo-1.3B            | **0.727** | 0.581                       | 0.625                   | 0.600                    | 0.605                                     | 0.514      |\n| ru         | EleutherAI-gpt-neo-125M            | 0.333     | 0.250                       | 0.375                   | 0.347                    | 0.308                                     | 0.190      |\n| ru         | bert-base-multilingual-cased       | 0.500     | 0.478                       | 0.561                   | 0.480                    | 0.545                                     | 0.538      |\n| ru         | distilbert-base-multilingual-cased | 0.410     | 0.414                       | 0.567                   | 0.567                    | 0.515                                     | 0.600      |\n| ru         | facebook-mbart-large-50            | 0.609     | 0.600                       | 0.585                   | 0.633                    | 0.672                                     | 0.685      |\n| ru         | gpt2                               | 0.231     | 0.174                       | 0.300                   | 0.118                    | 0.379                                     | 0.474      |\n| ru         | xlm-roberta-large                  | 0.615     | 0.673                       | 0.615                   | 0.625                    | 0.633                                     | 0.636      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title     | title and first paragraph   | title and 5 sentences   |   title and 10 sentences | title and first sentence each paragraph   | raw text   |\n|:-----------|:-----------------------------------|:----------|:----------------------------|:------------------------|-------------------------:|:------------------------------------------|:-----------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.058     | 0.136                       | 0.117                   |                    0.078 | 0.146                                     | 0.097      |\n| en         | EleutherAI-gpt-neo-125M            | 0.068     | 0.087                       | 0.107                   |                    0.097 | 0.078                                     | 0.019      |\n| en         | bert-base-multilingual-cased       | 0.097     | 0.136                       | 0.126                   |                    0.097 | 0.126                                     | 0.117      |\n| en         | distilbert-base-multilingual-cased | 0.087     | 0.097                       | 0.117                   |                    0.058 | 0.068                                     | 0.087      |\n| en         | facebook-mbart-large-50            | 0.097     | 0.126                       | **0.155**               |                    0.126 | 0.136                                     | 0.117      |\n| en         | gpt2                               | 0.049     | 0.078                       | 0.087                   |                    0.068 | 0.058                                     | 0.039      |\n| en         | xlm-roberta-large                  | 0.049     | 0.117                       | **0.155**               |                    0.126 | 0.068                                     | 0.097      |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.024     | 0.095                       | 0.071                   |                    0.048 | 0.071                                     | 0.071      |\n| fr         | EleutherAI-gpt-neo-125M            | 0.071     | 0.024                       | 0.048                   |                    0.071 | 0.048                                     | 0.000      |\n| fr         | bert-base-multilingual-cased       | 0.048     | 0.095                       | 0.071                   |                    0.071 | **0.167**                                 | 0.048      |\n| fr         | distilbert-base-multilingual-cased | 0.048     | 0.048                       | 0.048                   |                    0.048 | 0.048                                     | 0.071      |\n| fr         | facebook-mbart-large-50            | 0.000     | 0.095                       | 0.024                   |                    0.024 | 0.095                                     | 0.071      |\n| fr         | gpt2                               | 0.000     | 0.071                       | 0.024                   |                    0.024 | 0.071                                     | 0.071      |\n| fr         | xlm-roberta-large                  | 0.095     | 0.071                       | 0.071                   |                    0.071 | 0.095                                     | 0.095      |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.000     | 0.029                       | 0.000                   |                    0.057 | 0.029                                     | 0.057      |\n| ge         | EleutherAI-gpt-neo-125M            | 0.000     | 0.000                       | 0.000                   |                    0     | 0.000                                     | 0.000      |\n| ge         | bert-base-multilingual-cased       | 0.029     | 0.029                       | 0.086                   |                    0     | 0.000                                     | 0.000      |\n| ge         | distilbert-base-multilingual-cased | 0.000     | 0.029                       | 0.000                   |                    0     | 0.029                                     | 0.029      |\n| ge         | facebook-mbart-large-50            | 0.057     | **0.114**                   | 0.029                   |                    0.029 | 0.029                                     | 0.086      |\n| ge         | gpt2                               | 0.029     | 0.000                       | 0.000                   |                    0     | 0.029                                     | 0.000      |\n| ge         | xlm-roberta-large                  | 0.057     | 0.086                       | 0.029                   |                    0.029 | 0.057                                     | 0.029      |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.050     | 0.133                       | 0.117                   |                    0.183 | 0.133                                     | 0.200      |\n| it         | EleutherAI-gpt-neo-125M            | 0.017     | 0.067                       | 0.033                   |                    0.05  | 0.067                                     | 0.067      |\n| it         | bert-base-multilingual-cased       | 0.083     | 0.117                       | 0.100                   |                    0.167 | 0.100                                     | **0.267**  |\n| it         | distilbert-base-multilingual-cased | 0.017     | 0.100                       | 0.117                   |                    0.1   | 0.033                                     | 0.117      |\n| it         | facebook-mbart-large-50            | 0.117     | 0.117                       | 0.133                   |                    0.083 | 0.133                                     | 0.183      |\n| it         | gpt2                               | 0.050     | 0.083                       | 0.050                   |                    0.083 | 0.100                                     | 0.050      |\n| it         | xlm-roberta-large                  | 0.117     | 0.200                       | 0.167                   |                    0.15  | 0.117                                     | 0.200      |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.026     | 0.000                       | 0.026                   |                    0.026 | 0.051                                     | 0.051      |\n| po         | EleutherAI-gpt-neo-125M            | 0.000     | 0.000                       | 0.000                   |                    0.051 | 0.000                                     | 0.026      |\n| po         | bert-base-multilingual-cased       | 0.000     | 0.026                       | 0.051                   |                    0.026 | 0.051                                     | **0.103**  |\n| po         | distilbert-base-multilingual-cased | 0.000     | 0.000                       | 0.051                   |                    0.026 | 0.051                                     | 0.026      |\n| po         | facebook-mbart-large-50            | 0.000     | 0.026                       | 0.051                   |                    0.077 | 0.051                                     | 0.051      |\n| po         | gpt2                               | 0.000     | 0.026                       | 0.000                   |                    0.026 | 0.051                                     | 0.026      |\n| po         | xlm-roberta-large                  | 0.000     | 0.051                       | 0.026                   |                    0.026 | 0.077                                     | 0.077      |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.053     | 0.079                       | 0.079                   |                    0.158 | 0.158                                     | 0.053      |\n| ru         | EleutherAI-gpt-neo-125M            | 0.026     | 0.026                       | 0.053                   |                    0.053 | 0.105                                     | 0.026      |\n| ru         | bert-base-multilingual-cased       | 0.105     | 0.132                       | 0.211                   |                    0.158 | **0.237**                                 | 0.211      |\n| ru         | distilbert-base-multilingual-cased | 0.105     | 0.158                       | 0.132                   |                    0.211 | 0.132                                     | 0.184      |\n| ru         | facebook-mbart-large-50            | 0.158     | 0.211                       | 0.132                   |                    0.211 | 0.158                                     | 0.184      |\n| ru         | gpt2                               | 0.000     | 0.000                       | 0.026                   |                    0     | 0.079                                     | 0.026      |\n| ru         | xlm-roberta-large                  | **0.237** | 0.211                       | 0.211                   |                    0.211 | 0.211                                     | 0.211      |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17486/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    }
   ],
   "source": [
    "display_metrics_and_write_to_file(df=results_df, grouping_criterion=['model_name'], output_dir='per_model_name_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
