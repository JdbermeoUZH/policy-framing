\begin{tabular}{llllllll}
\toprule
language &                         model\_name & title & title and first paragraph & title and 5 sentences & title and 10 sentences & title and first sentence each paragraph &  raw text \\
\midrule
      en &            EleutherAI-gpt-neo-1.3B & 0.646 &                     0.687 &                 0.669 &                  0.682 &                                   0.709 &     0.693 \\
      en &            EleutherAI-gpt-neo-125M & 0.573 &                     0.647 &                 0.642 &                  0.636 &                                   0.649 &     0.631 \\
      en &       bert-base-multilingual-cased & 0.619 &                     0.690 &                 0.676 &                  0.689 &                                   0.688 &     0.711 \\
      en & distilbert-base-multilingual-cased & 0.592 &                     0.662 &                 0.685 &                  0.686 &                                   0.684 &     0.684 \\
      en &            facebook-mbart-large-50 & 0.666 &                 **0.734** &                 0.731 &                  0.718 &                                   0.708 &     0.711 \\
      en &                               gpt2 & 0.625 &                     0.664 &                 0.678 &                  0.660 &                                   0.680 &     0.654 \\
      en &                  xlm-roberta-large & 0.659 &                     0.710 &                 0.721 &                  0.710 &                                   0.709 &     0.700 \\
      fr &            EleutherAI-gpt-neo-1.3B & 0.368 &                     0.454 &                 0.452 &                  0.429 &                                   0.486 &     0.500 \\
      fr &            EleutherAI-gpt-neo-125M & 0.317 &                     0.314 &                 0.378 &                  0.396 &                                   0.439 &     0.338 \\
      fr &       bert-base-multilingual-cased & 0.429 &                     0.421 &                 0.475 &                  0.492 &                                   0.545 & **0.549** \\
      fr & distilbert-base-multilingual-cased & 0.377 &                     0.426 &                 0.459 &                  0.538 &                                   0.538 &     0.496 \\
      fr &            facebook-mbart-large-50 & 0.429 &                     0.498 &                 0.489 &                  0.498 &                                   0.513 &     0.509 \\
      fr &                               gpt2 & 0.356 &                     0.387 &                 0.410 &                  0.369 &                                   0.471 &     0.517 \\
      fr &                  xlm-roberta-large & 0.475 &                     0.484 &                 0.489 &                  0.533 &                                   0.526 &     0.498 \\
      ge &            EleutherAI-gpt-neo-1.3B & 0.502 &                     0.546 &                 0.567 &                  0.578 &                                   0.573 &     0.568 \\
      ge &            EleutherAI-gpt-neo-125M & 0.395 &                     0.462 &                 0.468 &                  0.486 &                                   0.507 &     0.452 \\
      ge &       bert-base-multilingual-cased & 0.488 &                     0.599 &                 0.587 &                  0.602 &                                   0.587 &     0.617 \\
      ge & distilbert-base-multilingual-cased & 0.483 &                     0.551 &                 0.561 &                  0.578 &                                   0.632 &     0.587 \\
      ge &            facebook-mbart-large-50 & 0.602 &                     0.625 &                 0.598 &                  0.647 &                                   0.604 & **0.693** \\
      ge &                               gpt2 & 0.462 &                     0.474 &                 0.469 &                  0.554 &                                   0.583 &     0.563 \\
      ge &                  xlm-roberta-large & 0.566 &                     0.595 &                 0.609 &                  0.634 &                                   0.622 &     0.645 \\
      it &            EleutherAI-gpt-neo-1.3B & 0.492 &                     0.522 &                 0.555 &                  0.540 &                                   0.538 &     0.603 \\
      it &            EleutherAI-gpt-neo-125M & 0.353 &                     0.471 &                 0.450 &                  0.481 &                                   0.524 &     0.450 \\
      it &       bert-base-multilingual-cased & 0.492 &                     0.562 &                 0.560 &                  0.610 &                                   0.601 &     0.607 \\
      it & distilbert-base-multilingual-cased & 0.458 &                     0.495 &                 0.540 &                  0.585 &                                   0.527 &     0.602 \\
      it &            facebook-mbart-large-50 & 0.545 &                     0.571 &                 0.596 &                  0.599 &                                   0.621 & **0.655** \\
      it &                               gpt2 & 0.409 &                     0.470 &                 0.491 &                  0.523 &                                   0.533 &     0.545 \\
      it &                  xlm-roberta-large & 0.565 &                     0.604 &                 0.608 &                  0.603 &                                   0.586 & **0.655** \\
      po &            EleutherAI-gpt-neo-1.3B & 0.463 &                     0.521 &                 0.585 &                  0.603 &                                   0.622 &     0.599 \\
      po &            EleutherAI-gpt-neo-125M & 0.448 &                     0.475 &                 0.503 &                  0.499 &                                   0.553 &     0.488 \\
      po &       bert-base-multilingual-cased & 0.578 &                     0.590 &                 0.636 &                  0.640 &                                   0.656 &     0.625 \\
      po & distilbert-base-multilingual-cased & 0.500 &                     0.600 &                 0.617 &                  0.647 &                                   0.593 &     0.620 \\
      po &            facebook-mbart-large-50 & 0.572 &                     0.597 &                 0.652 &                  0.657 &                                   0.701 & **0.727** \\
      po &                               gpt2 & 0.522 &                     0.548 &                 0.579 &                  0.558 &                                   0.575 &     0.634 \\
      po &                  xlm-roberta-large & 0.591 &                     0.630 &                 0.658 &                  0.667 &                                   0.622 &     0.667 \\
      ru &            EleutherAI-gpt-neo-1.3B & 0.296 &                     0.308 &                 0.397 &                  0.381 &                                   0.371 &     0.298 \\
      ru &            EleutherAI-gpt-neo-125M & 0.172 &                     0.169 &                 0.203 &                  0.252 &                                   0.192 &     0.125 \\
      ru &       bert-base-multilingual-cased & 0.386 &                     0.426 &                 0.487 &                  0.447 &                                   0.515 &     0.464 \\
      ru & distilbert-base-multilingual-cased & 0.340 &                     0.372 &                 0.497 &                  0.497 &                                   0.455 &     0.517 \\
      ru &            facebook-mbart-large-50 & 0.424 &                     0.441 &                 0.446 &                  0.521 &                               **0.573** &     0.529 \\
      ru &                               gpt2 & 0.061 &                     0.073 &                 0.062 &                  0.039 &                                   0.191 &     0.171 \\
      ru &                  xlm-roberta-large & 0.464 &                     0.489 &                 0.464 &                  0.533 &                                   0.521 &     0.553 \\
\bottomrule
\end{tabular}
