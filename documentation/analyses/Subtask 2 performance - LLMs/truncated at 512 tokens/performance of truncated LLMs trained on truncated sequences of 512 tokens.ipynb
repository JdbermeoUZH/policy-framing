{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7fd1c8-16ff-43f2-a049-a27d37ea2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3de210-17ae-42af-a361-fff5a0f66735",
   "metadata": {},
   "source": [
    "# Group types of models (experiment type and model type) and pick best performing in terms of f1-score per unit of analysis and report them in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd704f0c-6f9a-47d2-b05b-150230e363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filepaths = glob.glob('./logged_performance_per_model/*/*agg*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['./logged_performance_per_model/distilbert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_distilbert-base-multilingual-cased-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_distilbert-base-multilingual-cased-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_distilbert-base-multilingual-cased-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_distilbert-base-multilingual-cased-title_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_distilbert-base-multilingual-cased-raw_text_metrics.csv',\n './logged_performance_per_model/distilbert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_distilbert-base-multilingual-cased-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_bert-base-multilingual-cased-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_bert-base-multilingual-cased-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_bert-base-multilingual-cased-title_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_bert-base-multilingual-cased-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_bert-base-multilingual-cased-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/bert-base-multilingual-cased/multilingual_fit_trunc_512_tokens_agg_bert-base-multilingual-cased-raw_text_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-1.3B-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-1.3B-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-1.3B-raw_text_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-1.3B-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-1.3B-title_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-1.3B/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-1.3B-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/multilingual_fit_trunc_512_tokens_agg_facebook_mbart-large-50-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/multilingual_fit_trunc_512_tokens_agg_facebook_mbart-large-50-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/multilingual_fit_trunc_512_tokens_agg_facebook_mbart-large-50-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/multilingual_fit_trunc_512_tokens_agg_facebook_mbart-large-50-title_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/multilingual_fit_trunc_512_tokens_agg_facebook_mbart-large-50-raw_text_metrics.csv',\n './logged_performance_per_model/facebook-mbart-large-50/multilingual_fit_trunc_512_tokens_agg_facebook_mbart-large-50-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/gpt2/multilingual_fit_trunc_512_tokens_agg_gpt2-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/gpt2/multilingual_fit_trunc_512_tokens_agg_gpt2-raw_text_metrics.csv',\n './logged_performance_per_model/gpt2/multilingual_fit_trunc_512_tokens_agg_gpt2-title_metrics.csv',\n './logged_performance_per_model/gpt2/multilingual_fit_trunc_512_tokens_agg_gpt2-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/gpt2/multilingual_fit_trunc_512_tokens_agg_gpt2-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/gpt2/multilingual_fit_trunc_512_tokens_agg_gpt2-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-125M-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-125M-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-125M-title_and_first_sentence_each_paragraph_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-125M-title_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-125M-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/EleutherAI-gpt-neo-125M/multilingual_fit_trunc_512_tokens_agg_EleutherAI_gpt-neo-125M-raw_text_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/multilingual_fit_trunc_512_tokens_agg_xlm-roberta-large-title_and_5_sentences_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/multilingual_fit_trunc_512_tokens_agg_xlm-roberta-large-title_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/multilingual_fit_trunc_512_tokens_agg_xlm-roberta-large-title_and_10_sentences_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/multilingual_fit_trunc_512_tokens_agg_xlm-roberta-large-raw_text_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/multilingual_fit_trunc_512_tokens_agg_xlm-roberta-large-title_and_first_paragraph_metrics.csv',\n './logged_performance_per_model/xlm-roberta-large/multilingual_fit_trunc_512_tokens_agg_xlm-roberta-large-title_and_first_sentence_each_paragraph_metrics.csv']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filepaths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                          f1-micro_mean  \\\nlanguage model_name              unit_of_analysis                                         \nen       EleutherAI-gpt-neo-1.3B raw_text                                      0.680338   \n                                 title                                         0.619836   \n                                 title_and_10_sentences                        0.678407   \n                                 title_and_5_sentences                         0.659923   \n                                 title_and_first_paragraph                     0.669201   \n...                                                                                 ...   \nru       xlm-roberta-large       title                                         0.464124   \n                                 title_and_10_sentences                        0.559500   \n                                 title_and_5_sentences                         0.531625   \n                                 title_and_first_paragraph                     0.485692   \n                                 title_and_first_sentence_each_paragraph       0.547791   \n\n                                                                          f1-micro_std  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.021590   \n                                 title                                        0.028503   \n                                 title_and_10_sentences                       0.024102   \n                                 title_and_5_sentences                        0.017966   \n                                 title_and_first_paragraph                    0.017564   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.021064   \n                                 title_and_10_sentences                       0.039932   \n                                 title_and_5_sentences                        0.031019   \n                                 title_and_first_paragraph                    0.035329   \n                                 title_and_first_sentence_each_paragraph      0.024155   \n\n                                                                          precision-micro_mean  \\\nlanguage model_name              unit_of_analysis                                                \nen       EleutherAI-gpt-neo-1.3B raw_text                                             0.757036   \n                                 title                                                0.698842   \n                                 title_and_10_sentences                               0.771642   \n                                 title_and_5_sentences                                0.744041   \n                                 title_and_first_paragraph                            0.760164   \n...                                                                                        ...   \nru       xlm-roberta-large       title                                                0.544857   \n                                 title_and_10_sentences                               0.588743   \n                                 title_and_5_sentences                                0.568975   \n                                 title_and_first_paragraph                            0.567818   \n                                 title_and_first_sentence_each_paragraph              0.604673   \n\n                                                                          precision-micro_std  \\\nlanguage model_name              unit_of_analysis                                               \nen       EleutherAI-gpt-neo-1.3B raw_text                                            0.036201   \n                                 title                                               0.039243   \n                                 title_and_10_sentences                              0.022577   \n                                 title_and_5_sentences                               0.036081   \n                                 title_and_first_paragraph                           0.041103   \n...                                                                                       ...   \nru       xlm-roberta-large       title                                               0.022883   \n                                 title_and_10_sentences                              0.058294   \n                                 title_and_5_sentences                               0.045799   \n                                 title_and_first_paragraph                           0.074323   \n                                 title_and_first_sentence_each_paragraph             0.033945   \n\n                                                                          recall-micro_mean  \\\nlanguage model_name              unit_of_analysis                                             \nen       EleutherAI-gpt-neo-1.3B raw_text                                          0.618028   \n                                 title                                             0.557861   \n                                 title_and_10_sentences                            0.605301   \n                                 title_and_5_sentences                             0.593165   \n                                 title_and_first_paragraph                         0.598308   \n...                                                                                     ...   \nru       xlm-roberta-large       title                                             0.404234   \n                                 title_and_10_sentences                            0.534383   \n                                 title_and_5_sentences                             0.500251   \n                                 title_and_first_paragraph                         0.430418   \n                                 title_and_first_sentence_each_paragraph           0.505778   \n\n                                                                          recall-micro_std  \\\nlanguage model_name              unit_of_analysis                                            \nen       EleutherAI-gpt-neo-1.3B raw_text                                         0.015041   \n                                 title                                            0.034008   \n                                 title_and_10_sentences                           0.024534   \n                                 title_and_5_sentences                            0.006403   \n                                 title_and_first_paragraph                        0.014312   \n...                                                                                    ...   \nru       xlm-roberta-large       title                                            0.019387   \n                                 title_and_10_sentences                           0.038107   \n                                 title_and_5_sentences                            0.035962   \n                                 title_and_first_paragraph                        0.055271   \n                                 title_and_first_sentence_each_paragraph          0.063864   \n\n                                                                          roc-auc_mean  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.770165   \n                                 title                                        0.731642   \n                                 title_and_10_sentences                       0.767639   \n                                 title_and_5_sentences                        0.756439   \n                                 title_and_first_paragraph                    0.762168   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.666267   \n                                 title_and_10_sentences                       0.727331   \n                                 title_and_5_sentences                        0.709813   \n                                 title_and_first_paragraph                    0.679757   \n                                 title_and_first_sentence_each_paragraph      0.717610   \n\n                                                                          roc-auc_std  \\\nlanguage model_name              unit_of_analysis                                       \nen       EleutherAI-gpt-neo-1.3B raw_text                                    0.010745   \n                                 title                                       0.015480   \n                                 title_and_10_sentences                      0.014419   \n                                 title_and_5_sentences                       0.009565   \n                                 title_and_first_paragraph                   0.008821   \n...                                                                               ...   \nru       xlm-roberta-large       title                                       0.008018   \n                                 title_and_10_sentences                      0.024047   \n                                 title_and_5_sentences                       0.020059   \n                                 title_and_first_paragraph                   0.021258   \n                                 title_and_first_sentence_each_paragraph     0.025582   \n\n                                                                          accuracy_mean  \\\nlanguage model_name              unit_of_analysis                                         \nen       EleutherAI-gpt-neo-1.3B raw_text                                      0.116744   \n                                 title                                         0.058344   \n                                 title_and_10_sentences                        0.114738   \n                                 title_and_5_sentences                         0.095324   \n                                 title_and_first_paragraph                     0.103076   \n...                                                                                 ...   \nru       xlm-roberta-large       title                                         0.131118   \n                                 title_and_10_sentences                        0.146908   \n                                 title_and_5_sentences                         0.141617   \n                                 title_and_first_paragraph                     0.131200   \n                                 title_and_first_sentence_each_paragraph       0.162285   \n\n                                                                          accuracy_std  \nlanguage model_name              unit_of_analysis                                       \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.026872  \n                                 title                                        0.011493  \n                                 title_and_10_sentences                       0.033966  \n                                 title_and_5_sentences                        0.029398  \n                                 title_and_first_paragraph                    0.024063  \n...                                                                                ...  \nru       xlm-roberta-large       title                                        0.040772  \n                                 title_and_10_sentences                       0.056562  \n                                 title_and_5_sentences                        0.043028  \n                                 title_and_first_paragraph                    0.056432  \n                                 title_and_first_sentence_each_paragraph      0.008397  \n\n[252 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>f1-micro_mean</th>\n      <th>f1-micro_std</th>\n      <th>precision-micro_mean</th>\n      <th>precision-micro_std</th>\n      <th>recall-micro_mean</th>\n      <th>recall-micro_std</th>\n      <th>roc-auc_mean</th>\n      <th>roc-auc_std</th>\n      <th>accuracy_mean</th>\n      <th>accuracy_std</th>\n    </tr>\n    <tr>\n      <th>language</th>\n      <th>model_name</th>\n      <th>unit_of_analysis</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">en</th>\n      <th rowspan=\"5\" valign=\"top\">EleutherAI-gpt-neo-1.3B</th>\n      <th>raw_text</th>\n      <td>0.680338</td>\n      <td>0.021590</td>\n      <td>0.757036</td>\n      <td>0.036201</td>\n      <td>0.618028</td>\n      <td>0.015041</td>\n      <td>0.770165</td>\n      <td>0.010745</td>\n      <td>0.116744</td>\n      <td>0.026872</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>0.619836</td>\n      <td>0.028503</td>\n      <td>0.698842</td>\n      <td>0.039243</td>\n      <td>0.557861</td>\n      <td>0.034008</td>\n      <td>0.731642</td>\n      <td>0.015480</td>\n      <td>0.058344</td>\n      <td>0.011493</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.678407</td>\n      <td>0.024102</td>\n      <td>0.771642</td>\n      <td>0.022577</td>\n      <td>0.605301</td>\n      <td>0.024534</td>\n      <td>0.767639</td>\n      <td>0.014419</td>\n      <td>0.114738</td>\n      <td>0.033966</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.659923</td>\n      <td>0.017966</td>\n      <td>0.744041</td>\n      <td>0.036081</td>\n      <td>0.593165</td>\n      <td>0.006403</td>\n      <td>0.756439</td>\n      <td>0.009565</td>\n      <td>0.095324</td>\n      <td>0.029398</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.669201</td>\n      <td>0.017564</td>\n      <td>0.760164</td>\n      <td>0.041103</td>\n      <td>0.598308</td>\n      <td>0.014312</td>\n      <td>0.762168</td>\n      <td>0.008821</td>\n      <td>0.103076</td>\n      <td>0.024063</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">ru</th>\n      <th rowspan=\"5\" valign=\"top\">xlm-roberta-large</th>\n      <th>title</th>\n      <td>0.464124</td>\n      <td>0.021064</td>\n      <td>0.544857</td>\n      <td>0.022883</td>\n      <td>0.404234</td>\n      <td>0.019387</td>\n      <td>0.666267</td>\n      <td>0.008018</td>\n      <td>0.131118</td>\n      <td>0.040772</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.559500</td>\n      <td>0.039932</td>\n      <td>0.588743</td>\n      <td>0.058294</td>\n      <td>0.534383</td>\n      <td>0.038107</td>\n      <td>0.727331</td>\n      <td>0.024047</td>\n      <td>0.146908</td>\n      <td>0.056562</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.531625</td>\n      <td>0.031019</td>\n      <td>0.568975</td>\n      <td>0.045799</td>\n      <td>0.500251</td>\n      <td>0.035962</td>\n      <td>0.709813</td>\n      <td>0.020059</td>\n      <td>0.141617</td>\n      <td>0.043028</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.485692</td>\n      <td>0.035329</td>\n      <td>0.567818</td>\n      <td>0.074323</td>\n      <td>0.430418</td>\n      <td>0.055271</td>\n      <td>0.679757</td>\n      <td>0.021258</td>\n      <td>0.131200</td>\n      <td>0.056432</td>\n    </tr>\n    <tr>\n      <th>title_and_first_sentence_each_paragraph</th>\n      <td>0.547791</td>\n      <td>0.024155</td>\n      <td>0.604673</td>\n      <td>0.033945</td>\n      <td>0.505778</td>\n      <td>0.063864</td>\n      <td>0.717610</td>\n      <td>0.025582</td>\n      <td>0.162285</td>\n      <td>0.008397</td>\n    </tr>\n  </tbody>\n</table>\n<p>252 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_list = []\n",
    "for results_filepath in results_filepaths:\n",
    "    model_name = results_filepath.split('/')[-2]\n",
    "    results_df_i = pd.read_csv(results_filepath)\n",
    "    results_df_i['model_name'] = model_name\n",
    "    dfs_list.append(results_df_i)\n",
    "\n",
    "results_df = pd.concat(dfs_list).set_index(['language', 'model_name', 'unit_of_analysis']).sort_index()\n",
    "results_df.rename(columns={'f1-mico_mean': 'f1-micro_mean', 'f1-mico_std': 'f1-micro_std'}, inplace=True)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "results_df.to_csv('performance_of_models.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6a452c68-3d2e-43a1-b820-a7688df2cd33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the tables to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c97b164-c359-40ea-b974-a0dc65c1db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_table(df, metric, index_cols=['model_name'], display_=True):\n",
    "    report_table = df.reset_index().copy()\n",
    "    report_table['result'] = report_table[f'{metric}_mean'].map(lambda x: f'{x:.2f}') + \\\n",
    "    ' $\\pm$ ' + report_table[f'{metric}_std'].map(lambda x: f'{x:.2f}')\n",
    "    report_table['col_title'] = report_table.unit_of_analysis.str.split('_').str.join(' ') \n",
    "    report_table['col_title'] = pd.Categorical(\n",
    "        report_table.col_title,\n",
    "        categories=['title', 'title and first paragraph', 'title and 5 sentences', 'title and 10 sentences',\n",
    "                    'title and first sentence each paragraph', 'raw text'],\n",
    "        ordered=True)\n",
    "    report_table = report_table[index_cols + ['col_title', 'result']]\\\n",
    "        .pivot_table(index=index_cols, columns=['col_title'], values=['result'], aggfunc='first', fill_value=0)\\\n",
    "        .droplevel(0, axis=1)\n",
    "\n",
    "    report_table.columns.names = [None]\n",
    "\n",
    "    # Highlight best scoring models according to their average\n",
    "    mean_perf_arr = report_table.applymap(lambda x: float(str(x).split(' ')[0])).to_numpy()\n",
    "    highlight_mask = mean_perf_arr == mean_perf_arr.max()\n",
    "    report_table_arr = report_table.to_numpy()  # Note it passes the array by reference\n",
    "    report_table_arr[highlight_mask] = '**' + report_table_arr[highlight_mask] + '**'\n",
    "\n",
    "    if display_:\n",
    "        display(Markdown(report_table.to_markdown()))\n",
    "    \n",
    "    return report_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.03 | 0.67 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.52 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.00         | 0.62 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.60 $\\pm$ 0.05 | 0.65 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.01          | 0.68 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | **0.71 $\\pm$ 0.01**     | **0.71 $\\pm$ 0.01**      | **0.71 $\\pm$ 0.01**                       | **0.71 $\\pm$ 0.00** |\n| gpt2                               | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.66 $\\pm$ 0.00          | 0.66 $\\pm$ 0.02                           | 0.66 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.67 $\\pm$ 0.00 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.00         | **0.71 $\\pm$ 0.01**      | **0.71 $\\pm$ 0.01**                       | 0.70 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                              title title and first paragraph  \\\nmodel_name                                                                      \nEleutherAI-gpt-neo-1.3B             0.62 $\\pm$ 0.03           0.67 $\\pm$ 0.02   \nEleutherAI-gpt-neo-125M             0.52 $\\pm$ 0.02           0.59 $\\pm$ 0.02   \nbert-base-multilingual-cased        0.64 $\\pm$ 0.03           0.68 $\\pm$ 0.01   \ndistilbert-base-multilingual-cased  0.60 $\\pm$ 0.05           0.65 $\\pm$ 0.01   \nfacebook-mbart-large-50             0.66 $\\pm$ 0.01           0.70 $\\pm$ 0.01   \ngpt2                                0.64 $\\pm$ 0.03           0.68 $\\pm$ 0.01   \nxlm-roberta-large                   0.67 $\\pm$ 0.00           0.70 $\\pm$ 0.01   \n\n                                   title and 5 sentences  \\\nmodel_name                                                 \nEleutherAI-gpt-neo-1.3B                  0.66 $\\pm$ 0.02   \nEleutherAI-gpt-neo-125M                  0.62 $\\pm$ 0.00   \nbert-base-multilingual-cased             0.69 $\\pm$ 0.01   \ndistilbert-base-multilingual-cased       0.67 $\\pm$ 0.01   \nfacebook-mbart-large-50              **0.71 $\\pm$ 0.01**   \ngpt2                                     0.66 $\\pm$ 0.01   \nxlm-roberta-large                        0.70 $\\pm$ 0.00   \n\n                                   title and 10 sentences  \\\nmodel_name                                                  \nEleutherAI-gpt-neo-1.3B                   0.68 $\\pm$ 0.02   \nEleutherAI-gpt-neo-125M                   0.62 $\\pm$ 0.02   \nbert-base-multilingual-cased              0.70 $\\pm$ 0.01   \ndistilbert-base-multilingual-cased        0.68 $\\pm$ 0.01   \nfacebook-mbart-large-50               **0.71 $\\pm$ 0.01**   \ngpt2                                      0.66 $\\pm$ 0.00   \nxlm-roberta-large                     **0.71 $\\pm$ 0.01**   \n\n                                   title and first sentence each paragraph  \\\nmodel_name                                                                   \nEleutherAI-gpt-neo-1.3B                                    0.67 $\\pm$ 0.02   \nEleutherAI-gpt-neo-125M                                    0.62 $\\pm$ 0.01   \nbert-base-multilingual-cased                               0.69 $\\pm$ 0.01   \ndistilbert-base-multilingual-cased                         0.68 $\\pm$ 0.03   \nfacebook-mbart-large-50                                **0.71 $\\pm$ 0.01**   \ngpt2                                                       0.66 $\\pm$ 0.02   \nxlm-roberta-large                                      **0.71 $\\pm$ 0.01**   \n\n                                               raw text  \nmodel_name                                               \nEleutherAI-gpt-neo-1.3B                 0.68 $\\pm$ 0.02  \nEleutherAI-gpt-neo-125M                 0.60 $\\pm$ 0.02  \nbert-base-multilingual-cased            0.70 $\\pm$ 0.01  \ndistilbert-base-multilingual-cased      0.68 $\\pm$ 0.02  \nfacebook-mbart-large-50             **0.71 $\\pm$ 0.00**  \ngpt2                                    0.66 $\\pm$ 0.02  \nxlm-roberta-large                       0.70 $\\pm$ 0.01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>title and first paragraph</th>\n      <th>title and 5 sentences</th>\n      <th>title and 10 sentences</th>\n      <th>title and first sentence each paragraph</th>\n      <th>raw text</th>\n    </tr>\n    <tr>\n      <th>model_name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>EleutherAI-gpt-neo-1.3B</th>\n      <td>0.62 $\\pm$ 0.03</td>\n      <td>0.67 $\\pm$ 0.02</td>\n      <td>0.66 $\\pm$ 0.02</td>\n      <td>0.68 $\\pm$ 0.02</td>\n      <td>0.67 $\\pm$ 0.02</td>\n      <td>0.68 $\\pm$ 0.02</td>\n    </tr>\n    <tr>\n      <th>EleutherAI-gpt-neo-125M</th>\n      <td>0.52 $\\pm$ 0.02</td>\n      <td>0.59 $\\pm$ 0.02</td>\n      <td>0.62 $\\pm$ 0.00</td>\n      <td>0.62 $\\pm$ 0.02</td>\n      <td>0.62 $\\pm$ 0.01</td>\n      <td>0.60 $\\pm$ 0.02</td>\n    </tr>\n    <tr>\n      <th>bert-base-multilingual-cased</th>\n      <td>0.64 $\\pm$ 0.03</td>\n      <td>0.68 $\\pm$ 0.01</td>\n      <td>0.69 $\\pm$ 0.01</td>\n      <td>0.70 $\\pm$ 0.01</td>\n      <td>0.69 $\\pm$ 0.01</td>\n      <td>0.70 $\\pm$ 0.01</td>\n    </tr>\n    <tr>\n      <th>distilbert-base-multilingual-cased</th>\n      <td>0.60 $\\pm$ 0.05</td>\n      <td>0.65 $\\pm$ 0.01</td>\n      <td>0.67 $\\pm$ 0.01</td>\n      <td>0.68 $\\pm$ 0.01</td>\n      <td>0.68 $\\pm$ 0.03</td>\n      <td>0.68 $\\pm$ 0.02</td>\n    </tr>\n    <tr>\n      <th>facebook-mbart-large-50</th>\n      <td>0.66 $\\pm$ 0.01</td>\n      <td>0.70 $\\pm$ 0.01</td>\n      <td>**0.71 $\\pm$ 0.01**</td>\n      <td>**0.71 $\\pm$ 0.01**</td>\n      <td>**0.71 $\\pm$ 0.01**</td>\n      <td>**0.71 $\\pm$ 0.00**</td>\n    </tr>\n    <tr>\n      <th>gpt2</th>\n      <td>0.64 $\\pm$ 0.03</td>\n      <td>0.68 $\\pm$ 0.01</td>\n      <td>0.66 $\\pm$ 0.01</td>\n      <td>0.66 $\\pm$ 0.00</td>\n      <td>0.66 $\\pm$ 0.02</td>\n      <td>0.66 $\\pm$ 0.02</td>\n    </tr>\n    <tr>\n      <th>xlm-roberta-large</th>\n      <td>0.67 $\\pm$ 0.00</td>\n      <td>0.70 $\\pm$ 0.01</td>\n      <td>0.70 $\\pm$ 0.00</td>\n      <td>**0.71 $\\pm$ 0.01**</td>\n      <td>**0.71 $\\pm$ 0.01**</td>\n      <td>0.70 $\\pm$ 0.01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_performance_table(df=results_df.loc['en'], metric='f1-micro', index_cols=['model_name'], display_=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6f14e8da-1acf-4114-9cfd-eeaa97261f68",
   "metadata": {},
   "source": [
    "### Generate tables for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea51df3-87ed-4ace-ad56-25d235b5cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_report = ['f1-micro', 'recall-micro', 'precision-micro', 'roc-auc', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1650fa17-5ccc-439a-9818-d61bf369ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'en': 'English', 'it': 'Italian', 'fr': 'French', 'po': 'Polish', 'ru': 'Russian', 'ge': 'German'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                          f1-micro_mean  \\\nlanguage model_name              unit_of_analysis                                         \nen       EleutherAI-gpt-neo-1.3B raw_text                                      0.680338   \n                                 title                                         0.619836   \n                                 title_and_10_sentences                        0.678407   \n                                 title_and_5_sentences                         0.659923   \n                                 title_and_first_paragraph                     0.669201   \n...                                                                                 ...   \nru       xlm-roberta-large       title                                         0.464124   \n                                 title_and_10_sentences                        0.559500   \n                                 title_and_5_sentences                         0.531625   \n                                 title_and_first_paragraph                     0.485692   \n                                 title_and_first_sentence_each_paragraph       0.547791   \n\n                                                                          f1-micro_std  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.021590   \n                                 title                                        0.028503   \n                                 title_and_10_sentences                       0.024102   \n                                 title_and_5_sentences                        0.017966   \n                                 title_and_first_paragraph                    0.017564   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.021064   \n                                 title_and_10_sentences                       0.039932   \n                                 title_and_5_sentences                        0.031019   \n                                 title_and_first_paragraph                    0.035329   \n                                 title_and_first_sentence_each_paragraph      0.024155   \n\n                                                                          precision-micro_mean  \\\nlanguage model_name              unit_of_analysis                                                \nen       EleutherAI-gpt-neo-1.3B raw_text                                             0.757036   \n                                 title                                                0.698842   \n                                 title_and_10_sentences                               0.771642   \n                                 title_and_5_sentences                                0.744041   \n                                 title_and_first_paragraph                            0.760164   \n...                                                                                        ...   \nru       xlm-roberta-large       title                                                0.544857   \n                                 title_and_10_sentences                               0.588743   \n                                 title_and_5_sentences                                0.568975   \n                                 title_and_first_paragraph                            0.567818   \n                                 title_and_first_sentence_each_paragraph              0.604673   \n\n                                                                          precision-micro_std  \\\nlanguage model_name              unit_of_analysis                                               \nen       EleutherAI-gpt-neo-1.3B raw_text                                            0.036201   \n                                 title                                               0.039243   \n                                 title_and_10_sentences                              0.022577   \n                                 title_and_5_sentences                               0.036081   \n                                 title_and_first_paragraph                           0.041103   \n...                                                                                       ...   \nru       xlm-roberta-large       title                                               0.022883   \n                                 title_and_10_sentences                              0.058294   \n                                 title_and_5_sentences                               0.045799   \n                                 title_and_first_paragraph                           0.074323   \n                                 title_and_first_sentence_each_paragraph             0.033945   \n\n                                                                          recall-micro_mean  \\\nlanguage model_name              unit_of_analysis                                             \nen       EleutherAI-gpt-neo-1.3B raw_text                                          0.618028   \n                                 title                                             0.557861   \n                                 title_and_10_sentences                            0.605301   \n                                 title_and_5_sentences                             0.593165   \n                                 title_and_first_paragraph                         0.598308   \n...                                                                                     ...   \nru       xlm-roberta-large       title                                             0.404234   \n                                 title_and_10_sentences                            0.534383   \n                                 title_and_5_sentences                             0.500251   \n                                 title_and_first_paragraph                         0.430418   \n                                 title_and_first_sentence_each_paragraph           0.505778   \n\n                                                                          recall-micro_std  \\\nlanguage model_name              unit_of_analysis                                            \nen       EleutherAI-gpt-neo-1.3B raw_text                                         0.015041   \n                                 title                                            0.034008   \n                                 title_and_10_sentences                           0.024534   \n                                 title_and_5_sentences                            0.006403   \n                                 title_and_first_paragraph                        0.014312   \n...                                                                                    ...   \nru       xlm-roberta-large       title                                            0.019387   \n                                 title_and_10_sentences                           0.038107   \n                                 title_and_5_sentences                            0.035962   \n                                 title_and_first_paragraph                        0.055271   \n                                 title_and_first_sentence_each_paragraph          0.063864   \n\n                                                                          roc-auc_mean  \\\nlanguage model_name              unit_of_analysis                                        \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.770165   \n                                 title                                        0.731642   \n                                 title_and_10_sentences                       0.767639   \n                                 title_and_5_sentences                        0.756439   \n                                 title_and_first_paragraph                    0.762168   \n...                                                                                ...   \nru       xlm-roberta-large       title                                        0.666267   \n                                 title_and_10_sentences                       0.727331   \n                                 title_and_5_sentences                        0.709813   \n                                 title_and_first_paragraph                    0.679757   \n                                 title_and_first_sentence_each_paragraph      0.717610   \n\n                                                                          roc-auc_std  \\\nlanguage model_name              unit_of_analysis                                       \nen       EleutherAI-gpt-neo-1.3B raw_text                                    0.010745   \n                                 title                                       0.015480   \n                                 title_and_10_sentences                      0.014419   \n                                 title_and_5_sentences                       0.009565   \n                                 title_and_first_paragraph                   0.008821   \n...                                                                               ...   \nru       xlm-roberta-large       title                                       0.008018   \n                                 title_and_10_sentences                      0.024047   \n                                 title_and_5_sentences                       0.020059   \n                                 title_and_first_paragraph                   0.021258   \n                                 title_and_first_sentence_each_paragraph     0.025582   \n\n                                                                          accuracy_mean  \\\nlanguage model_name              unit_of_analysis                                         \nen       EleutherAI-gpt-neo-1.3B raw_text                                      0.116744   \n                                 title                                         0.058344   \n                                 title_and_10_sentences                        0.114738   \n                                 title_and_5_sentences                         0.095324   \n                                 title_and_first_paragraph                     0.103076   \n...                                                                                 ...   \nru       xlm-roberta-large       title                                         0.131118   \n                                 title_and_10_sentences                        0.146908   \n                                 title_and_5_sentences                         0.141617   \n                                 title_and_first_paragraph                     0.131200   \n                                 title_and_first_sentence_each_paragraph       0.162285   \n\n                                                                          accuracy_std  \nlanguage model_name              unit_of_analysis                                       \nen       EleutherAI-gpt-neo-1.3B raw_text                                     0.026872  \n                                 title                                        0.011493  \n                                 title_and_10_sentences                       0.033966  \n                                 title_and_5_sentences                        0.029398  \n                                 title_and_first_paragraph                    0.024063  \n...                                                                                ...  \nru       xlm-roberta-large       title                                        0.040772  \n                                 title_and_10_sentences                       0.056562  \n                                 title_and_5_sentences                        0.043028  \n                                 title_and_first_paragraph                    0.056432  \n                                 title_and_first_sentence_each_paragraph      0.008397  \n\n[252 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>f1-micro_mean</th>\n      <th>f1-micro_std</th>\n      <th>precision-micro_mean</th>\n      <th>precision-micro_std</th>\n      <th>recall-micro_mean</th>\n      <th>recall-micro_std</th>\n      <th>roc-auc_mean</th>\n      <th>roc-auc_std</th>\n      <th>accuracy_mean</th>\n      <th>accuracy_std</th>\n    </tr>\n    <tr>\n      <th>language</th>\n      <th>model_name</th>\n      <th>unit_of_analysis</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">en</th>\n      <th rowspan=\"5\" valign=\"top\">EleutherAI-gpt-neo-1.3B</th>\n      <th>raw_text</th>\n      <td>0.680338</td>\n      <td>0.021590</td>\n      <td>0.757036</td>\n      <td>0.036201</td>\n      <td>0.618028</td>\n      <td>0.015041</td>\n      <td>0.770165</td>\n      <td>0.010745</td>\n      <td>0.116744</td>\n      <td>0.026872</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>0.619836</td>\n      <td>0.028503</td>\n      <td>0.698842</td>\n      <td>0.039243</td>\n      <td>0.557861</td>\n      <td>0.034008</td>\n      <td>0.731642</td>\n      <td>0.015480</td>\n      <td>0.058344</td>\n      <td>0.011493</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.678407</td>\n      <td>0.024102</td>\n      <td>0.771642</td>\n      <td>0.022577</td>\n      <td>0.605301</td>\n      <td>0.024534</td>\n      <td>0.767639</td>\n      <td>0.014419</td>\n      <td>0.114738</td>\n      <td>0.033966</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.659923</td>\n      <td>0.017966</td>\n      <td>0.744041</td>\n      <td>0.036081</td>\n      <td>0.593165</td>\n      <td>0.006403</td>\n      <td>0.756439</td>\n      <td>0.009565</td>\n      <td>0.095324</td>\n      <td>0.029398</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.669201</td>\n      <td>0.017564</td>\n      <td>0.760164</td>\n      <td>0.041103</td>\n      <td>0.598308</td>\n      <td>0.014312</td>\n      <td>0.762168</td>\n      <td>0.008821</td>\n      <td>0.103076</td>\n      <td>0.024063</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">ru</th>\n      <th rowspan=\"5\" valign=\"top\">xlm-roberta-large</th>\n      <th>title</th>\n      <td>0.464124</td>\n      <td>0.021064</td>\n      <td>0.544857</td>\n      <td>0.022883</td>\n      <td>0.404234</td>\n      <td>0.019387</td>\n      <td>0.666267</td>\n      <td>0.008018</td>\n      <td>0.131118</td>\n      <td>0.040772</td>\n    </tr>\n    <tr>\n      <th>title_and_10_sentences</th>\n      <td>0.559500</td>\n      <td>0.039932</td>\n      <td>0.588743</td>\n      <td>0.058294</td>\n      <td>0.534383</td>\n      <td>0.038107</td>\n      <td>0.727331</td>\n      <td>0.024047</td>\n      <td>0.146908</td>\n      <td>0.056562</td>\n    </tr>\n    <tr>\n      <th>title_and_5_sentences</th>\n      <td>0.531625</td>\n      <td>0.031019</td>\n      <td>0.568975</td>\n      <td>0.045799</td>\n      <td>0.500251</td>\n      <td>0.035962</td>\n      <td>0.709813</td>\n      <td>0.020059</td>\n      <td>0.141617</td>\n      <td>0.043028</td>\n    </tr>\n    <tr>\n      <th>title_and_first_paragraph</th>\n      <td>0.485692</td>\n      <td>0.035329</td>\n      <td>0.567818</td>\n      <td>0.074323</td>\n      <td>0.430418</td>\n      <td>0.055271</td>\n      <td>0.679757</td>\n      <td>0.021258</td>\n      <td>0.131200</td>\n      <td>0.056432</td>\n    </tr>\n    <tr>\n      <th>title_and_first_sentence_each_paragraph</th>\n      <td>0.547791</td>\n      <td>0.024155</td>\n      <td>0.604673</td>\n      <td>0.033945</td>\n      <td>0.505778</td>\n      <td>0.063864</td>\n      <td>0.717610</td>\n      <td>0.025582</td>\n      <td>0.162285</td>\n      <td>0.008397</td>\n    </tr>\n  </tbody>\n</table>\n<p>252 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143ecba4-ffb9-45e9-869c-87d16caea017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_metrics_and_write_to_file(df, grouping_criterion, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    report_tables_dfs_dict = {metric: [] for metric in metrics_to_report}\n",
    "\n",
    "    for language, results_df in df.groupby(level=0):\n",
    "        display(Markdown(f'# {language_dict[language]}'))\n",
    "        \n",
    "        for metric in metrics_to_report:\n",
    "            os.makedirs(os.path.join(output_dir, metric), exist_ok=True)\n",
    "\n",
    "            output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "            output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "            output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "            os.makedirs(output_dir_markdown, exist_ok=True)\n",
    "            os.makedirs(output_dir_latex, exist_ok=True)\n",
    "            os.makedirs(output_dir_csv, exist_ok=True)\n",
    "\n",
    "            display(Markdown(f'## {metric}'))\n",
    "\n",
    "            report_table = display_performance_table(df=results_df, index_cols=grouping_criterion, metric=metric, display_=True)\n",
    "\n",
    "            # Export as markdown\n",
    "            markdown_file = open(os.path.join(output_dir_markdown, f\"{language_dict[language]}_{metric}.md\"), \"w\")\n",
    "            report_table.reset_index().to_markdown(markdown_file, index=False)\n",
    "            markdown_file.close()\n",
    "\n",
    "            # Export as latex table\n",
    "            latex_file = open(os.path.join(output_dir_latex, f\"{language_dict[language]}_{metric}.tex\"), \"w\")\n",
    "            report_table.reset_index().to_latex(latex_file, index=False)\n",
    "            latex_file.close()\n",
    "\n",
    "            # Export as csv\n",
    "            report_table.to_csv(os.path.join(output_dir_csv, f\"{language_dict[language]}_{metric}.csv\"))\n",
    "\n",
    "            # Stack all languages into single table\n",
    "            report_table['language'] = language\n",
    "            report_table = report_table.reset_index().set_index(['language'] + grouping_criterion)\n",
    "\n",
    "            report_tables_dfs_dict[metric].append(report_table)\n",
    "\n",
    "    # Report or store unified table\n",
    "    display(Markdown(f'# All 6 Languages'))\n",
    "    for metric in metrics_to_report:\n",
    "        display(Markdown(f'## {metric}'))\n",
    "        multi_language_report_table_metric = pd.concat(report_tables_dfs_dict[metric])\n",
    "        display(Markdown(multi_language_report_table_metric.reset_index().to_markdown(index=False)))\n",
    "\n",
    "        output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "        output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "        output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "        # Export as markdown\n",
    "        markdown_file = open(os.path.join(output_dir_markdown, f\"all_6_languages_{metric}.md\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_markdown(markdown_file, index=False)\n",
    "        markdown_file.close()\n",
    "\n",
    "        # Export as latex table\n",
    "        latex_file = open(os.path.join(output_dir_latex, f\"all_6_languages_{metric}.tex\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n",
    "        latex_file.close()\n",
    "\n",
    "        # Export as csv\n",
    "        multi_language_report_table_metric.to_csv(os.path.join(output_dir_csv, f\"all_6_languages_{metric}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d754-49ba-4c74-8220-2e144f624044",
   "metadata": {},
   "source": [
    "# Per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5abb824-f93a-44fd-8203-a0b84224b0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# English"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.03 | 0.67 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.52 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.00         | 0.62 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.60 $\\pm$ 0.05 | 0.65 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.01          | 0.68 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | **0.71 $\\pm$ 0.01**     | **0.71 $\\pm$ 0.01**      | **0.71 $\\pm$ 0.01**                       | **0.71 $\\pm$ 0.00** |\n| gpt2                               | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.66 $\\pm$ 0.00          | 0.66 $\\pm$ 0.02                           | 0.66 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.67 $\\pm$ 0.00 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.00         | **0.71 $\\pm$ 0.01**      | **0.71 $\\pm$ 0.01**                       | 0.70 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.03 | 0.60 $\\pm$ 0.01             | 0.59 $\\pm$ 0.01         | 0.61 $\\pm$ 0.02          | 0.60 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.46 $\\pm$ 0.01 | 0.52 $\\pm$ 0.01             | 0.54 $\\pm$ 0.02         | 0.56 $\\pm$ 0.01          | 0.56 $\\pm$ 0.01                           | 0.54 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.59 $\\pm$ 0.02 | 0.62 $\\pm$ 0.02             | 0.64 $\\pm$ 0.01         | 0.65 $\\pm$ 0.01          | 0.64 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.04 | 0.58 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.64 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.61 $\\pm$ 0.01 | 0.65 $\\pm$ 0.02             | 0.66 $\\pm$ 0.01         | 0.66 $\\pm$ 0.02          | 0.66 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| gpt2                               | 0.61 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.64 $\\pm$ 0.01         | 0.65 $\\pm$ 0.04          | 0.67 $\\pm$ 0.03                           | **0.70 $\\pm$ 0.00** |\n| xlm-roberta-large                  | 0.63 $\\pm$ 0.02 | 0.65 $\\pm$ 0.03             | 0.66 $\\pm$ 0.00         | 0.67 $\\pm$ 0.01          | 0.68 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.70 $\\pm$ 0.04 | 0.76 $\\pm$ 0.04             | 0.74 $\\pm$ 0.04         | **0.77 $\\pm$ 0.02**      | **0.77 $\\pm$ 0.03**                       | 0.76 $\\pm$ 0.04 |\n| EleutherAI-gpt-neo-125M            | 0.62 $\\pm$ 0.07 | 0.67 $\\pm$ 0.05             | 0.71 $\\pm$ 0.04         | 0.70 $\\pm$ 0.04          | 0.70 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.04 |\n| bert-base-multilingual-cased       | 0.70 $\\pm$ 0.04 | 0.75 $\\pm$ 0.02             | 0.75 $\\pm$ 0.01         | 0.75 $\\pm$ 0.02          | 0.75 $\\pm$ 0.02                           | 0.76 $\\pm$ 0.03 |\n| distilbert-base-multilingual-cased | 0.68 $\\pm$ 0.07 | 0.73 $\\pm$ 0.02             | 0.73 $\\pm$ 0.02         | 0.74 $\\pm$ 0.03          | 0.74 $\\pm$ 0.05                           | 0.75 $\\pm$ 0.03 |\n| facebook-mbart-large-50            | 0.71 $\\pm$ 0.03 | 0.76 $\\pm$ 0.02             | 0.76 $\\pm$ 0.02         | 0.76 $\\pm$ 0.02          | **0.77 $\\pm$ 0.02**                       | 0.76 $\\pm$ 0.01 |\n| gpt2                               | 0.68 $\\pm$ 0.04 | 0.71 $\\pm$ 0.05             | 0.69 $\\pm$ 0.04         | 0.68 $\\pm$ 0.04          | 0.66 $\\pm$ 0.04                           | 0.63 $\\pm$ 0.04 |\n| xlm-roberta-large                  | 0.72 $\\pm$ 0.02 | 0.76 $\\pm$ 0.02             | 0.75 $\\pm$ 0.00         | 0.76 $\\pm$ 0.02          | 0.74 $\\pm$ 0.02                           | 0.75 $\\pm$ 0.03 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.73 $\\pm$ 0.02 | 0.76 $\\pm$ 0.01             | 0.76 $\\pm$ 0.01         | 0.77 $\\pm$ 0.01          | 0.76 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.67 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.73 $\\pm$ 0.00         | 0.73 $\\pm$ 0.01          | 0.73 $\\pm$ 0.00                           | 0.72 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.74 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.78 $\\pm$ 0.01         | 0.78 $\\pm$ 0.01          | 0.78 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.72 $\\pm$ 0.03 | 0.75 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.77 $\\pm$ 0.00          | 0.77 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.76 $\\pm$ 0.01 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.00**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.00**                       | **0.79 $\\pm$ 0.00** |\n| gpt2                               | 0.75 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.76 $\\pm$ 0.01         | 0.77 $\\pm$ 0.01          | 0.77 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.77 $\\pm$ 0.00 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.00**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.00**                       | **0.79 $\\pm$ 0.00** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.06 $\\pm$ 0.01 | 0.10 $\\pm$ 0.02             | 0.10 $\\pm$ 0.03         | 0.11 $\\pm$ 0.03          | 0.10 $\\pm$ 0.03                           | **0.12 $\\pm$ 0.03** |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.02 | 0.05 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.00          | 0.07 $\\pm$ 0.01                           | 0.06 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.07 $\\pm$ 0.03 | 0.09 $\\pm$ 0.02             | 0.11 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.02 | 0.08 $\\pm$ 0.02             | 0.10 $\\pm$ 0.02         | 0.11 $\\pm$ 0.01          | 0.11 $\\pm$ 0.03                           | 0.11 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.09 $\\pm$ 0.02 | 0.10 $\\pm$ 0.01             | **0.12 $\\pm$ 0.02**     | 0.09 $\\pm$ 0.04          | **0.12 $\\pm$ 0.02**                       | 0.11 $\\pm$ 0.01     |\n| gpt2                               | 0.08 $\\pm$ 0.03 | 0.09 $\\pm$ 0.01             | 0.08 $\\pm$ 0.02         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.01                           | 0.07 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.09 $\\pm$ 0.01 | 0.10 $\\pm$ 0.02             | 0.11 $\\pm$ 0.02         | 0.11 $\\pm$ 0.02          | **0.12 $\\pm$ 0.02**                       | 0.10 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# French"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.02 | 0.48 $\\pm$ 0.05             | 0.50 $\\pm$ 0.02         | 0.48 $\\pm$ 0.04          | 0.50 $\\pm$ 0.04                           | 0.49 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.29 $\\pm$ 0.03 | 0.37 $\\pm$ 0.01             | 0.40 $\\pm$ 0.03         | 0.39 $\\pm$ 0.04          | 0.42 $\\pm$ 0.03                           | 0.39 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.47 $\\pm$ 0.03 | 0.51 $\\pm$ 0.03             | 0.51 $\\pm$ 0.02         | 0.57 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.01 | 0.51 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.53 $\\pm$ 0.01          | 0.54 $\\pm$ 0.03                           | 0.56 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.52 $\\pm$ 0.03 | 0.55 $\\pm$ 0.02             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | **0.60 $\\pm$ 0.02** |\n| gpt2                               | 0.38 $\\pm$ 0.03 | 0.42 $\\pm$ 0.02             | 0.42 $\\pm$ 0.03         | 0.46 $\\pm$ 0.02          | 0.49 $\\pm$ 0.04                           | 0.51 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.53 $\\pm$ 0.02 | 0.56 $\\pm$ 0.03             | 0.57 $\\pm$ 0.03         | 0.58 $\\pm$ 0.02          | 0.55 $\\pm$ 0.04                           | 0.59 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.37 $\\pm$ 0.03 | 0.39 $\\pm$ 0.06             | 0.43 $\\pm$ 0.03         | 0.39 $\\pm$ 0.05          | 0.42 $\\pm$ 0.06                           | 0.40 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.22 $\\pm$ 0.06 | 0.29 $\\pm$ 0.03             | 0.33 $\\pm$ 0.06         | 0.30 $\\pm$ 0.04          | 0.35 $\\pm$ 0.03                           | 0.31 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.40 $\\pm$ 0.02 | 0.44 $\\pm$ 0.04             | 0.47 $\\pm$ 0.05         | 0.53 $\\pm$ 0.02          | 0.51 $\\pm$ 0.04                           | 0.54 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.02 | 0.45 $\\pm$ 0.02             | 0.46 $\\pm$ 0.03         | 0.48 $\\pm$ 0.02          | 0.48 $\\pm$ 0.05                           | 0.51 $\\pm$ 0.04     |\n| facebook-mbart-large-50            | 0.47 $\\pm$ 0.03 | 0.48 $\\pm$ 0.02             | 0.52 $\\pm$ 0.06         | 0.53 $\\pm$ 0.03          | 0.52 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.01     |\n| gpt2                               | 0.31 $\\pm$ 0.02 | 0.36 $\\pm$ 0.02             | 0.36 $\\pm$ 0.03         | 0.40 $\\pm$ 0.02          | 0.44 $\\pm$ 0.04                           | 0.46 $\\pm$ 0.03     |\n| xlm-roberta-large                  | 0.49 $\\pm$ 0.03 | 0.51 $\\pm$ 0.06             | 0.55 $\\pm$ 0.04         | **0.56 $\\pm$ 0.02**      | 0.53 $\\pm$ 0.07                           | **0.56 $\\pm$ 0.04** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.52 $\\pm$ 0.02 | **0.65 $\\pm$ 0.04**         | 0.58 $\\pm$ 0.02         | 0.64 $\\pm$ 0.01          | 0.62 $\\pm$ 0.03                           | **0.65 $\\pm$ 0.01** |\n| EleutherAI-gpt-neo-125M            | 0.46 $\\pm$ 0.11 | 0.50 $\\pm$ 0.06             | 0.53 $\\pm$ 0.04         | 0.54 $\\pm$ 0.01          | 0.53 $\\pm$ 0.04                           | 0.54 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.57 $\\pm$ 0.04 | 0.60 $\\pm$ 0.03             | 0.58 $\\pm$ 0.02         | 0.62 $\\pm$ 0.03          | 0.63 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.04     |\n| distilbert-base-multilingual-cased | 0.56 $\\pm$ 0.04 | 0.58 $\\pm$ 0.03             | 0.56 $\\pm$ 0.03         | 0.61 $\\pm$ 0.02          | 0.62 $\\pm$ 0.06                           | 0.62 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.59 $\\pm$ 0.05 | 0.63 $\\pm$ 0.02             | 0.62 $\\pm$ 0.05         | **0.65 $\\pm$ 0.04**      | **0.65 $\\pm$ 0.02**                       | **0.65 $\\pm$ 0.03** |\n| gpt2                               | 0.49 $\\pm$ 0.06 | 0.51 $\\pm$ 0.01             | 0.50 $\\pm$ 0.03         | 0.55 $\\pm$ 0.02          | 0.56 $\\pm$ 0.04                           | 0.56 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.58 $\\pm$ 0.01 | 0.62 $\\pm$ 0.03             | 0.60 $\\pm$ 0.04         | 0.59 $\\pm$ 0.01          | 0.58 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.04     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.64 $\\pm$ 0.01 | 0.67 $\\pm$ 0.03             | 0.67 $\\pm$ 0.01         | 0.66 $\\pm$ 0.02          | 0.67 $\\pm$ 0.03                           | 0.67 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.01 | 0.61 $\\pm$ 0.00             | 0.63 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.63 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.72 $\\pm$ 0.02          | 0.71 $\\pm$ 0.02                           | 0.72 $\\pm$ 0.00     |\n| distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.68 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.69 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.72 $\\pm$ 0.02         | 0.72 $\\pm$ 0.01          | 0.72 $\\pm$ 0.01                           | **0.73 $\\pm$ 0.01** |\n| gpt2                               | 0.61 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.63 $\\pm$ 0.02         | 0.66 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.70 $\\pm$ 0.02 | 0.71 $\\pm$ 0.02             | **0.73 $\\pm$ 0.02**     | **0.73 $\\pm$ 0.01**      | 0.71 $\\pm$ 0.03                           | **0.73 $\\pm$ 0.01** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.02 | 0.07 $\\pm$ 0.01             | 0.08 $\\pm$ 0.04         | 0.08 $\\pm$ 0.02          | 0.09 $\\pm$ 0.01                           | 0.08 $\\pm$ 0.03 |\n| EleutherAI-gpt-neo-125M            | 0.02 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.02 $\\pm$ 0.02         | 0.04 $\\pm$ 0.01          | 0.05 $\\pm$ 0.02                           | 0.05 $\\pm$ 0.04 |\n| bert-base-multilingual-cased       | 0.05 $\\pm$ 0.03 | 0.07 $\\pm$ 0.01             | 0.06 $\\pm$ 0.00         | 0.09 $\\pm$ 0.04          | 0.09 $\\pm$ 0.02                           | 0.09 $\\pm$ 0.05 |\n| distilbert-base-multilingual-cased | 0.07 $\\pm$ 0.02 | 0.07 $\\pm$ 0.02             | 0.08 $\\pm$ 0.02         | 0.07 $\\pm$ 0.02          | 0.09 $\\pm$ 0.02                           | 0.09 $\\pm$ 0.03 |\n| facebook-mbart-large-50            | 0.08 $\\pm$ 0.02 | 0.08 $\\pm$ 0.04             | 0.07 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | **0.11 $\\pm$ 0.02**                       | 0.10 $\\pm$ 0.01 |\n| gpt2                               | 0.05 $\\pm$ 0.01 | 0.05 $\\pm$ 0.04             | 0.05 $\\pm$ 0.03         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.03 |\n| xlm-roberta-large                  | 0.09 $\\pm$ 0.00 | 0.08 $\\pm$ 0.04             | 0.08 $\\pm$ 0.04         | 0.09 $\\pm$ 0.04          | 0.09 $\\pm$ 0.02                           | 0.10 $\\pm$ 0.01 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# German"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.47 $\\pm$ 0.01 | 0.55 $\\pm$ 0.01             | 0.57 $\\pm$ 0.03         | 0.57 $\\pm$ 0.03          | 0.56 $\\pm$ 0.00                           | 0.56 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.41 $\\pm$ 0.04 | 0.48 $\\pm$ 0.01             | 0.52 $\\pm$ 0.02         | 0.50 $\\pm$ 0.02          | 0.50 $\\pm$ 0.04                           | 0.48 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.51 $\\pm$ 0.00 | 0.58 $\\pm$ 0.00             | 0.60 $\\pm$ 0.00         | 0.62 $\\pm$ 0.01          | 0.62 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.51 $\\pm$ 0.02 | 0.56 $\\pm$ 0.01             | 0.58 $\\pm$ 0.02         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.53 $\\pm$ 0.04 | 0.60 $\\pm$ 0.02             | 0.61 $\\pm$ 0.01         | 0.65 $\\pm$ 0.01          | 0.64 $\\pm$ 0.03                           | **0.68 $\\pm$ 0.02** |\n| gpt2                               | 0.50 $\\pm$ 0.06 | 0.52 $\\pm$ 0.02             | 0.54 $\\pm$ 0.05         | 0.55 $\\pm$ 0.03          | 0.55 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.57 $\\pm$ 0.01 | 0.60 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.64 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.42 $\\pm$ 0.03 | 0.47 $\\pm$ 0.01             | 0.50 $\\pm$ 0.05         | 0.48 $\\pm$ 0.04          | 0.48 $\\pm$ 0.01                           | 0.47 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.34 $\\pm$ 0.05 | 0.41 $\\pm$ 0.03             | 0.46 $\\pm$ 0.01         | 0.43 $\\pm$ 0.02          | 0.44 $\\pm$ 0.04                           | 0.39 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.45 $\\pm$ 0.01 | 0.54 $\\pm$ 0.02             | 0.54 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | 0.57 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.05 | 0.51 $\\pm$ 0.02             | 0.54 $\\pm$ 0.01         | 0.54 $\\pm$ 0.01          | 0.52 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.04     |\n| facebook-mbart-large-50            | 0.47 $\\pm$ 0.06 | 0.54 $\\pm$ 0.01             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.01          | 0.58 $\\pm$ 0.02                           | **0.62 $\\pm$ 0.03** |\n| gpt2                               | 0.46 $\\pm$ 0.06 | 0.49 $\\pm$ 0.03             | 0.49 $\\pm$ 0.06         | 0.50 $\\pm$ 0.02          | 0.51 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.52 $\\pm$ 0.00 | 0.55 $\\pm$ 0.03             | 0.60 $\\pm$ 0.01         | 0.61 $\\pm$ 0.02          | 0.58 $\\pm$ 0.00                           | **0.62 $\\pm$ 0.01** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.53 $\\pm$ 0.02 | 0.66 $\\pm$ 0.05             | 0.65 $\\pm$ 0.03         | 0.70 $\\pm$ 0.02          | 0.68 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.53 $\\pm$ 0.05 | 0.57 $\\pm$ 0.07             | 0.61 $\\pm$ 0.03         | 0.62 $\\pm$ 0.03          | 0.59 $\\pm$ 0.03                           | 0.61 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.60 $\\pm$ 0.01 | 0.64 $\\pm$ 0.02             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.00     |\n| distilbert-base-multilingual-cased | 0.58 $\\pm$ 0.02 | 0.62 $\\pm$ 0.01             | 0.62 $\\pm$ 0.07         | 0.66 $\\pm$ 0.05          | 0.66 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.62 $\\pm$ 0.04 | 0.66 $\\pm$ 0.04             | 0.66 $\\pm$ 0.02         | 0.73 $\\pm$ 0.01          | 0.71 $\\pm$ 0.04                           | **0.74 $\\pm$ 0.01** |\n| gpt2                               | 0.55 $\\pm$ 0.06 | 0.55 $\\pm$ 0.02             | 0.60 $\\pm$ 0.03         | 0.61 $\\pm$ 0.06          | 0.59 $\\pm$ 0.04                           | 0.60 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.62 $\\pm$ 0.02 | 0.67 $\\pm$ 0.04             | 0.66 $\\pm$ 0.02         | 0.66 $\\pm$ 0.02          | 0.67 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.00     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.02          | 0.69 $\\pm$ 0.00                           | 0.69 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.60 $\\pm$ 0.03 | 0.63 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.65 $\\pm$ 0.01          | 0.65 $\\pm$ 0.02                           | 0.64 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.00             | 0.71 $\\pm$ 0.01         | 0.72 $\\pm$ 0.01          | 0.72 $\\pm$ 0.01                           | 0.73 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.01 | 0.68 $\\pm$ 0.00             | 0.69 $\\pm$ 0.02         | 0.71 $\\pm$ 0.01          | 0.70 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.67 $\\pm$ 0.03 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.01         | 0.74 $\\pm$ 0.01          | 0.74 $\\pm$ 0.02                           | **0.76 $\\pm$ 0.01** |\n| gpt2                               | 0.64 $\\pm$ 0.04 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.04         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.01                           | 0.67 $\\pm$ 0.03     |\n| xlm-roberta-large                  | 0.69 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.73 $\\pm$ 0.00         | 0.74 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | 0.75 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.02 | 0.04 $\\pm$ 0.05             | 0.04 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | 0.05 $\\pm$ 0.04                           | 0.06 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.00 $\\pm$ 0.00 | 0.03 $\\pm$ 0.03             | 0.02 $\\pm$ 0.03         | 0.03 $\\pm$ 0.02          | 0.02 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.08 $\\pm$ 0.04         | 0.09 $\\pm$ 0.01          | 0.06 $\\pm$ 0.02                           | 0.09 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.00 | 0.03 $\\pm$ 0.03             | 0.05 $\\pm$ 0.04         | 0.06 $\\pm$ 0.04          | 0.05 $\\pm$ 0.02                           | 0.05 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.05 $\\pm$ 0.01 | 0.08 $\\pm$ 0.04             | 0.07 $\\pm$ 0.04         | 0.09 $\\pm$ 0.01          | 0.07 $\\pm$ 0.06                           | **0.11 $\\pm$ 0.03** |\n| gpt2                               | 0.03 $\\pm$ 0.03 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.01         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.03                           | 0.02 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.06 $\\pm$ 0.01 | 0.08 $\\pm$ 0.01             | 0.06 $\\pm$ 0.04         | 0.09 $\\pm$ 0.01          | 0.09 $\\pm$ 0.05                           | **0.11 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Italian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.03 | 0.48 $\\pm$ 0.04             | 0.51 $\\pm$ 0.05         | 0.52 $\\pm$ 0.03          | 0.51 $\\pm$ 0.04                           | 0.52 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.32 $\\pm$ 0.02 | 0.38 $\\pm$ 0.03             | 0.42 $\\pm$ 0.03         | 0.45 $\\pm$ 0.03          | 0.43 $\\pm$ 0.02                           | 0.42 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.47 $\\pm$ 0.03 | 0.52 $\\pm$ 0.03             | 0.53 $\\pm$ 0.02         | 0.55 $\\pm$ 0.03          | 0.57 $\\pm$ 0.04                           | 0.58 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.46 $\\pm$ 0.02 | 0.50 $\\pm$ 0.07             | 0.53 $\\pm$ 0.02         | 0.53 $\\pm$ 0.03          | 0.52 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.06     |\n| facebook-mbart-large-50            | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.54 $\\pm$ 0.02         | 0.56 $\\pm$ 0.04          | 0.58 $\\pm$ 0.03                           | **0.63 $\\pm$ 0.03** |\n| gpt2                               | 0.40 $\\pm$ 0.03 | 0.46 $\\pm$ 0.03             | 0.48 $\\pm$ 0.01         | 0.51 $\\pm$ 0.02          | 0.50 $\\pm$ 0.02                           | 0.53 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.52 $\\pm$ 0.03 | 0.55 $\\pm$ 0.03             | 0.57 $\\pm$ 0.01         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.38 $\\pm$ 0.05 | 0.40 $\\pm$ 0.04             | 0.45 $\\pm$ 0.04         | 0.42 $\\pm$ 0.03          | 0.42 $\\pm$ 0.03                           | 0.43 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.26 $\\pm$ 0.03 | 0.31 $\\pm$ 0.04             | 0.35 $\\pm$ 0.02         | 0.37 $\\pm$ 0.01          | 0.35 $\\pm$ 0.01                           | 0.34 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.39 $\\pm$ 0.03 | 0.47 $\\pm$ 0.04             | 0.47 $\\pm$ 0.03         | 0.49 $\\pm$ 0.01          | 0.51 $\\pm$ 0.05                           | 0.53 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.02 | 0.46 $\\pm$ 0.08             | 0.47 $\\pm$ 0.02         | 0.48 $\\pm$ 0.04          | 0.46 $\\pm$ 0.02                           | 0.52 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.43 $\\pm$ 0.02 | 0.49 $\\pm$ 0.06             | 0.49 $\\pm$ 0.04         | 0.50 $\\pm$ 0.04          | 0.53 $\\pm$ 0.01                           | **0.59 $\\pm$ 0.01** |\n| gpt2                               | 0.34 $\\pm$ 0.03 | 0.42 $\\pm$ 0.02             | 0.43 $\\pm$ 0.01         | 0.47 $\\pm$ 0.03          | 0.45 $\\pm$ 0.03                           | 0.50 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.47 $\\pm$ 0.04 | 0.51 $\\pm$ 0.05             | 0.56 $\\pm$ 0.02         | 0.57 $\\pm$ 0.04          | 0.54 $\\pm$ 0.04                           | 0.58 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.50 $\\pm$ 0.04 | 0.63 $\\pm$ 0.05             | 0.59 $\\pm$ 0.05         | 0.66 $\\pm$ 0.01          | 0.65 $\\pm$ 0.05                           | 0.66 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.43 $\\pm$ 0.01 | 0.49 $\\pm$ 0.04             | 0.54 $\\pm$ 0.04         | 0.58 $\\pm$ 0.06          | 0.55 $\\pm$ 0.06                           | 0.56 $\\pm$ 0.05     |\n| bert-base-multilingual-cased       | 0.58 $\\pm$ 0.03 | 0.59 $\\pm$ 0.03             | 0.61 $\\pm$ 0.03         | 0.62 $\\pm$ 0.06          | 0.63 $\\pm$ 0.05                           | 0.63 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.55 $\\pm$ 0.03 | 0.55 $\\pm$ 0.04             | 0.60 $\\pm$ 0.05         | 0.61 $\\pm$ 0.03          | 0.60 $\\pm$ 0.04                           | 0.62 $\\pm$ 0.07     |\n| facebook-mbart-large-50            | 0.58 $\\pm$ 0.03 | 0.60 $\\pm$ 0.03             | 0.61 $\\pm$ 0.04         | 0.64 $\\pm$ 0.06          | 0.65 $\\pm$ 0.07                           | **0.69 $\\pm$ 0.06** |\n| gpt2                               | 0.47 $\\pm$ 0.02 | 0.52 $\\pm$ 0.05             | 0.56 $\\pm$ 0.03         | 0.56 $\\pm$ 0.03          | 0.56 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.60 $\\pm$ 0.05             | 0.59 $\\pm$ 0.02         | 0.62 $\\pm$ 0.03          | 0.62 $\\pm$ 0.05                           | 0.66 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.02 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.03         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.67 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.01 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.64 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.02 | 0.67 $\\pm$ 0.02             | 0.68 $\\pm$ 0.01         | 0.69 $\\pm$ 0.02          | 0.70 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.01 | 0.66 $\\pm$ 0.04             | 0.68 $\\pm$ 0.01         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.04     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.03             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.03          | 0.71 $\\pm$ 0.02                           | **0.74 $\\pm$ 0.02** |\n| gpt2                               | 0.60 $\\pm$ 0.02 | 0.64 $\\pm$ 0.02             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.01          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.67 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.71 $\\pm$ 0.01         | 0.72 $\\pm$ 0.02          | 0.71 $\\pm$ 0.01                           | 0.73 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.02 | 0.07 $\\pm$ 0.02             | 0.06 $\\pm$ 0.01         | 0.07 $\\pm$ 0.03          | 0.06 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.04 $\\pm$ 0.02         | 0.04 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.05 $\\pm$ 0.02 | 0.06 $\\pm$ 0.02             | 0.06 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.06 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.02 | 0.03 $\\pm$ 0.00             | 0.07 $\\pm$ 0.02         | 0.06 $\\pm$ 0.02          | 0.06 $\\pm$ 0.02                           | 0.08 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.05 $\\pm$ 0.02 | 0.08 $\\pm$ 0.03             | 0.06 $\\pm$ 0.03         | 0.08 $\\pm$ 0.03          | 0.07 $\\pm$ 0.05                           | 0.10 $\\pm$ 0.05     |\n| gpt2                               | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.07 $\\pm$ 0.04 | 0.07 $\\pm$ 0.03             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.02                           | **0.11 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Polish"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.52 $\\pm$ 0.01 | 0.54 $\\pm$ 0.04             | 0.61 $\\pm$ 0.03         | 0.59 $\\pm$ 0.02          | 0.61 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.44 $\\pm$ 0.03 | 0.49 $\\pm$ 0.03             | 0.54 $\\pm$ 0.02         | 0.54 $\\pm$ 0.02          | 0.54 $\\pm$ 0.03                           | 0.50 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.53 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.61 $\\pm$ 0.02          | 0.64 $\\pm$ 0.02                           | 0.65 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.51 $\\pm$ 0.04 | 0.58 $\\pm$ 0.03             | 0.61 $\\pm$ 0.03         | 0.62 $\\pm$ 0.04          | 0.61 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.02 | 0.59 $\\pm$ 0.04             | 0.63 $\\pm$ 0.01         | 0.64 $\\pm$ 0.01          | 0.65 $\\pm$ 0.01                           | **0.68 $\\pm$ 0.03** |\n| gpt2                               | 0.47 $\\pm$ 0.01 | 0.53 $\\pm$ 0.01             | 0.56 $\\pm$ 0.03         | 0.57 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.56 $\\pm$ 0.04 | 0.60 $\\pm$ 0.03             | 0.64 $\\pm$ 0.02         | 0.65 $\\pm$ 0.03          | 0.66 $\\pm$ 0.00                           | **0.68 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.50 $\\pm$ 0.04 | 0.49 $\\pm$ 0.05             | 0.57 $\\pm$ 0.00         | 0.52 $\\pm$ 0.01          | 0.54 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.38 $\\pm$ 0.03 | 0.46 $\\pm$ 0.03             | 0.49 $\\pm$ 0.01         | 0.49 $\\pm$ 0.02          | 0.49 $\\pm$ 0.06                           | 0.43 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.48 $\\pm$ 0.03 | 0.55 $\\pm$ 0.01             | 0.57 $\\pm$ 0.01         | 0.57 $\\pm$ 0.05          | 0.60 $\\pm$ 0.04                           | 0.60 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.03 | 0.55 $\\pm$ 0.03             | 0.57 $\\pm$ 0.02         | 0.58 $\\pm$ 0.02          | 0.55 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.50 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | 0.62 $\\pm$ 0.03                           | 0.64 $\\pm$ 0.03     |\n| gpt2                               | 0.45 $\\pm$ 0.03 | 0.48 $\\pm$ 0.02             | 0.54 $\\pm$ 0.02         | 0.54 $\\pm$ 0.03          | 0.56 $\\pm$ 0.04                           | 0.57 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.52 $\\pm$ 0.03 | 0.57 $\\pm$ 0.05             | 0.61 $\\pm$ 0.03         | 0.63 $\\pm$ 0.03          | 0.64 $\\pm$ 0.04                           | **0.67 $\\pm$ 0.03** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.09 | 0.61 $\\pm$ 0.04             | 0.65 $\\pm$ 0.05         | 0.68 $\\pm$ 0.08          | 0.70 $\\pm$ 0.04                           | 0.67 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.51 $\\pm$ 0.04 | 0.53 $\\pm$ 0.05             | 0.60 $\\pm$ 0.05         | 0.60 $\\pm$ 0.05          | 0.60 $\\pm$ 0.03                           | 0.60 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.60 $\\pm$ 0.05 | 0.64 $\\pm$ 0.04             | 0.68 $\\pm$ 0.06         | 0.67 $\\pm$ 0.07          | 0.69 $\\pm$ 0.01                           | **0.73 $\\pm$ 0.03** |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.05 | 0.62 $\\pm$ 0.04             | 0.65 $\\pm$ 0.07         | 0.66 $\\pm$ 0.06          | 0.68 $\\pm$ 0.05                           | 0.67 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.61 $\\pm$ 0.08 | 0.65 $\\pm$ 0.06             | 0.68 $\\pm$ 0.03         | **0.73 $\\pm$ 0.02**      | 0.69 $\\pm$ 0.05                           | 0.72 $\\pm$ 0.03     |\n| gpt2                               | 0.50 $\\pm$ 0.05 | 0.59 $\\pm$ 0.03             | 0.59 $\\pm$ 0.07         | 0.61 $\\pm$ 0.05          | 0.61 $\\pm$ 0.05                           | 0.61 $\\pm$ 0.05     |\n| xlm-roberta-large                  | 0.60 $\\pm$ 0.06 | 0.63 $\\pm$ 0.02             | 0.66 $\\pm$ 0.05         | 0.68 $\\pm$ 0.07          | 0.69 $\\pm$ 0.05                           | 0.70 $\\pm$ 0.04     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.70 $\\pm$ 0.01         | 0.69 $\\pm$ 0.01          | 0.71 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.00     |\n| EleutherAI-gpt-neo-125M            | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.01             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.01          | 0.66 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.65 $\\pm$ 0.01 | 0.69 $\\pm$ 0.00             | 0.71 $\\pm$ 0.01         | 0.71 $\\pm$ 0.01          | 0.72 $\\pm$ 0.02                           | 0.74 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.70 $\\pm$ 0.02         | 0.71 $\\pm$ 0.02          | 0.70 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.01 | 0.69 $\\pm$ 0.02             | 0.71 $\\pm$ 0.00         | 0.73 $\\pm$ 0.01          | 0.73 $\\pm$ 0.00                           | 0.75 $\\pm$ 0.01     |\n| gpt2                               | 0.60 $\\pm$ 0.01 | 0.65 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.68 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.66 $\\pm$ 0.02 | 0.69 $\\pm$ 0.01             | 0.72 $\\pm$ 0.00         | 0.74 $\\pm$ 0.02          | 0.74 $\\pm$ 0.01                           | **0.76 $\\pm$ 0.01** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.04         | 0.04 $\\pm$ 0.02          | 0.06 $\\pm$ 0.01                           | 0.07 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.03 $\\pm$ 0.01             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.02          | 0.02 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.01 $\\pm$ 0.01 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.02         | 0.06 $\\pm$ 0.03          | 0.07 $\\pm$ 0.05                           | 0.08 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.01 $\\pm$ 0.01 | 0.04 $\\pm$ 0.03             | 0.05 $\\pm$ 0.04         | 0.04 $\\pm$ 0.01          | 0.03 $\\pm$ 0.00                           | 0.05 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.03 $\\pm$ 0.02 | 0.03 $\\pm$ 0.03             | 0.07 $\\pm$ 0.02         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.02                           | **0.11 $\\pm$ 0.01** |\n| gpt2                               | 0.01 $\\pm$ 0.02 | 0.02 $\\pm$ 0.01             | 0.03 $\\pm$ 0.02         | 0.05 $\\pm$ 0.00          | 0.04 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.02 $\\pm$ 0.02 | 0.07 $\\pm$ 0.04             | 0.07 $\\pm$ 0.03         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.02                           | 0.10 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Russian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.24 $\\pm$ 0.07 | 0.28 $\\pm$ 0.02             | 0.38 $\\pm$ 0.04         | 0.38 $\\pm$ 0.02          | 0.37 $\\pm$ 0.01                           | 0.30 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.18 $\\pm$ 0.01 | 0.21 $\\pm$ 0.03             | 0.20 $\\pm$ 0.03         | 0.25 $\\pm$ 0.01          | 0.21 $\\pm$ 0.02                           | 0.17 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.36 $\\pm$ 0.02 | 0.47 $\\pm$ 0.02             | 0.48 $\\pm$ 0.01         | 0.50 $\\pm$ 0.02          | 0.54 $\\pm$ 0.01                           | 0.55 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.34 $\\pm$ 0.02 | 0.39 $\\pm$ 0.00             | 0.46 $\\pm$ 0.06         | 0.50 $\\pm$ 0.03          | 0.48 $\\pm$ 0.01                           | 0.50 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.45 $\\pm$ 0.04 | 0.49 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.55 $\\pm$ 0.02          | 0.54 $\\pm$ 0.00                           | **0.60 $\\pm$ 0.01** |\n| gpt2                               | 0.16 $\\pm$ 0.02 | 0.13 $\\pm$ 0.05             | 0.11 $\\pm$ 0.06         | 0.13 $\\pm$ 0.05          | 0.19 $\\pm$ 0.08                           | 0.17 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.46 $\\pm$ 0.02 | 0.49 $\\pm$ 0.04             | 0.53 $\\pm$ 0.03         | 0.56 $\\pm$ 0.04          | 0.55 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.18 $\\pm$ 0.06 | 0.19 $\\pm$ 0.02             | 0.29 $\\pm$ 0.04         | 0.29 $\\pm$ 0.02          | 0.28 $\\pm$ 0.03                           | 0.21 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.13 $\\pm$ 0.00 | 0.15 $\\pm$ 0.03             | 0.13 $\\pm$ 0.03         | 0.18 $\\pm$ 0.02          | 0.16 $\\pm$ 0.01                           | 0.13 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.28 $\\pm$ 0.02 | 0.40 $\\pm$ 0.02             | 0.41 $\\pm$ 0.03         | 0.44 $\\pm$ 0.03          | 0.47 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.07     |\n| distilbert-base-multilingual-cased | 0.26 $\\pm$ 0.02 | 0.31 $\\pm$ 0.03             | 0.40 $\\pm$ 0.07         | 0.43 $\\pm$ 0.06          | 0.41 $\\pm$ 0.04                           | 0.44 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.39 $\\pm$ 0.03 | 0.43 $\\pm$ 0.01             | 0.45 $\\pm$ 0.02         | 0.49 $\\pm$ 0.05          | 0.47 $\\pm$ 0.02                           | **0.54 $\\pm$ 0.02** |\n| gpt2                               | 0.10 $\\pm$ 0.01 | 0.08 $\\pm$ 0.03             | 0.07 $\\pm$ 0.04         | 0.08 $\\pm$ 0.04          | 0.13 $\\pm$ 0.07                           | 0.12 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.40 $\\pm$ 0.02 | 0.43 $\\pm$ 0.06             | 0.50 $\\pm$ 0.04         | 0.53 $\\pm$ 0.04          | 0.51 $\\pm$ 0.06                           | 0.49 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.37 $\\pm$ 0.06 | 0.55 $\\pm$ 0.15             | 0.54 $\\pm$ 0.06         | 0.55 $\\pm$ 0.00          | 0.55 $\\pm$ 0.07                           | 0.51 $\\pm$ 0.09     |\n| EleutherAI-gpt-neo-125M            | 0.32 $\\pm$ 0.05 | 0.38 $\\pm$ 0.06             | 0.42 $\\pm$ 0.09         | 0.42 $\\pm$ 0.06          | 0.33 $\\pm$ 0.04                           | 0.29 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.49 $\\pm$ 0.02 | 0.56 $\\pm$ 0.01             | 0.58 $\\pm$ 0.02         | 0.60 $\\pm$ 0.00          | 0.62 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.06     |\n| distilbert-base-multilingual-cased | 0.50 $\\pm$ 0.07 | 0.55 $\\pm$ 0.07             | 0.56 $\\pm$ 0.05         | 0.60 $\\pm$ 0.04          | 0.59 $\\pm$ 0.05                           | 0.60 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.53 $\\pm$ 0.07 | 0.59 $\\pm$ 0.02             | 0.60 $\\pm$ 0.06         | 0.63 $\\pm$ 0.06          | 0.64 $\\pm$ 0.03                           | **0.68 $\\pm$ 0.05** |\n| gpt2                               | 0.32 $\\pm$ 0.06 | 0.43 $\\pm$ 0.13             | 0.38 $\\pm$ 0.11         | 0.32 $\\pm$ 0.02          | 0.36 $\\pm$ 0.05                           | 0.32 $\\pm$ 0.06     |\n| xlm-roberta-large                  | 0.54 $\\pm$ 0.02 | 0.57 $\\pm$ 0.07             | 0.57 $\\pm$ 0.05         | 0.59 $\\pm$ 0.06          | 0.60 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.09     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.03 | 0.58 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.01          | 0.62 $\\pm$ 0.01                           | 0.58 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.53 $\\pm$ 0.01 | 0.55 $\\pm$ 0.01             | 0.55 $\\pm$ 0.01         | 0.56 $\\pm$ 0.00          | 0.54 $\\pm$ 0.01                           | 0.53 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.61 $\\pm$ 0.01 | 0.67 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.69 $\\pm$ 0.01          | 0.71 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.60 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.66 $\\pm$ 0.04         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.71 $\\pm$ 0.02          | 0.71 $\\pm$ 0.00                           | **0.74 $\\pm$ 0.01** |\n| gpt2                               | 0.53 $\\pm$ 0.01 | 0.53 $\\pm$ 0.02             | 0.52 $\\pm$ 0.01         | 0.52 $\\pm$ 0.01          | 0.54 $\\pm$ 0.02                           | 0.53 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.67 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.71 $\\pm$ 0.02         | 0.73 $\\pm$ 0.02          | 0.72 $\\pm$ 0.03                           | 0.71 $\\pm$ 0.00     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.05 $\\pm$ 0.03 | 0.08 $\\pm$ 0.02             | 0.12 $\\pm$ 0.02         | 0.09 $\\pm$ 0.03          | 0.09 $\\pm$ 0.07                           | 0.08 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.04 $\\pm$ 0.02 | 0.03 $\\pm$ 0.02             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.01          | 0.02 $\\pm$ 0.02                           | 0.04 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.06 $\\pm$ 0.02 | 0.09 $\\pm$ 0.02             | 0.13 $\\pm$ 0.06         | 0.14 $\\pm$ 0.03          | 0.18 $\\pm$ 0.04                           | 0.15 $\\pm$ 0.06     |\n| distilbert-base-multilingual-cased | 0.06 $\\pm$ 0.03 | 0.11 $\\pm$ 0.04             | 0.11 $\\pm$ 0.05         | 0.11 $\\pm$ 0.03          | 0.13 $\\pm$ 0.08                           | 0.15 $\\pm$ 0.06     |\n| facebook-mbart-large-50            | 0.13 $\\pm$ 0.04 | 0.12 $\\pm$ 0.05             | 0.13 $\\pm$ 0.08         | 0.18 $\\pm$ 0.06          | 0.16 $\\pm$ 0.00                           | **0.19 $\\pm$ 0.02** |\n| gpt2                               | 0.03 $\\pm$ 0.01 | 0.03 $\\pm$ 0.03             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.13 $\\pm$ 0.04 | 0.13 $\\pm$ 0.06             | 0.14 $\\pm$ 0.04         | 0.15 $\\pm$ 0.06          | 0.16 $\\pm$ 0.01                           | 0.13 $\\pm$ 0.08     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# All 6 Languages"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.03 | 0.67 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| en         | EleutherAI-gpt-neo-125M            | 0.52 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.00         | 0.62 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.02     |\n| en         | bert-base-multilingual-cased       | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.60 $\\pm$ 0.05 | 0.65 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.01          | 0.68 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.02     |\n| en         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | **0.71 $\\pm$ 0.01**     | **0.71 $\\pm$ 0.01**      | **0.71 $\\pm$ 0.01**                       | **0.71 $\\pm$ 0.00** |\n| en         | gpt2                               | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.66 $\\pm$ 0.00          | 0.66 $\\pm$ 0.02                           | 0.66 $\\pm$ 0.02     |\n| en         | xlm-roberta-large                  | 0.67 $\\pm$ 0.00 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.00         | **0.71 $\\pm$ 0.01**      | **0.71 $\\pm$ 0.01**                       | 0.70 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.02 | 0.48 $\\pm$ 0.05             | 0.50 $\\pm$ 0.02         | 0.48 $\\pm$ 0.04          | 0.50 $\\pm$ 0.04                           | 0.49 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.29 $\\pm$ 0.03 | 0.37 $\\pm$ 0.01             | 0.40 $\\pm$ 0.03         | 0.39 $\\pm$ 0.04          | 0.42 $\\pm$ 0.03                           | 0.39 $\\pm$ 0.02     |\n| fr         | bert-base-multilingual-cased       | 0.47 $\\pm$ 0.03 | 0.51 $\\pm$ 0.03             | 0.51 $\\pm$ 0.02         | 0.57 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.01     |\n| fr         | distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.01 | 0.51 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.53 $\\pm$ 0.01          | 0.54 $\\pm$ 0.03                           | 0.56 $\\pm$ 0.03     |\n| fr         | facebook-mbart-large-50            | 0.52 $\\pm$ 0.03 | 0.55 $\\pm$ 0.02             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | **0.60 $\\pm$ 0.02** |\n| fr         | gpt2                               | 0.38 $\\pm$ 0.03 | 0.42 $\\pm$ 0.02             | 0.42 $\\pm$ 0.03         | 0.46 $\\pm$ 0.02          | 0.49 $\\pm$ 0.04                           | 0.51 $\\pm$ 0.02     |\n| fr         | xlm-roberta-large                  | 0.53 $\\pm$ 0.02 | 0.56 $\\pm$ 0.03             | 0.57 $\\pm$ 0.03         | 0.58 $\\pm$ 0.02          | 0.55 $\\pm$ 0.04                           | 0.59 $\\pm$ 0.01     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.47 $\\pm$ 0.01 | 0.55 $\\pm$ 0.01             | 0.57 $\\pm$ 0.03         | 0.57 $\\pm$ 0.03          | 0.56 $\\pm$ 0.00                           | 0.56 $\\pm$ 0.02     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.41 $\\pm$ 0.04 | 0.48 $\\pm$ 0.01             | 0.52 $\\pm$ 0.02         | 0.50 $\\pm$ 0.02          | 0.50 $\\pm$ 0.04                           | 0.48 $\\pm$ 0.02     |\n| ge         | bert-base-multilingual-cased       | 0.51 $\\pm$ 0.00 | 0.58 $\\pm$ 0.00             | 0.60 $\\pm$ 0.00         | 0.62 $\\pm$ 0.01          | 0.62 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.02     |\n| ge         | distilbert-base-multilingual-cased | 0.51 $\\pm$ 0.02 | 0.56 $\\pm$ 0.01             | 0.58 $\\pm$ 0.02         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.02     |\n| ge         | facebook-mbart-large-50            | 0.53 $\\pm$ 0.04 | 0.60 $\\pm$ 0.02             | 0.61 $\\pm$ 0.01         | 0.65 $\\pm$ 0.01          | 0.64 $\\pm$ 0.03                           | **0.68 $\\pm$ 0.02** |\n| ge         | gpt2                               | 0.50 $\\pm$ 0.06 | 0.52 $\\pm$ 0.02             | 0.54 $\\pm$ 0.05         | 0.55 $\\pm$ 0.03          | 0.55 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.04     |\n| ge         | xlm-roberta-large                  | 0.57 $\\pm$ 0.01 | 0.60 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.64 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.03 | 0.48 $\\pm$ 0.04             | 0.51 $\\pm$ 0.05         | 0.52 $\\pm$ 0.03          | 0.51 $\\pm$ 0.04                           | 0.52 $\\pm$ 0.02     |\n| it         | EleutherAI-gpt-neo-125M            | 0.32 $\\pm$ 0.02 | 0.38 $\\pm$ 0.03             | 0.42 $\\pm$ 0.03         | 0.45 $\\pm$ 0.03          | 0.43 $\\pm$ 0.02                           | 0.42 $\\pm$ 0.03     |\n| it         | bert-base-multilingual-cased       | 0.47 $\\pm$ 0.03 | 0.52 $\\pm$ 0.03             | 0.53 $\\pm$ 0.02         | 0.55 $\\pm$ 0.03          | 0.57 $\\pm$ 0.04                           | 0.58 $\\pm$ 0.03     |\n| it         | distilbert-base-multilingual-cased | 0.46 $\\pm$ 0.02 | 0.50 $\\pm$ 0.07             | 0.53 $\\pm$ 0.02         | 0.53 $\\pm$ 0.03          | 0.52 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.06     |\n| it         | facebook-mbart-large-50            | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.54 $\\pm$ 0.02         | 0.56 $\\pm$ 0.04          | 0.58 $\\pm$ 0.03                           | **0.63 $\\pm$ 0.03** |\n| it         | gpt2                               | 0.40 $\\pm$ 0.03 | 0.46 $\\pm$ 0.03             | 0.48 $\\pm$ 0.01         | 0.51 $\\pm$ 0.02          | 0.50 $\\pm$ 0.02                           | 0.53 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.52 $\\pm$ 0.03 | 0.55 $\\pm$ 0.03             | 0.57 $\\pm$ 0.01         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.52 $\\pm$ 0.01 | 0.54 $\\pm$ 0.04             | 0.61 $\\pm$ 0.03         | 0.59 $\\pm$ 0.02          | 0.61 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.01     |\n| po         | EleutherAI-gpt-neo-125M            | 0.44 $\\pm$ 0.03 | 0.49 $\\pm$ 0.03             | 0.54 $\\pm$ 0.02         | 0.54 $\\pm$ 0.02          | 0.54 $\\pm$ 0.03                           | 0.50 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.53 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.61 $\\pm$ 0.02          | 0.64 $\\pm$ 0.02                           | 0.65 $\\pm$ 0.02     |\n| po         | distilbert-base-multilingual-cased | 0.51 $\\pm$ 0.04 | 0.58 $\\pm$ 0.03             | 0.61 $\\pm$ 0.03         | 0.62 $\\pm$ 0.04          | 0.61 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.02 | 0.59 $\\pm$ 0.04             | 0.63 $\\pm$ 0.01         | 0.64 $\\pm$ 0.01          | 0.65 $\\pm$ 0.01                           | **0.68 $\\pm$ 0.03** |\n| po         | gpt2                               | 0.47 $\\pm$ 0.01 | 0.53 $\\pm$ 0.01             | 0.56 $\\pm$ 0.03         | 0.57 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.02     |\n| po         | xlm-roberta-large                  | 0.56 $\\pm$ 0.04 | 0.60 $\\pm$ 0.03             | 0.64 $\\pm$ 0.02         | 0.65 $\\pm$ 0.03          | 0.66 $\\pm$ 0.00                           | **0.68 $\\pm$ 0.02** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.24 $\\pm$ 0.07 | 0.28 $\\pm$ 0.02             | 0.38 $\\pm$ 0.04         | 0.38 $\\pm$ 0.02          | 0.37 $\\pm$ 0.01                           | 0.30 $\\pm$ 0.03     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.18 $\\pm$ 0.01 | 0.21 $\\pm$ 0.03             | 0.20 $\\pm$ 0.03         | 0.25 $\\pm$ 0.01          | 0.21 $\\pm$ 0.02                           | 0.17 $\\pm$ 0.04     |\n| ru         | bert-base-multilingual-cased       | 0.36 $\\pm$ 0.02 | 0.47 $\\pm$ 0.02             | 0.48 $\\pm$ 0.01         | 0.50 $\\pm$ 0.02          | 0.54 $\\pm$ 0.01                           | 0.55 $\\pm$ 0.02     |\n| ru         | distilbert-base-multilingual-cased | 0.34 $\\pm$ 0.02 | 0.39 $\\pm$ 0.00             | 0.46 $\\pm$ 0.06         | 0.50 $\\pm$ 0.03          | 0.48 $\\pm$ 0.01                           | 0.50 $\\pm$ 0.02     |\n| ru         | facebook-mbart-large-50            | 0.45 $\\pm$ 0.04 | 0.49 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.55 $\\pm$ 0.02          | 0.54 $\\pm$ 0.00                           | **0.60 $\\pm$ 0.01** |\n| ru         | gpt2                               | 0.16 $\\pm$ 0.02 | 0.13 $\\pm$ 0.05             | 0.11 $\\pm$ 0.06         | 0.13 $\\pm$ 0.05          | 0.19 $\\pm$ 0.08                           | 0.17 $\\pm$ 0.04     |\n| ru         | xlm-roberta-large                  | 0.46 $\\pm$ 0.02 | 0.49 $\\pm$ 0.04             | 0.53 $\\pm$ 0.03         | 0.56 $\\pm$ 0.04          | 0.55 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.03 | 0.60 $\\pm$ 0.01             | 0.59 $\\pm$ 0.01         | 0.61 $\\pm$ 0.02          | 0.60 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.02     |\n| en         | EleutherAI-gpt-neo-125M            | 0.46 $\\pm$ 0.01 | 0.52 $\\pm$ 0.01             | 0.54 $\\pm$ 0.02         | 0.56 $\\pm$ 0.01          | 0.56 $\\pm$ 0.01                           | 0.54 $\\pm$ 0.02     |\n| en         | bert-base-multilingual-cased       | 0.59 $\\pm$ 0.02 | 0.62 $\\pm$ 0.02             | 0.64 $\\pm$ 0.01         | 0.65 $\\pm$ 0.01          | 0.64 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.02     |\n| en         | distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.04 | 0.58 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.64 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.01     |\n| en         | facebook-mbart-large-50            | 0.61 $\\pm$ 0.01 | 0.65 $\\pm$ 0.02             | 0.66 $\\pm$ 0.01         | 0.66 $\\pm$ 0.02          | 0.66 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| en         | gpt2                               | 0.61 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.64 $\\pm$ 0.01         | 0.65 $\\pm$ 0.04          | 0.67 $\\pm$ 0.03                           | **0.70 $\\pm$ 0.00** |\n| en         | xlm-roberta-large                  | 0.63 $\\pm$ 0.02 | 0.65 $\\pm$ 0.03             | 0.66 $\\pm$ 0.00         | 0.67 $\\pm$ 0.01          | 0.68 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.37 $\\pm$ 0.03 | 0.39 $\\pm$ 0.06             | 0.43 $\\pm$ 0.03         | 0.39 $\\pm$ 0.05          | 0.42 $\\pm$ 0.06                           | 0.40 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.22 $\\pm$ 0.06 | 0.29 $\\pm$ 0.03             | 0.33 $\\pm$ 0.06         | 0.30 $\\pm$ 0.04          | 0.35 $\\pm$ 0.03                           | 0.31 $\\pm$ 0.02     |\n| fr         | bert-base-multilingual-cased       | 0.40 $\\pm$ 0.02 | 0.44 $\\pm$ 0.04             | 0.47 $\\pm$ 0.05         | 0.53 $\\pm$ 0.02          | 0.51 $\\pm$ 0.04                           | 0.54 $\\pm$ 0.01     |\n| fr         | distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.02 | 0.45 $\\pm$ 0.02             | 0.46 $\\pm$ 0.03         | 0.48 $\\pm$ 0.02          | 0.48 $\\pm$ 0.05                           | 0.51 $\\pm$ 0.04     |\n| fr         | facebook-mbart-large-50            | 0.47 $\\pm$ 0.03 | 0.48 $\\pm$ 0.02             | 0.52 $\\pm$ 0.06         | 0.53 $\\pm$ 0.03          | 0.52 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.01     |\n| fr         | gpt2                               | 0.31 $\\pm$ 0.02 | 0.36 $\\pm$ 0.02             | 0.36 $\\pm$ 0.03         | 0.40 $\\pm$ 0.02          | 0.44 $\\pm$ 0.04                           | 0.46 $\\pm$ 0.03     |\n| fr         | xlm-roberta-large                  | 0.49 $\\pm$ 0.03 | 0.51 $\\pm$ 0.06             | 0.55 $\\pm$ 0.04         | **0.56 $\\pm$ 0.02**      | 0.53 $\\pm$ 0.07                           | **0.56 $\\pm$ 0.04** |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.42 $\\pm$ 0.03 | 0.47 $\\pm$ 0.01             | 0.50 $\\pm$ 0.05         | 0.48 $\\pm$ 0.04          | 0.48 $\\pm$ 0.01                           | 0.47 $\\pm$ 0.02     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.34 $\\pm$ 0.05 | 0.41 $\\pm$ 0.03             | 0.46 $\\pm$ 0.01         | 0.43 $\\pm$ 0.02          | 0.44 $\\pm$ 0.04                           | 0.39 $\\pm$ 0.03     |\n| ge         | bert-base-multilingual-cased       | 0.45 $\\pm$ 0.01 | 0.54 $\\pm$ 0.02             | 0.54 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | 0.57 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.03     |\n| ge         | distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.05 | 0.51 $\\pm$ 0.02             | 0.54 $\\pm$ 0.01         | 0.54 $\\pm$ 0.01          | 0.52 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.04     |\n| ge         | facebook-mbart-large-50            | 0.47 $\\pm$ 0.06 | 0.54 $\\pm$ 0.01             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.01          | 0.58 $\\pm$ 0.02                           | **0.62 $\\pm$ 0.03** |\n| ge         | gpt2                               | 0.46 $\\pm$ 0.06 | 0.49 $\\pm$ 0.03             | 0.49 $\\pm$ 0.06         | 0.50 $\\pm$ 0.02          | 0.51 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.04     |\n| ge         | xlm-roberta-large                  | 0.52 $\\pm$ 0.00 | 0.55 $\\pm$ 0.03             | 0.60 $\\pm$ 0.01         | 0.61 $\\pm$ 0.02          | 0.58 $\\pm$ 0.00                           | **0.62 $\\pm$ 0.01** |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.38 $\\pm$ 0.05 | 0.40 $\\pm$ 0.04             | 0.45 $\\pm$ 0.04         | 0.42 $\\pm$ 0.03          | 0.42 $\\pm$ 0.03                           | 0.43 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-125M            | 0.26 $\\pm$ 0.03 | 0.31 $\\pm$ 0.04             | 0.35 $\\pm$ 0.02         | 0.37 $\\pm$ 0.01          | 0.35 $\\pm$ 0.01                           | 0.34 $\\pm$ 0.02     |\n| it         | bert-base-multilingual-cased       | 0.39 $\\pm$ 0.03 | 0.47 $\\pm$ 0.04             | 0.47 $\\pm$ 0.03         | 0.49 $\\pm$ 0.01          | 0.51 $\\pm$ 0.05                           | 0.53 $\\pm$ 0.03     |\n| it         | distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.02 | 0.46 $\\pm$ 0.08             | 0.47 $\\pm$ 0.02         | 0.48 $\\pm$ 0.04          | 0.46 $\\pm$ 0.02                           | 0.52 $\\pm$ 0.05     |\n| it         | facebook-mbart-large-50            | 0.43 $\\pm$ 0.02 | 0.49 $\\pm$ 0.06             | 0.49 $\\pm$ 0.04         | 0.50 $\\pm$ 0.04          | 0.53 $\\pm$ 0.01                           | **0.59 $\\pm$ 0.01** |\n| it         | gpt2                               | 0.34 $\\pm$ 0.03 | 0.42 $\\pm$ 0.02             | 0.43 $\\pm$ 0.01         | 0.47 $\\pm$ 0.03          | 0.45 $\\pm$ 0.03                           | 0.50 $\\pm$ 0.02     |\n| it         | xlm-roberta-large                  | 0.47 $\\pm$ 0.04 | 0.51 $\\pm$ 0.05             | 0.56 $\\pm$ 0.02         | 0.57 $\\pm$ 0.04          | 0.54 $\\pm$ 0.04                           | 0.58 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.50 $\\pm$ 0.04 | 0.49 $\\pm$ 0.05             | 0.57 $\\pm$ 0.00         | 0.52 $\\pm$ 0.01          | 0.54 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.01     |\n| po         | EleutherAI-gpt-neo-125M            | 0.38 $\\pm$ 0.03 | 0.46 $\\pm$ 0.03             | 0.49 $\\pm$ 0.01         | 0.49 $\\pm$ 0.02          | 0.49 $\\pm$ 0.06                           | 0.43 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.48 $\\pm$ 0.03 | 0.55 $\\pm$ 0.01             | 0.57 $\\pm$ 0.01         | 0.57 $\\pm$ 0.05          | 0.60 $\\pm$ 0.04                           | 0.60 $\\pm$ 0.01     |\n| po         | distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.03 | 0.55 $\\pm$ 0.03             | 0.57 $\\pm$ 0.02         | 0.58 $\\pm$ 0.02          | 0.55 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.50 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | 0.62 $\\pm$ 0.03                           | 0.64 $\\pm$ 0.03     |\n| po         | gpt2                               | 0.45 $\\pm$ 0.03 | 0.48 $\\pm$ 0.02             | 0.54 $\\pm$ 0.02         | 0.54 $\\pm$ 0.03          | 0.56 $\\pm$ 0.04                           | 0.57 $\\pm$ 0.01     |\n| po         | xlm-roberta-large                  | 0.52 $\\pm$ 0.03 | 0.57 $\\pm$ 0.05             | 0.61 $\\pm$ 0.03         | 0.63 $\\pm$ 0.03          | 0.64 $\\pm$ 0.04                           | **0.67 $\\pm$ 0.03** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.18 $\\pm$ 0.06 | 0.19 $\\pm$ 0.02             | 0.29 $\\pm$ 0.04         | 0.29 $\\pm$ 0.02          | 0.28 $\\pm$ 0.03                           | 0.21 $\\pm$ 0.03     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.13 $\\pm$ 0.00 | 0.15 $\\pm$ 0.03             | 0.13 $\\pm$ 0.03         | 0.18 $\\pm$ 0.02          | 0.16 $\\pm$ 0.01                           | 0.13 $\\pm$ 0.04     |\n| ru         | bert-base-multilingual-cased       | 0.28 $\\pm$ 0.02 | 0.40 $\\pm$ 0.02             | 0.41 $\\pm$ 0.03         | 0.44 $\\pm$ 0.03          | 0.47 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.07     |\n| ru         | distilbert-base-multilingual-cased | 0.26 $\\pm$ 0.02 | 0.31 $\\pm$ 0.03             | 0.40 $\\pm$ 0.07         | 0.43 $\\pm$ 0.06          | 0.41 $\\pm$ 0.04                           | 0.44 $\\pm$ 0.05     |\n| ru         | facebook-mbart-large-50            | 0.39 $\\pm$ 0.03 | 0.43 $\\pm$ 0.01             | 0.45 $\\pm$ 0.02         | 0.49 $\\pm$ 0.05          | 0.47 $\\pm$ 0.02                           | **0.54 $\\pm$ 0.02** |\n| ru         | gpt2                               | 0.10 $\\pm$ 0.01 | 0.08 $\\pm$ 0.03             | 0.07 $\\pm$ 0.04         | 0.08 $\\pm$ 0.04          | 0.13 $\\pm$ 0.07                           | 0.12 $\\pm$ 0.04     |\n| ru         | xlm-roberta-large                  | 0.40 $\\pm$ 0.02 | 0.43 $\\pm$ 0.06             | 0.50 $\\pm$ 0.04         | 0.53 $\\pm$ 0.04          | 0.51 $\\pm$ 0.06                           | 0.49 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.70 $\\pm$ 0.04 | 0.76 $\\pm$ 0.04             | 0.74 $\\pm$ 0.04         | **0.77 $\\pm$ 0.02**      | **0.77 $\\pm$ 0.03**                       | 0.76 $\\pm$ 0.04     |\n| en         | EleutherAI-gpt-neo-125M            | 0.62 $\\pm$ 0.07 | 0.67 $\\pm$ 0.05             | 0.71 $\\pm$ 0.04         | 0.70 $\\pm$ 0.04          | 0.70 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.04     |\n| en         | bert-base-multilingual-cased       | 0.70 $\\pm$ 0.04 | 0.75 $\\pm$ 0.02             | 0.75 $\\pm$ 0.01         | 0.75 $\\pm$ 0.02          | 0.75 $\\pm$ 0.02                           | 0.76 $\\pm$ 0.03     |\n| en         | distilbert-base-multilingual-cased | 0.68 $\\pm$ 0.07 | 0.73 $\\pm$ 0.02             | 0.73 $\\pm$ 0.02         | 0.74 $\\pm$ 0.03          | 0.74 $\\pm$ 0.05                           | 0.75 $\\pm$ 0.03     |\n| en         | facebook-mbart-large-50            | 0.71 $\\pm$ 0.03 | 0.76 $\\pm$ 0.02             | 0.76 $\\pm$ 0.02         | 0.76 $\\pm$ 0.02          | **0.77 $\\pm$ 0.02**                       | 0.76 $\\pm$ 0.01     |\n| en         | gpt2                               | 0.68 $\\pm$ 0.04 | 0.71 $\\pm$ 0.05             | 0.69 $\\pm$ 0.04         | 0.68 $\\pm$ 0.04          | 0.66 $\\pm$ 0.04                           | 0.63 $\\pm$ 0.04     |\n| en         | xlm-roberta-large                  | 0.72 $\\pm$ 0.02 | 0.76 $\\pm$ 0.02             | 0.75 $\\pm$ 0.00         | 0.76 $\\pm$ 0.02          | 0.74 $\\pm$ 0.02                           | 0.75 $\\pm$ 0.03     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.52 $\\pm$ 0.02 | **0.65 $\\pm$ 0.04**         | 0.58 $\\pm$ 0.02         | 0.64 $\\pm$ 0.01          | 0.62 $\\pm$ 0.03                           | **0.65 $\\pm$ 0.01** |\n| fr         | EleutherAI-gpt-neo-125M            | 0.46 $\\pm$ 0.11 | 0.50 $\\pm$ 0.06             | 0.53 $\\pm$ 0.04         | 0.54 $\\pm$ 0.01          | 0.53 $\\pm$ 0.04                           | 0.54 $\\pm$ 0.02     |\n| fr         | bert-base-multilingual-cased       | 0.57 $\\pm$ 0.04 | 0.60 $\\pm$ 0.03             | 0.58 $\\pm$ 0.02         | 0.62 $\\pm$ 0.03          | 0.63 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.04     |\n| fr         | distilbert-base-multilingual-cased | 0.56 $\\pm$ 0.04 | 0.58 $\\pm$ 0.03             | 0.56 $\\pm$ 0.03         | 0.61 $\\pm$ 0.02          | 0.62 $\\pm$ 0.06                           | 0.62 $\\pm$ 0.03     |\n| fr         | facebook-mbart-large-50            | 0.59 $\\pm$ 0.05 | 0.63 $\\pm$ 0.02             | 0.62 $\\pm$ 0.05         | **0.65 $\\pm$ 0.04**      | **0.65 $\\pm$ 0.02**                       | **0.65 $\\pm$ 0.03** |\n| fr         | gpt2                               | 0.49 $\\pm$ 0.06 | 0.51 $\\pm$ 0.01             | 0.50 $\\pm$ 0.03         | 0.55 $\\pm$ 0.02          | 0.56 $\\pm$ 0.04                           | 0.56 $\\pm$ 0.02     |\n| fr         | xlm-roberta-large                  | 0.58 $\\pm$ 0.01 | 0.62 $\\pm$ 0.03             | 0.60 $\\pm$ 0.04         | 0.59 $\\pm$ 0.01          | 0.58 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.04     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.53 $\\pm$ 0.02 | 0.66 $\\pm$ 0.05             | 0.65 $\\pm$ 0.03         | 0.70 $\\pm$ 0.02          | 0.68 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.04     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.53 $\\pm$ 0.05 | 0.57 $\\pm$ 0.07             | 0.61 $\\pm$ 0.03         | 0.62 $\\pm$ 0.03          | 0.59 $\\pm$ 0.03                           | 0.61 $\\pm$ 0.03     |\n| ge         | bert-base-multilingual-cased       | 0.60 $\\pm$ 0.01 | 0.64 $\\pm$ 0.02             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.00     |\n| ge         | distilbert-base-multilingual-cased | 0.58 $\\pm$ 0.02 | 0.62 $\\pm$ 0.01             | 0.62 $\\pm$ 0.07         | 0.66 $\\pm$ 0.05          | 0.66 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| ge         | facebook-mbart-large-50            | 0.62 $\\pm$ 0.04 | 0.66 $\\pm$ 0.04             | 0.66 $\\pm$ 0.02         | 0.73 $\\pm$ 0.01          | 0.71 $\\pm$ 0.04                           | **0.74 $\\pm$ 0.01** |\n| ge         | gpt2                               | 0.55 $\\pm$ 0.06 | 0.55 $\\pm$ 0.02             | 0.60 $\\pm$ 0.03         | 0.61 $\\pm$ 0.06          | 0.59 $\\pm$ 0.04                           | 0.60 $\\pm$ 0.04     |\n| ge         | xlm-roberta-large                  | 0.62 $\\pm$ 0.02 | 0.67 $\\pm$ 0.04             | 0.66 $\\pm$ 0.02         | 0.66 $\\pm$ 0.02          | 0.67 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.00     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.50 $\\pm$ 0.04 | 0.63 $\\pm$ 0.05             | 0.59 $\\pm$ 0.05         | 0.66 $\\pm$ 0.01          | 0.65 $\\pm$ 0.05                           | 0.66 $\\pm$ 0.02     |\n| it         | EleutherAI-gpt-neo-125M            | 0.43 $\\pm$ 0.01 | 0.49 $\\pm$ 0.04             | 0.54 $\\pm$ 0.04         | 0.58 $\\pm$ 0.06          | 0.55 $\\pm$ 0.06                           | 0.56 $\\pm$ 0.05     |\n| it         | bert-base-multilingual-cased       | 0.58 $\\pm$ 0.03 | 0.59 $\\pm$ 0.03             | 0.61 $\\pm$ 0.03         | 0.62 $\\pm$ 0.06          | 0.63 $\\pm$ 0.05                           | 0.63 $\\pm$ 0.02     |\n| it         | distilbert-base-multilingual-cased | 0.55 $\\pm$ 0.03 | 0.55 $\\pm$ 0.04             | 0.60 $\\pm$ 0.05         | 0.61 $\\pm$ 0.03          | 0.60 $\\pm$ 0.04                           | 0.62 $\\pm$ 0.07     |\n| it         | facebook-mbart-large-50            | 0.58 $\\pm$ 0.03 | 0.60 $\\pm$ 0.03             | 0.61 $\\pm$ 0.04         | 0.64 $\\pm$ 0.06          | 0.65 $\\pm$ 0.07                           | **0.69 $\\pm$ 0.06** |\n| it         | gpt2                               | 0.47 $\\pm$ 0.02 | 0.52 $\\pm$ 0.05             | 0.56 $\\pm$ 0.03         | 0.56 $\\pm$ 0.03          | 0.56 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.60 $\\pm$ 0.05             | 0.59 $\\pm$ 0.02         | 0.62 $\\pm$ 0.03          | 0.62 $\\pm$ 0.05                           | 0.66 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.09 | 0.61 $\\pm$ 0.04             | 0.65 $\\pm$ 0.05         | 0.68 $\\pm$ 0.08          | 0.70 $\\pm$ 0.04                           | 0.67 $\\pm$ 0.01     |\n| po         | EleutherAI-gpt-neo-125M            | 0.51 $\\pm$ 0.04 | 0.53 $\\pm$ 0.05             | 0.60 $\\pm$ 0.05         | 0.60 $\\pm$ 0.05          | 0.60 $\\pm$ 0.03                           | 0.60 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.60 $\\pm$ 0.05 | 0.64 $\\pm$ 0.04             | 0.68 $\\pm$ 0.06         | 0.67 $\\pm$ 0.07          | 0.69 $\\pm$ 0.01                           | **0.73 $\\pm$ 0.03** |\n| po         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.05 | 0.62 $\\pm$ 0.04             | 0.65 $\\pm$ 0.07         | 0.66 $\\pm$ 0.06          | 0.68 $\\pm$ 0.05                           | 0.67 $\\pm$ 0.03     |\n| po         | facebook-mbart-large-50            | 0.61 $\\pm$ 0.08 | 0.65 $\\pm$ 0.06             | 0.68 $\\pm$ 0.03         | **0.73 $\\pm$ 0.02**      | 0.69 $\\pm$ 0.05                           | 0.72 $\\pm$ 0.03     |\n| po         | gpt2                               | 0.50 $\\pm$ 0.05 | 0.59 $\\pm$ 0.03             | 0.59 $\\pm$ 0.07         | 0.61 $\\pm$ 0.05          | 0.61 $\\pm$ 0.05                           | 0.61 $\\pm$ 0.05     |\n| po         | xlm-roberta-large                  | 0.60 $\\pm$ 0.06 | 0.63 $\\pm$ 0.02             | 0.66 $\\pm$ 0.05         | 0.68 $\\pm$ 0.07          | 0.69 $\\pm$ 0.05                           | 0.70 $\\pm$ 0.04     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.37 $\\pm$ 0.06 | 0.55 $\\pm$ 0.15             | 0.54 $\\pm$ 0.06         | 0.55 $\\pm$ 0.00          | 0.55 $\\pm$ 0.07                           | 0.51 $\\pm$ 0.09     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.32 $\\pm$ 0.05 | 0.38 $\\pm$ 0.06             | 0.42 $\\pm$ 0.09         | 0.42 $\\pm$ 0.06          | 0.33 $\\pm$ 0.04                           | 0.29 $\\pm$ 0.03     |\n| ru         | bert-base-multilingual-cased       | 0.49 $\\pm$ 0.02 | 0.56 $\\pm$ 0.01             | 0.58 $\\pm$ 0.02         | 0.60 $\\pm$ 0.00          | 0.62 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.06     |\n| ru         | distilbert-base-multilingual-cased | 0.50 $\\pm$ 0.07 | 0.55 $\\pm$ 0.07             | 0.56 $\\pm$ 0.05         | 0.60 $\\pm$ 0.04          | 0.59 $\\pm$ 0.05                           | 0.60 $\\pm$ 0.05     |\n| ru         | facebook-mbart-large-50            | 0.53 $\\pm$ 0.07 | 0.59 $\\pm$ 0.02             | 0.60 $\\pm$ 0.06         | 0.63 $\\pm$ 0.06          | 0.64 $\\pm$ 0.03                           | **0.68 $\\pm$ 0.05** |\n| ru         | gpt2                               | 0.32 $\\pm$ 0.06 | 0.43 $\\pm$ 0.13             | 0.38 $\\pm$ 0.11         | 0.32 $\\pm$ 0.02          | 0.36 $\\pm$ 0.05                           | 0.32 $\\pm$ 0.06     |\n| ru         | xlm-roberta-large                  | 0.54 $\\pm$ 0.02 | 0.57 $\\pm$ 0.07             | 0.57 $\\pm$ 0.05         | 0.59 $\\pm$ 0.06          | 0.60 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.09     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.73 $\\pm$ 0.02 | 0.76 $\\pm$ 0.01             | 0.76 $\\pm$ 0.01         | 0.77 $\\pm$ 0.01          | 0.76 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| en         | EleutherAI-gpt-neo-125M            | 0.67 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.73 $\\pm$ 0.00         | 0.73 $\\pm$ 0.01          | 0.73 $\\pm$ 0.00                           | 0.72 $\\pm$ 0.01     |\n| en         | bert-base-multilingual-cased       | 0.74 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.78 $\\pm$ 0.01         | 0.78 $\\pm$ 0.01          | 0.78 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.72 $\\pm$ 0.03 | 0.75 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.77 $\\pm$ 0.00          | 0.77 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| en         | facebook-mbart-large-50            | 0.76 $\\pm$ 0.01 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.00**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.00**                       | **0.79 $\\pm$ 0.00** |\n| en         | gpt2                               | 0.75 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.76 $\\pm$ 0.01         | 0.77 $\\pm$ 0.01          | 0.77 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| en         | xlm-roberta-large                  | 0.77 $\\pm$ 0.00 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.00**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.00**                       | **0.79 $\\pm$ 0.00** |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.64 $\\pm$ 0.01 | 0.67 $\\pm$ 0.03             | 0.67 $\\pm$ 0.01         | 0.66 $\\pm$ 0.02          | 0.67 $\\pm$ 0.03                           | 0.67 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.01 | 0.61 $\\pm$ 0.00             | 0.63 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.63 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.01     |\n| fr         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.72 $\\pm$ 0.02          | 0.71 $\\pm$ 0.02                           | 0.72 $\\pm$ 0.00     |\n| fr         | distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.68 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.02     |\n| fr         | facebook-mbart-large-50            | 0.69 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.72 $\\pm$ 0.02         | 0.72 $\\pm$ 0.01          | 0.72 $\\pm$ 0.01                           | **0.73 $\\pm$ 0.01** |\n| fr         | gpt2                               | 0.61 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.63 $\\pm$ 0.02         | 0.66 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| fr         | xlm-roberta-large                  | 0.70 $\\pm$ 0.02 | 0.71 $\\pm$ 0.02             | **0.73 $\\pm$ 0.02**     | **0.73 $\\pm$ 0.01**      | 0.71 $\\pm$ 0.03                           | **0.73 $\\pm$ 0.01** |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.02          | 0.69 $\\pm$ 0.00                           | 0.69 $\\pm$ 0.01     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.60 $\\pm$ 0.03 | 0.63 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.65 $\\pm$ 0.01          | 0.65 $\\pm$ 0.02                           | 0.64 $\\pm$ 0.01     |\n| ge         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.00             | 0.71 $\\pm$ 0.01         | 0.72 $\\pm$ 0.01          | 0.72 $\\pm$ 0.01                           | 0.73 $\\pm$ 0.01     |\n| ge         | distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.01 | 0.68 $\\pm$ 0.00             | 0.69 $\\pm$ 0.02         | 0.71 $\\pm$ 0.01          | 0.70 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| ge         | facebook-mbart-large-50            | 0.67 $\\pm$ 0.03 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.01         | 0.74 $\\pm$ 0.01          | 0.74 $\\pm$ 0.02                           | **0.76 $\\pm$ 0.01** |\n| ge         | gpt2                               | 0.64 $\\pm$ 0.04 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.04         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.01                           | 0.67 $\\pm$ 0.03     |\n| ge         | xlm-roberta-large                  | 0.69 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.73 $\\pm$ 0.00         | 0.74 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | 0.75 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.02 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.03         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.67 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.01 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.64 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.02     |\n| it         | bert-base-multilingual-cased       | 0.64 $\\pm$ 0.02 | 0.67 $\\pm$ 0.02             | 0.68 $\\pm$ 0.01         | 0.69 $\\pm$ 0.02          | 0.70 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.02     |\n| it         | distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.01 | 0.66 $\\pm$ 0.04             | 0.68 $\\pm$ 0.01         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.04     |\n| it         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.03             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.03          | 0.71 $\\pm$ 0.02                           | **0.74 $\\pm$ 0.02** |\n| it         | gpt2                               | 0.60 $\\pm$ 0.02 | 0.64 $\\pm$ 0.02             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.01          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.67 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.71 $\\pm$ 0.01         | 0.72 $\\pm$ 0.02          | 0.71 $\\pm$ 0.01                           | 0.73 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.70 $\\pm$ 0.01         | 0.69 $\\pm$ 0.01          | 0.71 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.00     |\n| po         | EleutherAI-gpt-neo-125M            | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.01             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.01          | 0.66 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| po         | bert-base-multilingual-cased       | 0.65 $\\pm$ 0.01 | 0.69 $\\pm$ 0.00             | 0.71 $\\pm$ 0.01         | 0.71 $\\pm$ 0.01          | 0.72 $\\pm$ 0.02                           | 0.74 $\\pm$ 0.01     |\n| po         | distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.70 $\\pm$ 0.02         | 0.71 $\\pm$ 0.02          | 0.70 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.01 | 0.69 $\\pm$ 0.02             | 0.71 $\\pm$ 0.00         | 0.73 $\\pm$ 0.01          | 0.73 $\\pm$ 0.00                           | 0.75 $\\pm$ 0.01     |\n| po         | gpt2                               | 0.60 $\\pm$ 0.01 | 0.65 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.68 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.01     |\n| po         | xlm-roberta-large                  | 0.66 $\\pm$ 0.02 | 0.69 $\\pm$ 0.01             | 0.72 $\\pm$ 0.00         | 0.74 $\\pm$ 0.02          | 0.74 $\\pm$ 0.01                           | **0.76 $\\pm$ 0.01** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.03 | 0.58 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.01          | 0.62 $\\pm$ 0.01                           | 0.58 $\\pm$ 0.01     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.53 $\\pm$ 0.01 | 0.55 $\\pm$ 0.01             | 0.55 $\\pm$ 0.01         | 0.56 $\\pm$ 0.00          | 0.54 $\\pm$ 0.01                           | 0.53 $\\pm$ 0.01     |\n| ru         | bert-base-multilingual-cased       | 0.61 $\\pm$ 0.01 | 0.67 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.69 $\\pm$ 0.01          | 0.71 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.03     |\n| ru         | distilbert-base-multilingual-cased | 0.60 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.66 $\\pm$ 0.04         | 0.68 $\\pm$ 0.02          | 0.67 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| ru         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.71 $\\pm$ 0.02          | 0.71 $\\pm$ 0.00                           | **0.74 $\\pm$ 0.01** |\n| ru         | gpt2                               | 0.53 $\\pm$ 0.01 | 0.53 $\\pm$ 0.02             | 0.52 $\\pm$ 0.01         | 0.52 $\\pm$ 0.01          | 0.54 $\\pm$ 0.02                           | 0.53 $\\pm$ 0.01     |\n| ru         | xlm-roberta-large                  | 0.67 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.71 $\\pm$ 0.02         | 0.73 $\\pm$ 0.02          | 0.72 $\\pm$ 0.03                           | 0.71 $\\pm$ 0.00     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.06 $\\pm$ 0.01 | 0.10 $\\pm$ 0.02             | 0.10 $\\pm$ 0.03         | 0.11 $\\pm$ 0.03          | 0.10 $\\pm$ 0.03                           | **0.12 $\\pm$ 0.03** |\n| en         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.02 | 0.05 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.00          | 0.07 $\\pm$ 0.01                           | 0.06 $\\pm$ 0.02     |\n| en         | bert-base-multilingual-cased       | 0.07 $\\pm$ 0.03 | 0.09 $\\pm$ 0.02             | 0.11 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.02     |\n| en         | distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.02 | 0.08 $\\pm$ 0.02             | 0.10 $\\pm$ 0.02         | 0.11 $\\pm$ 0.01          | 0.11 $\\pm$ 0.03                           | 0.11 $\\pm$ 0.02     |\n| en         | facebook-mbart-large-50            | 0.09 $\\pm$ 0.02 | 0.10 $\\pm$ 0.01             | **0.12 $\\pm$ 0.02**     | 0.09 $\\pm$ 0.04          | **0.12 $\\pm$ 0.02**                       | 0.11 $\\pm$ 0.01     |\n| en         | gpt2                               | 0.08 $\\pm$ 0.03 | 0.09 $\\pm$ 0.01             | 0.08 $\\pm$ 0.02         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.01                           | 0.07 $\\pm$ 0.01     |\n| en         | xlm-roberta-large                  | 0.09 $\\pm$ 0.01 | 0.10 $\\pm$ 0.02             | 0.11 $\\pm$ 0.02         | 0.11 $\\pm$ 0.02          | **0.12 $\\pm$ 0.02**                       | 0.10 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.02 | 0.07 $\\pm$ 0.01             | 0.08 $\\pm$ 0.04         | 0.08 $\\pm$ 0.02          | 0.09 $\\pm$ 0.01                           | 0.08 $\\pm$ 0.03     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.02 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.02 $\\pm$ 0.02         | 0.04 $\\pm$ 0.01          | 0.05 $\\pm$ 0.02                           | 0.05 $\\pm$ 0.04     |\n| fr         | bert-base-multilingual-cased       | 0.05 $\\pm$ 0.03 | 0.07 $\\pm$ 0.01             | 0.06 $\\pm$ 0.00         | 0.09 $\\pm$ 0.04          | 0.09 $\\pm$ 0.02                           | 0.09 $\\pm$ 0.05     |\n| fr         | distilbert-base-multilingual-cased | 0.07 $\\pm$ 0.02 | 0.07 $\\pm$ 0.02             | 0.08 $\\pm$ 0.02         | 0.07 $\\pm$ 0.02          | 0.09 $\\pm$ 0.02                           | 0.09 $\\pm$ 0.03     |\n| fr         | facebook-mbart-large-50            | 0.08 $\\pm$ 0.02 | 0.08 $\\pm$ 0.04             | 0.07 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | **0.11 $\\pm$ 0.02**                       | 0.10 $\\pm$ 0.01     |\n| fr         | gpt2                               | 0.05 $\\pm$ 0.01 | 0.05 $\\pm$ 0.04             | 0.05 $\\pm$ 0.03         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.03     |\n| fr         | xlm-roberta-large                  | 0.09 $\\pm$ 0.00 | 0.08 $\\pm$ 0.04             | 0.08 $\\pm$ 0.04         | 0.09 $\\pm$ 0.04          | 0.09 $\\pm$ 0.02                           | 0.10 $\\pm$ 0.01     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.02 | 0.04 $\\pm$ 0.05             | 0.04 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | 0.05 $\\pm$ 0.04                           | 0.06 $\\pm$ 0.03     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.00 $\\pm$ 0.00 | 0.03 $\\pm$ 0.03             | 0.02 $\\pm$ 0.03         | 0.03 $\\pm$ 0.02          | 0.02 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.03     |\n| ge         | bert-base-multilingual-cased       | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.08 $\\pm$ 0.04         | 0.09 $\\pm$ 0.01          | 0.06 $\\pm$ 0.02                           | 0.09 $\\pm$ 0.02     |\n| ge         | distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.00 | 0.03 $\\pm$ 0.03             | 0.05 $\\pm$ 0.04         | 0.06 $\\pm$ 0.04          | 0.05 $\\pm$ 0.02                           | 0.05 $\\pm$ 0.03     |\n| ge         | facebook-mbart-large-50            | 0.05 $\\pm$ 0.01 | 0.08 $\\pm$ 0.04             | 0.07 $\\pm$ 0.04         | 0.09 $\\pm$ 0.01          | 0.07 $\\pm$ 0.06                           | **0.11 $\\pm$ 0.03** |\n| ge         | gpt2                               | 0.03 $\\pm$ 0.03 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.01         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.03                           | 0.02 $\\pm$ 0.02     |\n| ge         | xlm-roberta-large                  | 0.06 $\\pm$ 0.01 | 0.08 $\\pm$ 0.01             | 0.06 $\\pm$ 0.04         | 0.09 $\\pm$ 0.01          | 0.09 $\\pm$ 0.05                           | **0.11 $\\pm$ 0.02** |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.02 | 0.07 $\\pm$ 0.02             | 0.06 $\\pm$ 0.01         | 0.07 $\\pm$ 0.03          | 0.06 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-125M            | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.04 $\\pm$ 0.02         | 0.04 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.02     |\n| it         | bert-base-multilingual-cased       | 0.05 $\\pm$ 0.02 | 0.06 $\\pm$ 0.02             | 0.06 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.06 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.01     |\n| it         | distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.02 | 0.03 $\\pm$ 0.00             | 0.07 $\\pm$ 0.02         | 0.06 $\\pm$ 0.02          | 0.06 $\\pm$ 0.02                           | 0.08 $\\pm$ 0.03     |\n| it         | facebook-mbart-large-50            | 0.05 $\\pm$ 0.02 | 0.08 $\\pm$ 0.03             | 0.06 $\\pm$ 0.03         | 0.08 $\\pm$ 0.03          | 0.07 $\\pm$ 0.05                           | 0.10 $\\pm$ 0.05     |\n| it         | gpt2                               | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.02     |\n| it         | xlm-roberta-large                  | 0.07 $\\pm$ 0.04 | 0.07 $\\pm$ 0.03             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.02                           | **0.11 $\\pm$ 0.02** |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.04         | 0.04 $\\pm$ 0.02          | 0.06 $\\pm$ 0.01                           | 0.07 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.03 $\\pm$ 0.01             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.02          | 0.02 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.01 $\\pm$ 0.01 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.02         | 0.06 $\\pm$ 0.03          | 0.07 $\\pm$ 0.05                           | 0.08 $\\pm$ 0.02     |\n| po         | distilbert-base-multilingual-cased | 0.01 $\\pm$ 0.01 | 0.04 $\\pm$ 0.03             | 0.05 $\\pm$ 0.04         | 0.04 $\\pm$ 0.01          | 0.03 $\\pm$ 0.00                           | 0.05 $\\pm$ 0.02     |\n| po         | facebook-mbart-large-50            | 0.03 $\\pm$ 0.02 | 0.03 $\\pm$ 0.03             | 0.07 $\\pm$ 0.02         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.02                           | **0.11 $\\pm$ 0.01** |\n| po         | gpt2                               | 0.01 $\\pm$ 0.02 | 0.02 $\\pm$ 0.01             | 0.03 $\\pm$ 0.02         | 0.05 $\\pm$ 0.00          | 0.04 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.01     |\n| po         | xlm-roberta-large                  | 0.02 $\\pm$ 0.02 | 0.07 $\\pm$ 0.04             | 0.07 $\\pm$ 0.03         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.02                           | 0.10 $\\pm$ 0.01     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.05 $\\pm$ 0.03 | 0.08 $\\pm$ 0.02             | 0.12 $\\pm$ 0.02         | 0.09 $\\pm$ 0.03          | 0.09 $\\pm$ 0.07                           | 0.08 $\\pm$ 0.04     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.04 $\\pm$ 0.02 | 0.03 $\\pm$ 0.02             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.01          | 0.02 $\\pm$ 0.02                           | 0.04 $\\pm$ 0.03     |\n| ru         | bert-base-multilingual-cased       | 0.06 $\\pm$ 0.02 | 0.09 $\\pm$ 0.02             | 0.13 $\\pm$ 0.06         | 0.14 $\\pm$ 0.03          | 0.18 $\\pm$ 0.04                           | 0.15 $\\pm$ 0.06     |\n| ru         | distilbert-base-multilingual-cased | 0.06 $\\pm$ 0.03 | 0.11 $\\pm$ 0.04             | 0.11 $\\pm$ 0.05         | 0.11 $\\pm$ 0.03          | 0.13 $\\pm$ 0.08                           | 0.15 $\\pm$ 0.06     |\n| ru         | facebook-mbart-large-50            | 0.13 $\\pm$ 0.04 | 0.12 $\\pm$ 0.05             | 0.13 $\\pm$ 0.08         | 0.18 $\\pm$ 0.06          | 0.16 $\\pm$ 0.00                           | **0.19 $\\pm$ 0.02** |\n| ru         | gpt2                               | 0.03 $\\pm$ 0.01 | 0.03 $\\pm$ 0.03             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.01     |\n| ru         | xlm-roberta-large                  | 0.13 $\\pm$ 0.04 | 0.13 $\\pm$ 0.06             | 0.14 $\\pm$ 0.04         | 0.15 $\\pm$ 0.06          | 0.16 $\\pm$ 0.01                           | 0.13 $\\pm$ 0.08     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64637/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    }
   ],
   "source": [
    "display_metrics_and_write_to_file(df=results_df, grouping_criterion=['model_name'], output_dir='per_model_name_tables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
