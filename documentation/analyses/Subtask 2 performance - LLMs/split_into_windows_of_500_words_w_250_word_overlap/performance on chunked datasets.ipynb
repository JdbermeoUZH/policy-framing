{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac7fd1c8-16ff-43f2-a049-a27d37ea2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3de210-17ae-42af-a361-fff5a0f66735",
   "metadata": {},
   "source": [
    "# Group types of models (experiment type and model type) and pick best performing in terms of f1-score per unit of analysis and report them in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd704f0c-6f9a-47d2-b05b-150230e363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filepaths_mean_pred = glob.glob('./logged_performance_per_model/*/*agg_mean*.csv')\n",
    "results_filepaths_majority_pred = glob.glob('./logged_performance_per_model/*/*agg_majority*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def conantenate_results(filepath_list):\n",
    "    dfs_list = []\n",
    "    for results_filepath in filepath_list:\n",
    "        model_name = results_filepath.split('/')[-2]\n",
    "        results_df_i = pd.read_csv(results_filepath)\n",
    "        results_df_i['model_name'] = model_name\n",
    "        dfs_list.append(results_df_i)\n",
    "\n",
    "    results_df_ = pd.concat(dfs_list).set_index(['language', 'model_name', 'unit_of_analysis']).sort_index()\n",
    "    results_df_.rename(columns={'f1-mico_mean': 'f1-micro_mean', 'f1-mico_std': 'f1-micro_std'}, inplace=True)\n",
    "\n",
    "    return results_df_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "results_mean_pred_df = conantenate_results(results_filepaths_mean_pred)\n",
    "results_majority_vote_pred_df = conantenate_results(results_filepaths_majority_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6a452c68-3d2e-43a1-b820-a7688df2cd33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the tables to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c97b164-c359-40ea-b974-a0dc65c1db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_table(df, metric, index_cols=['model_name'], display_=True):\n",
    "    report_table = df.reset_index().copy()\n",
    "    report_table['result'] = report_table[f'{metric}_mean'].map(lambda x: f'{x:.2f}') + \\\n",
    "    ' $\\pm$ ' + report_table[f'{metric}_std'].map(lambda x: f'{x:.2f}')\n",
    "    report_table['col_title'] = report_table.unit_of_analysis.str.split('_').str.join(' ') \n",
    "    report_table['col_title'] = pd.Categorical(\n",
    "        report_table.col_title,\n",
    "        categories=['title', 'title and first paragraph', 'title and 5 sentences', 'title and 10 sentences',\n",
    "                    'title and first sentence each paragraph', 'raw text'],\n",
    "        ordered=True)\n",
    "    report_table = report_table[index_cols + ['col_title', 'result']]\\\n",
    "        .pivot_table(index=index_cols, columns=['col_title'], values=['result'], aggfunc='first', fill_value=0)\\\n",
    "        .droplevel(0, axis=1)\n",
    "\n",
    "    report_table.columns.names = [None]\n",
    "\n",
    "    # Highlight best scoring models according to their average\n",
    "    mean_perf_arr = report_table.applymap(lambda x: float(str(x).split(' ')[0])).to_numpy()\n",
    "    highlight_mask = mean_perf_arr == mean_perf_arr.max()\n",
    "    report_table_arr = report_table.to_numpy()  # Note it passes the array by reference\n",
    "    report_table_arr[highlight_mask] = '**' + report_table_arr[highlight_mask] + '**'\n",
    "\n",
    "    if display_:\n",
    "        display(Markdown(report_table.to_markdown()))\n",
    "    \n",
    "    return report_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14e8da-1acf-4114-9cfd-eeaa97261f68",
   "metadata": {},
   "source": [
    "### Generate tables for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bea51df3-87ed-4ace-ad56-25d235b5cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_report = ['f1-micro', 'recall-micro', 'precision-micro', 'roc-auc', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1650fa17-5ccc-439a-9818-d61bf369ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'en': 'English', 'it': 'Italian', 'fr': 'French', 'po': 'Polish', 'ru': 'Russian', 'ge': 'German'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143ecba4-ffb9-45e9-869c-87d16caea017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_metrics_and_write_to_file(df, grouping_criterion, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    report_tables_dfs_dict = {metric: [] for metric in metrics_to_report}\n",
    "\n",
    "    for language, results_df in df.groupby(level=0):\n",
    "        display(Markdown(f'# {language_dict[language]}'))\n",
    "        \n",
    "        for metric in metrics_to_report:\n",
    "            os.makedirs(os.path.join(output_dir, metric), exist_ok=True)\n",
    "\n",
    "            output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "            output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "            output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "            os.makedirs(output_dir_markdown, exist_ok=True)\n",
    "            os.makedirs(output_dir_latex, exist_ok=True)\n",
    "            os.makedirs(output_dir_csv, exist_ok=True)\n",
    "\n",
    "            display(Markdown(f'## {metric}'))\n",
    "\n",
    "            report_table = display_performance_table(df=results_df, index_cols=grouping_criterion, metric=metric, display_=True)\n",
    "\n",
    "            # Export as markdown\n",
    "            markdown_file = open(os.path.join(output_dir_markdown, f\"{language_dict[language]}_{metric}.md\"), \"w\")\n",
    "            report_table.reset_index().to_markdown(markdown_file, index=False)\n",
    "            markdown_file.close()\n",
    "\n",
    "            # Export as latex table\n",
    "            latex_file = open(os.path.join(output_dir_latex, f\"{language_dict[language]}_{metric}.tex\"), \"w\")\n",
    "            report_table.reset_index().to_latex(latex_file, index=False)\n",
    "            latex_file.close()\n",
    "\n",
    "            # Export as csv\n",
    "            report_table.to_csv(os.path.join(output_dir_csv, f\"{language_dict[language]}_{metric}.csv\"))\n",
    "\n",
    "            # Stack all languages into single table\n",
    "            report_table['language'] = language\n",
    "            report_table = report_table.reset_index().set_index(['language'] + grouping_criterion)\n",
    "\n",
    "            report_tables_dfs_dict[metric].append(report_table)\n",
    "\n",
    "    # Report or store unified table\n",
    "    display(Markdown(f'# All 6 Languages'))\n",
    "    for metric in metrics_to_report:\n",
    "        display(Markdown(f'## {metric}'))\n",
    "        multi_language_report_table_metric = pd.concat(report_tables_dfs_dict[metric])\n",
    "        display(Markdown(multi_language_report_table_metric.reset_index().to_markdown(index=False)))\n",
    "\n",
    "        output_dir_markdown = os.path.join(output_dir, metric, 'markdown')\n",
    "        output_dir_latex = os.path.join(output_dir, metric, 'latex')\n",
    "        output_dir_csv = os.path.join(output_dir, metric, 'csv')\n",
    "\n",
    "        # Export as markdown\n",
    "        markdown_file = open(os.path.join(output_dir_markdown, f\"all_6_languages_{metric}.md\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_markdown(markdown_file, index=False)\n",
    "        markdown_file.close()\n",
    "\n",
    "        # Export as latex table\n",
    "        latex_file = open(os.path.join(output_dir_latex, f\"all_6_languages_{metric}.tex\"), \"w\")\n",
    "        multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n",
    "        latex_file.close()\n",
    "\n",
    "        # Export as csv\n",
    "        multi_language_report_table_metric.to_csv(os.path.join(output_dir_csv, f\"all_6_languages_{metric}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d754-49ba-4c74-8220-2e144f624044",
   "metadata": {},
   "source": [
    "# Per model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5abb824-f93a-44fd-8203-a0b84224b0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# English"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.01 | 0.67 $\\pm$ 0.01             | 0.68 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.68 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.00     |\n| EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.03 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.01          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.69 $\\pm$ 0.02          | 0.69 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.61 $\\pm$ 0.03 | 0.66 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.03          | 0.67 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | **0.71 $\\pm$ 0.02**                       | 0.69 $\\pm$ 0.02     |\n| gpt2                               | 0.63 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.02          | 0.68 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | **0.71 $\\pm$ 0.02**      | **0.71 $\\pm$ 0.01**                       | **0.71 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.02          | 0.60 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.01             | 0.55 $\\pm$ 0.03         | 0.55 $\\pm$ 0.00          | 0.59 $\\pm$ 0.03                           | 0.62 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.58 $\\pm$ 0.01 | 0.63 $\\pm$ 0.03             | 0.64 $\\pm$ 0.02         | 0.65 $\\pm$ 0.01          | 0.64 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.56 $\\pm$ 0.03 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.63 $\\pm$ 0.03          | 0.60 $\\pm$ 0.01                           | 0.64 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.60 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.66 $\\pm$ 0.01          | 0.66 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.02     |\n| gpt2                               | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.65 $\\pm$ 0.02         | 0.67 $\\pm$ 0.04          | 0.65 $\\pm$ 0.04                           | 0.66 $\\pm$ 0.03     |\n| xlm-roberta-large                  | 0.60 $\\pm$ 0.01 | 0.66 $\\pm$ 0.01             | 0.66 $\\pm$ 0.02         | 0.67 $\\pm$ 0.01          | **0.68 $\\pm$ 0.02**                       | **0.68 $\\pm$ 0.01** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.72 $\\pm$ 0.04 | 0.76 $\\pm$ 0.02             | 0.74 $\\pm$ 0.02         | 0.76 $\\pm$ 0.01          | **0.77 $\\pm$ 0.03**                       | **0.77 $\\pm$ 0.03** |\n| EleutherAI-gpt-neo-125M            | 0.63 $\\pm$ 0.04 | 0.67 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | 0.71 $\\pm$ 0.03          | 0.73 $\\pm$ 0.04                           | 0.75 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.70 $\\pm$ 0.03 | 0.74 $\\pm$ 0.03             | 0.76 $\\pm$ 0.02         | 0.74 $\\pm$ 0.04          | 0.75 $\\pm$ 0.03                           | 0.75 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.68 $\\pm$ 0.05 | 0.74 $\\pm$ 0.03             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.03          | 0.75 $\\pm$ 0.02                           | 0.75 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.73 $\\pm$ 0.01 | 0.76 $\\pm$ 0.01             | 0.74 $\\pm$ 0.03         | 0.76 $\\pm$ 0.01          | **0.77 $\\pm$ 0.02**                       | 0.74 $\\pm$ 0.02     |\n| gpt2                               | 0.67 $\\pm$ 0.06 | 0.72 $\\pm$ 0.02             | 0.70 $\\pm$ 0.03         | 0.67 $\\pm$ 0.03          | 0.71 $\\pm$ 0.05                           | 0.72 $\\pm$ 0.03     |\n| xlm-roberta-large                  | 0.73 $\\pm$ 0.03 | 0.75 $\\pm$ 0.03             | 0.75 $\\pm$ 0.01         | 0.76 $\\pm$ 0.03          | 0.74 $\\pm$ 0.01                           | 0.74 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.73 $\\pm$ 0.00 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.76 $\\pm$ 0.00          | 0.77 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.00     |\n| EleutherAI-gpt-neo-125M            | 0.69 $\\pm$ 0.01 | 0.72 $\\pm$ 0.01             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.00          | 0.75 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.74 $\\pm$ 0.01 | 0.77 $\\pm$ 0.01             | 0.78 $\\pm$ 0.01         | 0.78 $\\pm$ 0.01          | 0.78 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.73 $\\pm$ 0.02 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.77 $\\pm$ 0.02          | 0.76 $\\pm$ 0.00                           | 0.78 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.76 $\\pm$ 0.01 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | 0.78 $\\pm$ 0.01     |\n| gpt2                               | 0.74 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.77 $\\pm$ 0.00         | 0.77 $\\pm$ 0.02          | 0.77 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.76 $\\pm$ 0.00 | **0.79 $\\pm$ 0.00**         | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | **0.79 $\\pm$ 0.01** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.08 $\\pm$ 0.02 | 0.12 $\\pm$ 0.02             | 0.08 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.11 $\\pm$ 0.02                           | 0.12 $\\pm$ 0.02 |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.05 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.01          | 0.09 $\\pm$ 0.01                           | 0.09 $\\pm$ 0.01 |\n| bert-base-multilingual-cased       | 0.07 $\\pm$ 0.01 | 0.10 $\\pm$ 0.03             | **0.13 $\\pm$ 0.03**     | 0.09 $\\pm$ 0.00          | 0.10 $\\pm$ 0.03                           | 0.10 $\\pm$ 0.00 |\n| distilbert-base-multilingual-cased | 0.06 $\\pm$ 0.01 | 0.09 $\\pm$ 0.02             | 0.10 $\\pm$ 0.02         | 0.09 $\\pm$ 0.01          | 0.09 $\\pm$ 0.00                           | 0.11 $\\pm$ 0.02 |\n| facebook-mbart-large-50            | 0.07 $\\pm$ 0.04 | 0.10 $\\pm$ 0.03             | 0.11 $\\pm$ 0.03         | 0.12 $\\pm$ 0.03          | 0.12 $\\pm$ 0.02                           | 0.10 $\\pm$ 0.00 |\n| gpt2                               | 0.06 $\\pm$ 0.01 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.06 $\\pm$ 0.03          | 0.08 $\\pm$ 0.01                           | 0.09 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.10 $\\pm$ 0.01 | 0.12 $\\pm$ 0.01             | 0.10 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.11 $\\pm$ 0.02                           | 0.11 $\\pm$ 0.02 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# French"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.04 | 0.47 $\\pm$ 0.01             | 0.50 $\\pm$ 0.04         | 0.50 $\\pm$ 0.01          | 0.52 $\\pm$ 0.01                           | 0.56 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.31 $\\pm$ 0.01 | 0.38 $\\pm$ 0.03             | 0.39 $\\pm$ 0.03         | 0.39 $\\pm$ 0.01          | 0.44 $\\pm$ 0.03                           | 0.47 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.47 $\\pm$ 0.04 | 0.52 $\\pm$ 0.03             | 0.53 $\\pm$ 0.02         | 0.55 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.58 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.05 | 0.50 $\\pm$ 0.02             | 0.53 $\\pm$ 0.03         | 0.53 $\\pm$ 0.02          | 0.52 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.53 $\\pm$ 0.02             | 0.56 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | **0.60 $\\pm$ 0.01** |\n| gpt2                               | 0.40 $\\pm$ 0.07 | 0.43 $\\pm$ 0.02             | 0.47 $\\pm$ 0.02         | 0.49 $\\pm$ 0.02          | 0.49 $\\pm$ 0.03                           | 0.53 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.51 $\\pm$ 0.02 | 0.56 $\\pm$ 0.03             | 0.57 $\\pm$ 0.01         | 0.58 $\\pm$ 0.03          | 0.58 $\\pm$ 0.01                           | 0.57 $\\pm$ 0.04     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.32 $\\pm$ 0.05 | 0.37 $\\pm$ 0.01             | 0.45 $\\pm$ 0.04         | 0.42 $\\pm$ 0.03          | 0.44 $\\pm$ 0.04                           | 0.50 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.24 $\\pm$ 0.01 | 0.32 $\\pm$ 0.04             | 0.32 $\\pm$ 0.04         | 0.31 $\\pm$ 0.03          | 0.38 $\\pm$ 0.04                           | 0.38 $\\pm$ 0.07     |\n| bert-base-multilingual-cased       | 0.38 $\\pm$ 0.04 | 0.46 $\\pm$ 0.03             | 0.49 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.51 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.37 $\\pm$ 0.06 | 0.44 $\\pm$ 0.02             | 0.48 $\\pm$ 0.04         | 0.48 $\\pm$ 0.02          | 0.46 $\\pm$ 0.03                           | 0.49 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.44 $\\pm$ 0.03 | 0.47 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.52 $\\pm$ 0.03          | 0.53 $\\pm$ 0.03                           | **0.55 $\\pm$ 0.04** |\n| gpt2                               | 0.36 $\\pm$ 0.06 | 0.36 $\\pm$ 0.02             | 0.43 $\\pm$ 0.02         | 0.43 $\\pm$ 0.02          | 0.43 $\\pm$ 0.06                           | 0.48 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.44 $\\pm$ 0.02 | 0.51 $\\pm$ 0.02             | 0.52 $\\pm$ 0.04         | **0.55 $\\pm$ 0.04**      | **0.55 $\\pm$ 0.02**                       | 0.54 $\\pm$ 0.07     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.03 | 0.63 $\\pm$ 0.04             | 0.57 $\\pm$ 0.04         | 0.62 $\\pm$ 0.06          | 0.65 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.42 $\\pm$ 0.05 | 0.48 $\\pm$ 0.03             | 0.51 $\\pm$ 0.02         | 0.55 $\\pm$ 0.06          | 0.53 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.59 $\\pm$ 0.05 | 0.60 $\\pm$ 0.02             | 0.59 $\\pm$ 0.03         | 0.61 $\\pm$ 0.05          | 0.62 $\\pm$ 0.01                           | 0.64 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.53 $\\pm$ 0.05 | 0.59 $\\pm$ 0.05             | 0.59 $\\pm$ 0.03         | 0.59 $\\pm$ 0.03          | 0.62 $\\pm$ 0.01                           | 0.61 $\\pm$ 0.00     |\n| facebook-mbart-large-50            | 0.60 $\\pm$ 0.06 | 0.62 $\\pm$ 0.01             | 0.62 $\\pm$ 0.04         | 0.63 $\\pm$ 0.02          | 0.64 $\\pm$ 0.03                           | **0.67 $\\pm$ 0.05** |\n| gpt2                               | 0.45 $\\pm$ 0.10 | 0.51 $\\pm$ 0.04             | 0.53 $\\pm$ 0.02         | 0.57 $\\pm$ 0.04          | 0.59 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.62 $\\pm$ 0.01          | 0.61 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.68 $\\pm$ 0.00          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.01         | 0.62 $\\pm$ 0.01          | 0.64 $\\pm$ 0.02                           | 0.66 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.70 $\\pm$ 0.02         | 0.71 $\\pm$ 0.01          | 0.71 $\\pm$ 0.02                           | 0.73 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.00         | 0.72 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | **0.74 $\\pm$ 0.01** |\n| gpt2                               | 0.62 $\\pm$ 0.04 | 0.64 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.01         | 0.73 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.02 | 0.07 $\\pm$ 0.01             | 0.06 $\\pm$ 0.03         | 0.08 $\\pm$ 0.02          | 0.09 $\\pm$ 0.03                           | 0.11 $\\pm$ 0.04 |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.01 $\\pm$ 0.01             | 0.03 $\\pm$ 0.01         | 0.04 $\\pm$ 0.01          | 0.04 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.03 |\n| bert-base-multilingual-cased       | 0.07 $\\pm$ 0.02 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.09 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | 0.11 $\\pm$ 0.03 |\n| distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.06 $\\pm$ 0.04          | 0.09 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.02 |\n| facebook-mbart-large-50            | 0.08 $\\pm$ 0.04 | 0.11 $\\pm$ 0.02             | 0.09 $\\pm$ 0.01         | 0.10 $\\pm$ 0.03          | 0.11 $\\pm$ 0.03                           | 0.11 $\\pm$ 0.01 |\n| gpt2                               | 0.03 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.04         | 0.07 $\\pm$ 0.02          | 0.06 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.06 |\n| xlm-roberta-large                  | 0.07 $\\pm$ 0.03 | 0.07 $\\pm$ 0.04             | 0.10 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | **0.12 $\\pm$ 0.05**                       | 0.08 $\\pm$ 0.05 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# German"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.00 | 0.56 $\\pm$ 0.02             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.00          | 0.57 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.01 | 0.46 $\\pm$ 0.01             | 0.48 $\\pm$ 0.00         | 0.50 $\\pm$ 0.02          | 0.51 $\\pm$ 0.04                           | 0.56 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.52 $\\pm$ 0.03 | 0.58 $\\pm$ 0.02             | 0.59 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.59 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.04     |\n| distilbert-base-multilingual-cased | 0.50 $\\pm$ 0.02 | 0.56 $\\pm$ 0.00             | 0.55 $\\pm$ 0.03         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.61 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.02             | 0.61 $\\pm$ 0.02         | 0.63 $\\pm$ 0.00          | 0.64 $\\pm$ 0.02                           | **0.65 $\\pm$ 0.02** |\n| gpt2                               | 0.47 $\\pm$ 0.03 | 0.52 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.55 $\\pm$ 0.03          | 0.53 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.55 $\\pm$ 0.02 | 0.61 $\\pm$ 0.00             | 0.62 $\\pm$ 0.03         | 0.64 $\\pm$ 0.01          | **0.65 $\\pm$ 0.01**                       | **0.65 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.03 | 0.49 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.51 $\\pm$ 0.02                           | 0.59 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.33 $\\pm$ 0.03 | 0.41 $\\pm$ 0.02             | 0.41 $\\pm$ 0.01         | 0.43 $\\pm$ 0.02          | 0.45 $\\pm$ 0.03                           | 0.51 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.46 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.54 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.54 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.06     |\n| distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.01 | 0.51 $\\pm$ 0.02             | 0.49 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.52 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.55 $\\pm$ 0.02             | 0.56 $\\pm$ 0.03         | 0.59 $\\pm$ 0.01          | 0.59 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.05     |\n| gpt2                               | 0.46 $\\pm$ 0.05 | 0.49 $\\pm$ 0.05             | 0.50 $\\pm$ 0.05         | 0.52 $\\pm$ 0.05          | 0.52 $\\pm$ 0.04                           | 0.56 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.50 $\\pm$ 0.02 | 0.57 $\\pm$ 0.02             | 0.57 $\\pm$ 0.02         | 0.58 $\\pm$ 0.01          | 0.63 $\\pm$ 0.07                           | **0.66 $\\pm$ 0.04** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.57 $\\pm$ 0.05 | 0.65 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.70 $\\pm$ 0.06          | 0.66 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.02 |\n| EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.02 | 0.51 $\\pm$ 0.04             | 0.57 $\\pm$ 0.01         | 0.58 $\\pm$ 0.02          | 0.60 $\\pm$ 0.05                           | 0.62 $\\pm$ 0.01 |\n| bert-base-multilingual-cased       | 0.60 $\\pm$ 0.02 | 0.62 $\\pm$ 0.02             | 0.66 $\\pm$ 0.03         | 0.68 $\\pm$ 0.06          | 0.66 $\\pm$ 0.01                           | 0.65 $\\pm$ 0.03 |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.63 $\\pm$ 0.02             | 0.63 $\\pm$ 0.04         | 0.64 $\\pm$ 0.05          | 0.66 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.01 |\n| facebook-mbart-large-50            | 0.62 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.68 $\\pm$ 0.00         | 0.68 $\\pm$ 0.02          | 0.70 $\\pm$ 0.04                           | 0.70 $\\pm$ 0.02 |\n| gpt2                               | 0.49 $\\pm$ 0.01 | 0.56 $\\pm$ 0.03             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.00          | 0.55 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02 |\n| xlm-roberta-large                  | 0.62 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.04         | **0.71 $\\pm$ 0.01**      | 0.68 $\\pm$ 0.04                           | 0.64 $\\pm$ 0.06 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.64 $\\pm$ 0.00 | 0.69 $\\pm$ 0.02             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.00          | 0.69 $\\pm$ 0.01                           | 0.73 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.01             | 0.64 $\\pm$ 0.00         | 0.65 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.00     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.01         | 0.73 $\\pm$ 0.01          | 0.71 $\\pm$ 0.00                           | 0.73 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.69 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.00                           | 0.72 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.01          | **0.74 $\\pm$ 0.01**                       | **0.74 $\\pm$ 0.01** |\n| gpt2                               | 0.62 $\\pm$ 0.02 | 0.66 $\\pm$ 0.03             | 0.66 $\\pm$ 0.03         | 0.67 $\\pm$ 0.02          | 0.66 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.72 $\\pm$ 0.00             | 0.72 $\\pm$ 0.02         | **0.74 $\\pm$ 0.01**      | **0.74 $\\pm$ 0.01**                       | **0.74 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.05 $\\pm$ 0.03         | 0.05 $\\pm$ 0.03          | 0.03 $\\pm$ 0.03                           | 0.06 $\\pm$ 0.04 |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.03 $\\pm$ 0.02         | 0.02 $\\pm$ 0.01          | 0.04 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.03 |\n| bert-base-multilingual-cased       | 0.05 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.03         | 0.09 $\\pm$ 0.05          | 0.06 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.07 |\n| distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.02             | 0.05 $\\pm$ 0.04         | 0.05 $\\pm$ 0.03          | 0.05 $\\pm$ 0.04                           | 0.04 $\\pm$ 0.04 |\n| facebook-mbart-large-50            | 0.05 $\\pm$ 0.03 | 0.06 $\\pm$ 0.04             | 0.06 $\\pm$ 0.03         | 0.05 $\\pm$ 0.02          | **0.11 $\\pm$ 0.03**                       | 0.07 $\\pm$ 0.04 |\n| gpt2                               | 0.01 $\\pm$ 0.01 | 0.01 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.03          | 0.03 $\\pm$ 0.01                           | 0.02 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.03 $\\pm$ 0.02 | 0.07 $\\pm$ 0.03             | 0.09 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | 0.07 $\\pm$ 0.04                           | 0.05 $\\pm$ 0.03 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Italian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.42 $\\pm$ 0.02 | 0.46 $\\pm$ 0.05             | 0.50 $\\pm$ 0.03         | 0.52 $\\pm$ 0.04          | 0.52 $\\pm$ 0.06                           | 0.58 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.04 | 0.42 $\\pm$ 0.02             | 0.44 $\\pm$ 0.02         | 0.44 $\\pm$ 0.01          | 0.48 $\\pm$ 0.04                           | 0.51 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.47 $\\pm$ 0.01 | 0.53 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.56 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.59 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.01 | 0.50 $\\pm$ 0.05             | 0.52 $\\pm$ 0.03         | 0.56 $\\pm$ 0.05          | 0.52 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.04             | 0.55 $\\pm$ 0.02         | 0.57 $\\pm$ 0.03          | 0.57 $\\pm$ 0.04                           | **0.61 $\\pm$ 0.03** |\n| gpt2                               | 0.41 $\\pm$ 0.02 | 0.45 $\\pm$ 0.01             | 0.48 $\\pm$ 0.03         | 0.52 $\\pm$ 0.02          | 0.50 $\\pm$ 0.03                           | 0.56 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.50 $\\pm$ 0.04 | 0.54 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.59 $\\pm$ 0.03          | 0.57 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.35 $\\pm$ 0.04 | 0.38 $\\pm$ 0.06             | 0.44 $\\pm$ 0.02         | 0.45 $\\pm$ 0.07          | 0.45 $\\pm$ 0.09                           | 0.51 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.28 $\\pm$ 0.04 | 0.36 $\\pm$ 0.04             | 0.36 $\\pm$ 0.02         | 0.35 $\\pm$ 0.02          | 0.41 $\\pm$ 0.06                           | 0.42 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.40 $\\pm$ 0.00 | 0.48 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | 0.51 $\\pm$ 0.03          | 0.52 $\\pm$ 0.01                           | 0.54 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.01 | 0.44 $\\pm$ 0.04             | 0.46 $\\pm$ 0.03         | 0.50 $\\pm$ 0.06          | 0.46 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.00     |\n| facebook-mbart-large-50            | 0.42 $\\pm$ 0.01 | 0.47 $\\pm$ 0.06             | 0.49 $\\pm$ 0.02         | 0.51 $\\pm$ 0.03          | 0.51 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.04     |\n| gpt2                               | 0.37 $\\pm$ 0.03 | 0.40 $\\pm$ 0.01             | 0.45 $\\pm$ 0.03         | 0.49 $\\pm$ 0.03          | 0.47 $\\pm$ 0.00                           | 0.52 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.43 $\\pm$ 0.05 | 0.49 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | 0.54 $\\pm$ 0.02          | 0.54 $\\pm$ 0.02                           | **0.57 $\\pm$ 0.04** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.53 $\\pm$ 0.03 | 0.58 $\\pm$ 0.03             | 0.58 $\\pm$ 0.05         | 0.63 $\\pm$ 0.05          | 0.63 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.47 $\\pm$ 0.05 | 0.49 $\\pm$ 0.01             | 0.55 $\\pm$ 0.01         | 0.57 $\\pm$ 0.01          | 0.59 $\\pm$ 0.04                           | 0.64 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.57 $\\pm$ 0.02 | 0.59 $\\pm$ 0.04             | 0.61 $\\pm$ 0.04         | 0.62 $\\pm$ 0.05          | 0.62 $\\pm$ 0.07                           | 0.65 $\\pm$ 0.06     |\n| distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.02 | 0.57 $\\pm$ 0.05             | 0.58 $\\pm$ 0.03         | 0.62 $\\pm$ 0.03          | 0.60 $\\pm$ 0.04                           | 0.62 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.57 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.03         | 0.64 $\\pm$ 0.03          | 0.65 $\\pm$ 0.08                           | **0.68 $\\pm$ 0.05** |\n| gpt2                               | 0.47 $\\pm$ 0.01 | 0.52 $\\pm$ 0.02             | 0.52 $\\pm$ 0.03         | 0.55 $\\pm$ 0.01          | 0.55 $\\pm$ 0.06                           | 0.61 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.58 $\\pm$ 0.03 | 0.59 $\\pm$ 0.03             | 0.61 $\\pm$ 0.02         | 0.64 $\\pm$ 0.05          | 0.61 $\\pm$ 0.04                           | 0.61 $\\pm$ 0.05     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.61 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.66 $\\pm$ 0.02         | 0.67 $\\pm$ 0.02          | 0.68 $\\pm$ 0.03                           | 0.71 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.69 $\\pm$ 0.03          | 0.67 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.65 $\\pm$ 0.00 | 0.68 $\\pm$ 0.03             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.02          | 0.70 $\\pm$ 0.03                           | **0.73 $\\pm$ 0.02** |\n| gpt2                               | 0.61 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.65 $\\pm$ 0.02         | 0.67 $\\pm$ 0.01          | 0.66 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.01         | 0.71 $\\pm$ 0.02          | 0.70 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.04             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.03          | 0.06 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.04 $\\pm$ 0.01             | 0.04 $\\pm$ 0.04         | 0.01 $\\pm$ 0.01          | 0.05 $\\pm$ 0.01                           | 0.08 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.06 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | 0.09 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.04     |\n| distilbert-base-multilingual-cased | 0.04 $\\pm$ 0.02 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.02         | 0.06 $\\pm$ 0.04          | 0.08 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.04             | 0.05 $\\pm$ 0.02         | 0.09 $\\pm$ 0.02          | 0.08 $\\pm$ 0.03                           | **0.10 $\\pm$ 0.02** |\n| gpt2                               | 0.02 $\\pm$ 0.02 | 0.02 $\\pm$ 0.02             | 0.04 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.03 $\\pm$ 0.00                           | 0.04 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.06 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.04                           | 0.07 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Polish"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.60 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.63 $\\pm$ 0.03                           | 0.64 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.04 | 0.47 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.51 $\\pm$ 0.06          | 0.55 $\\pm$ 0.03                           | 0.59 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.55 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.65 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.03 | 0.58 $\\pm$ 0.04             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.01          | 0.61 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.04 | 0.60 $\\pm$ 0.01             | 0.63 $\\pm$ 0.03         | 0.66 $\\pm$ 0.02          | 0.65 $\\pm$ 0.02                           | **0.68 $\\pm$ 0.02** |\n| gpt2                               | 0.49 $\\pm$ 0.01 | 0.55 $\\pm$ 0.05             | 0.57 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.59 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.59 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.65 $\\pm$ 0.03          | 0.66 $\\pm$ 0.02                           | 0.67 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.03 | 0.49 $\\pm$ 0.02             | 0.58 $\\pm$ 0.02         | 0.53 $\\pm$ 0.04          | 0.58 $\\pm$ 0.05                           | 0.62 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.34 $\\pm$ 0.05 | 0.42 $\\pm$ 0.04             | 0.50 $\\pm$ 0.02         | 0.44 $\\pm$ 0.06          | 0.52 $\\pm$ 0.01                           | 0.56 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.50 $\\pm$ 0.04 | 0.56 $\\pm$ 0.04             | 0.56 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.63 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.05         | 0.55 $\\pm$ 0.03          | 0.56 $\\pm$ 0.05                           | 0.62 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.59 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.61 $\\pm$ 0.03                           | 0.64 $\\pm$ 0.05     |\n| gpt2                               | 0.48 $\\pm$ 0.03 | 0.52 $\\pm$ 0.01             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.03          | 0.59 $\\pm$ 0.06                           | 0.63 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.54 $\\pm$ 0.02 | 0.56 $\\pm$ 0.05             | 0.60 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.64 $\\pm$ 0.03                           | **0.73 $\\pm$ 0.05** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.10 | 0.63 $\\pm$ 0.05             | 0.63 $\\pm$ 0.06         | 0.64 $\\pm$ 0.06          | 0.69 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.03             | 0.60 $\\pm$ 0.06         | 0.61 $\\pm$ 0.07          | 0.59 $\\pm$ 0.05                           | 0.63 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.61 $\\pm$ 0.07 | 0.63 $\\pm$ 0.03             | 0.67 $\\pm$ 0.05         | 0.69 $\\pm$ 0.04          | 0.67 $\\pm$ 0.06                           | 0.66 $\\pm$ 0.08     |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.64 $\\pm$ 0.06             | 0.68 $\\pm$ 0.07         | 0.67 $\\pm$ 0.08          | 0.67 $\\pm$ 0.08                           | 0.65 $\\pm$ 0.04     |\n| facebook-mbart-large-50            | 0.61 $\\pm$ 0.06 | 0.67 $\\pm$ 0.04             | 0.67 $\\pm$ 0.07         | 0.70 $\\pm$ 0.02          | 0.69 $\\pm$ 0.05                           | **0.71 $\\pm$ 0.02** |\n| gpt2                               | 0.51 $\\pm$ 0.07 | 0.59 $\\pm$ 0.10             | 0.59 $\\pm$ 0.05         | 0.58 $\\pm$ 0.06          | 0.61 $\\pm$ 0.06                           | 0.61 $\\pm$ 0.05     |\n| xlm-roberta-large                  | 0.62 $\\pm$ 0.11 | 0.64 $\\pm$ 0.03             | 0.67 $\\pm$ 0.04         | 0.68 $\\pm$ 0.05          | 0.68 $\\pm$ 0.06                           | 0.63 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.69 $\\pm$ 0.01         | 0.69 $\\pm$ 0.02          | 0.71 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.64 $\\pm$ 0.03          | 0.66 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.69 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.72 $\\pm$ 0.01          | 0.73 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.72 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.00             | 0.72 $\\pm$ 0.01         | 0.74 $\\pm$ 0.01          | 0.73 $\\pm$ 0.00                           | **0.75 $\\pm$ 0.01** |\n| gpt2                               | 0.61 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.68 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.68 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | 0.74 $\\pm$ 0.00     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.01 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.02                           | 0.05 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.02 $\\pm$ 0.00         | 0.02 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.04 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.06 $\\pm$ 0.02          | 0.07 $\\pm$ 0.04                           | 0.05 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.03 $\\pm$ 0.02 | 0.04 $\\pm$ 0.01             | 0.05 $\\pm$ 0.03         | 0.04 $\\pm$ 0.01          | 0.05 $\\pm$ 0.02                           | 0.04 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.03          | 0.05 $\\pm$ 0.03                           | **0.11 $\\pm$ 0.03** |\n| gpt2                               | 0.00 $\\pm$ 0.00 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.03         | 0.04 $\\pm$ 0.06          | 0.02 $\\pm$ 0.02                           | 0.03 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.07 $\\pm$ 0.04          | 0.06 $\\pm$ 0.03                           | 0.06 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Russian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.21 $\\pm$ 0.02 | 0.25 $\\pm$ 0.03             | 0.38 $\\pm$ 0.03         | 0.37 $\\pm$ 0.04          | 0.41 $\\pm$ 0.03                           | 0.41 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.22 $\\pm$ 0.01 | 0.22 $\\pm$ 0.04             | 0.21 $\\pm$ 0.02         | 0.22 $\\pm$ 0.02          | 0.21 $\\pm$ 0.03                           | 0.21 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.39 $\\pm$ 0.01 | 0.45 $\\pm$ 0.02             | 0.50 $\\pm$ 0.05         | 0.51 $\\pm$ 0.06          | 0.51 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.31 $\\pm$ 0.02 | 0.41 $\\pm$ 0.01             | 0.44 $\\pm$ 0.03         | 0.46 $\\pm$ 0.06          | 0.47 $\\pm$ 0.05                           | 0.45 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.40 $\\pm$ 0.04 | 0.50 $\\pm$ 0.03             | 0.51 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.51 $\\pm$ 0.01                           | **0.55 $\\pm$ 0.02** |\n| gpt2                               | 0.16 $\\pm$ 0.08 | 0.14 $\\pm$ 0.05             | 0.07 $\\pm$ 0.07         | 0.14 $\\pm$ 0.08          | 0.16 $\\pm$ 0.06                           | 0.15 $\\pm$ 0.11     |\n| xlm-roberta-large                  | 0.47 $\\pm$ 0.03 | 0.53 $\\pm$ 0.04             | 0.53 $\\pm$ 0.03         | **0.55 $\\pm$ 0.01**      | 0.52 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.14 $\\pm$ 0.01 | 0.16 $\\pm$ 0.02             | 0.31 $\\pm$ 0.05         | 0.29 $\\pm$ 0.05          | 0.32 $\\pm$ 0.02                           | 0.35 $\\pm$ 0.06 |\n| EleutherAI-gpt-neo-125M            | 0.16 $\\pm$ 0.01 | 0.17 $\\pm$ 0.03             | 0.15 $\\pm$ 0.02         | 0.16 $\\pm$ 0.01          | 0.15 $\\pm$ 0.02                           | 0.15 $\\pm$ 0.04 |\n| bert-base-multilingual-cased       | 0.31 $\\pm$ 0.03 | 0.40 $\\pm$ 0.05             | 0.44 $\\pm$ 0.05         | 0.45 $\\pm$ 0.10          | 0.45 $\\pm$ 0.04                           | 0.49 $\\pm$ 0.02 |\n| distilbert-base-multilingual-cased | 0.23 $\\pm$ 0.01 | 0.33 $\\pm$ 0.02             | 0.35 $\\pm$ 0.04         | 0.39 $\\pm$ 0.06          | 0.39 $\\pm$ 0.06                           | 0.38 $\\pm$ 0.03 |\n| facebook-mbart-large-50            | 0.31 $\\pm$ 0.04 | 0.42 $\\pm$ 0.03             | 0.44 $\\pm$ 0.05         | 0.47 $\\pm$ 0.06          | 0.44 $\\pm$ 0.03                           | 0.48 $\\pm$ 0.01 |\n| gpt2                               | 0.11 $\\pm$ 0.06 | 0.09 $\\pm$ 0.04             | 0.04 $\\pm$ 0.05         | 0.09 $\\pm$ 0.06          | 0.10 $\\pm$ 0.04                           | 0.10 $\\pm$ 0.08 |\n| xlm-roberta-large                  | 0.40 $\\pm$ 0.03 | 0.47 $\\pm$ 0.05             | 0.48 $\\pm$ 0.06         | **0.52 $\\pm$ 0.03**      | 0.46 $\\pm$ 0.05                           | 0.50 $\\pm$ 0.07 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.06 | 0.59 $\\pm$ 0.14             | 0.50 $\\pm$ 0.04         | 0.52 $\\pm$ 0.09          | 0.58 $\\pm$ 0.15                           | 0.52 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.03 | 0.33 $\\pm$ 0.08             | 0.36 $\\pm$ 0.05         | 0.37 $\\pm$ 0.03          | 0.37 $\\pm$ 0.10                           | 0.40 $\\pm$ 0.11     |\n| bert-base-multilingual-cased       | 0.53 $\\pm$ 0.02 | 0.53 $\\pm$ 0.05             | 0.59 $\\pm$ 0.04         | 0.60 $\\pm$ 0.02          | 0.60 $\\pm$ 0.04                           | 0.59 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.01         | 0.57 $\\pm$ 0.04          | 0.60 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.06 | 0.60 $\\pm$ 0.03             | 0.61 $\\pm$ 0.04         | 0.63 $\\pm$ 0.04          | 0.62 $\\pm$ 0.02                           | **0.66 $\\pm$ 0.07** |\n| gpt2                               | 0.30 $\\pm$ 0.03 | 0.27 $\\pm$ 0.09             | 0.25 $\\pm$ 0.04         | 0.37 $\\pm$ 0.02          | 0.41 $\\pm$ 0.00                           | 0.46 $\\pm$ 0.15     |\n| xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.61 $\\pm$ 0.01             | 0.61 $\\pm$ 0.06         | 0.58 $\\pm$ 0.03          | 0.58 $\\pm$ 0.02                           | 0.59 $\\pm$ 0.08     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.57 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.63 $\\pm$ 0.01                           | 0.64 $\\pm$ 0.02 |\n| EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.01 | 0.55 $\\pm$ 0.02             | 0.55 $\\pm$ 0.01         | 0.55 $\\pm$ 0.01          | 0.55 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.00 |\n| bert-base-multilingual-cased       | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.69 $\\pm$ 0.03         | 0.69 $\\pm$ 0.04          | 0.69 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.01 |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.01             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.03          | 0.67 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.01 |\n| facebook-mbart-large-50            | 0.63 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.71 $\\pm$ 0.03          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01 |\n| gpt2                               | 0.53 $\\pm$ 0.02 | 0.52 $\\pm$ 0.02             | 0.51 $\\pm$ 0.01         | 0.53 $\\pm$ 0.02          | 0.54 $\\pm$ 0.01                           | 0.53 $\\pm$ 0.03 |\n| xlm-roberta-large                  | 0.67 $\\pm$ 0.01 | 0.70 $\\pm$ 0.03             | 0.71 $\\pm$ 0.02         | **0.72 $\\pm$ 0.01**      | 0.70 $\\pm$ 0.03                           | 0.71 $\\pm$ 0.02 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.03 | 0.06 $\\pm$ 0.03             | 0.10 $\\pm$ 0.01         | 0.08 $\\pm$ 0.03          | 0.13 $\\pm$ 0.05                           | 0.06 $\\pm$ 0.03 |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.03 | 0.04 $\\pm$ 0.05             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.04          | 0.03 $\\pm$ 0.03                           | 0.03 $\\pm$ 0.05 |\n| bert-base-multilingual-cased       | 0.09 $\\pm$ 0.03 | 0.12 $\\pm$ 0.04             | **0.17 $\\pm$ 0.03**     | 0.15 $\\pm$ 0.08          | **0.17 $\\pm$ 0.03**                       | 0.14 $\\pm$ 0.05 |\n| distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.01 | 0.11 $\\pm$ 0.06             | 0.08 $\\pm$ 0.03         | 0.13 $\\pm$ 0.03          | 0.13 $\\pm$ 0.07                           | 0.12 $\\pm$ 0.02 |\n| facebook-mbart-large-50            | 0.11 $\\pm$ 0.03 | 0.16 $\\pm$ 0.03             | 0.14 $\\pm$ 0.03         | **0.17 $\\pm$ 0.05**      | 0.15 $\\pm$ 0.03                           | 0.13 $\\pm$ 0.03 |\n| gpt2                               | 0.02 $\\pm$ 0.02 | 0.03 $\\pm$ 0.02             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.00          | 0.02 $\\pm$ 0.01                           | 0.02 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.12 $\\pm$ 0.05 | **0.17 $\\pm$ 0.04**         | 0.14 $\\pm$ 0.06         | 0.14 $\\pm$ 0.08          | 0.13 $\\pm$ 0.03                           | 0.14 $\\pm$ 0.06 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# All 6 Languages"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.01 | 0.67 $\\pm$ 0.01             | 0.68 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.68 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.00     |\n| en         | EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.03 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.01          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.02     |\n| en         | bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.69 $\\pm$ 0.02          | 0.69 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.61 $\\pm$ 0.03 | 0.66 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.03          | 0.67 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| en         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | **0.71 $\\pm$ 0.02**                       | 0.69 $\\pm$ 0.02     |\n| en         | gpt2                               | 0.63 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.02          | 0.68 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.02     |\n| en         | xlm-roberta-large                  | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | **0.71 $\\pm$ 0.02**      | **0.71 $\\pm$ 0.01**                       | **0.71 $\\pm$ 0.02** |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.04 | 0.47 $\\pm$ 0.01             | 0.50 $\\pm$ 0.04         | 0.50 $\\pm$ 0.01          | 0.52 $\\pm$ 0.01                           | 0.56 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.31 $\\pm$ 0.01 | 0.38 $\\pm$ 0.03             | 0.39 $\\pm$ 0.03         | 0.39 $\\pm$ 0.01          | 0.44 $\\pm$ 0.03                           | 0.47 $\\pm$ 0.04     |\n| fr         | bert-base-multilingual-cased       | 0.47 $\\pm$ 0.04 | 0.52 $\\pm$ 0.03             | 0.53 $\\pm$ 0.02         | 0.55 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.58 $\\pm$ 0.01     |\n| fr         | distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.05 | 0.50 $\\pm$ 0.02             | 0.53 $\\pm$ 0.03         | 0.53 $\\pm$ 0.02          | 0.52 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.02     |\n| fr         | facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.53 $\\pm$ 0.02             | 0.56 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | **0.60 $\\pm$ 0.01** |\n| fr         | gpt2                               | 0.40 $\\pm$ 0.07 | 0.43 $\\pm$ 0.02             | 0.47 $\\pm$ 0.02         | 0.49 $\\pm$ 0.02          | 0.49 $\\pm$ 0.03                           | 0.53 $\\pm$ 0.01     |\n| fr         | xlm-roberta-large                  | 0.51 $\\pm$ 0.02 | 0.56 $\\pm$ 0.03             | 0.57 $\\pm$ 0.01         | 0.58 $\\pm$ 0.03          | 0.58 $\\pm$ 0.01                           | 0.57 $\\pm$ 0.04     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.00 | 0.56 $\\pm$ 0.02             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.00          | 0.57 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.01 | 0.46 $\\pm$ 0.01             | 0.48 $\\pm$ 0.00         | 0.50 $\\pm$ 0.02          | 0.51 $\\pm$ 0.04                           | 0.56 $\\pm$ 0.01     |\n| ge         | bert-base-multilingual-cased       | 0.52 $\\pm$ 0.03 | 0.58 $\\pm$ 0.02             | 0.59 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.59 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.04     |\n| ge         | distilbert-base-multilingual-cased | 0.50 $\\pm$ 0.02 | 0.56 $\\pm$ 0.00             | 0.55 $\\pm$ 0.03         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.61 $\\pm$ 0.02     |\n| ge         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.02             | 0.61 $\\pm$ 0.02         | 0.63 $\\pm$ 0.00          | 0.64 $\\pm$ 0.02                           | **0.65 $\\pm$ 0.02** |\n| ge         | gpt2                               | 0.47 $\\pm$ 0.03 | 0.52 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.55 $\\pm$ 0.03          | 0.53 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.55 $\\pm$ 0.02 | 0.61 $\\pm$ 0.00             | 0.62 $\\pm$ 0.03         | 0.64 $\\pm$ 0.01          | **0.65 $\\pm$ 0.01**                       | **0.65 $\\pm$ 0.02** |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.42 $\\pm$ 0.02 | 0.46 $\\pm$ 0.05             | 0.50 $\\pm$ 0.03         | 0.52 $\\pm$ 0.04          | 0.52 $\\pm$ 0.06                           | 0.58 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.04 | 0.42 $\\pm$ 0.02             | 0.44 $\\pm$ 0.02         | 0.44 $\\pm$ 0.01          | 0.48 $\\pm$ 0.04                           | 0.51 $\\pm$ 0.02     |\n| it         | bert-base-multilingual-cased       | 0.47 $\\pm$ 0.01 | 0.53 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.56 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.59 $\\pm$ 0.03     |\n| it         | distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.01 | 0.50 $\\pm$ 0.05             | 0.52 $\\pm$ 0.03         | 0.56 $\\pm$ 0.05          | 0.52 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.02     |\n| it         | facebook-mbart-large-50            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.04             | 0.55 $\\pm$ 0.02         | 0.57 $\\pm$ 0.03          | 0.57 $\\pm$ 0.04                           | **0.61 $\\pm$ 0.03** |\n| it         | gpt2                               | 0.41 $\\pm$ 0.02 | 0.45 $\\pm$ 0.01             | 0.48 $\\pm$ 0.03         | 0.52 $\\pm$ 0.02          | 0.50 $\\pm$ 0.03                           | 0.56 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.50 $\\pm$ 0.04 | 0.54 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.59 $\\pm$ 0.03          | 0.57 $\\pm$ 0.01                           | 0.59 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.60 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.63 $\\pm$ 0.03                           | 0.64 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.04 | 0.47 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.51 $\\pm$ 0.06          | 0.55 $\\pm$ 0.03                           | 0.59 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.55 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.65 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.02     |\n| po         | distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.03 | 0.58 $\\pm$ 0.04             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.01          | 0.61 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| po         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.04 | 0.60 $\\pm$ 0.01             | 0.63 $\\pm$ 0.03         | 0.66 $\\pm$ 0.02          | 0.65 $\\pm$ 0.02                           | **0.68 $\\pm$ 0.02** |\n| po         | gpt2                               | 0.49 $\\pm$ 0.01 | 0.55 $\\pm$ 0.05             | 0.57 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.59 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.04     |\n| po         | xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.59 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.65 $\\pm$ 0.03          | 0.66 $\\pm$ 0.02                           | 0.67 $\\pm$ 0.02     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.21 $\\pm$ 0.02 | 0.25 $\\pm$ 0.03             | 0.38 $\\pm$ 0.03         | 0.37 $\\pm$ 0.04          | 0.41 $\\pm$ 0.03                           | 0.41 $\\pm$ 0.04     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.22 $\\pm$ 0.01 | 0.22 $\\pm$ 0.04             | 0.21 $\\pm$ 0.02         | 0.22 $\\pm$ 0.02          | 0.21 $\\pm$ 0.03                           | 0.21 $\\pm$ 0.02     |\n| ru         | bert-base-multilingual-cased       | 0.39 $\\pm$ 0.01 | 0.45 $\\pm$ 0.02             | 0.50 $\\pm$ 0.05         | 0.51 $\\pm$ 0.06          | 0.51 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.02     |\n| ru         | distilbert-base-multilingual-cased | 0.31 $\\pm$ 0.02 | 0.41 $\\pm$ 0.01             | 0.44 $\\pm$ 0.03         | 0.46 $\\pm$ 0.06          | 0.47 $\\pm$ 0.05                           | 0.45 $\\pm$ 0.02     |\n| ru         | facebook-mbart-large-50            | 0.40 $\\pm$ 0.04 | 0.50 $\\pm$ 0.03             | 0.51 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.51 $\\pm$ 0.01                           | **0.55 $\\pm$ 0.02** |\n| ru         | gpt2                               | 0.16 $\\pm$ 0.08 | 0.14 $\\pm$ 0.05             | 0.07 $\\pm$ 0.07         | 0.14 $\\pm$ 0.08          | 0.16 $\\pm$ 0.06                           | 0.15 $\\pm$ 0.11     |\n| ru         | xlm-roberta-large                  | 0.47 $\\pm$ 0.03 | 0.53 $\\pm$ 0.04             | 0.53 $\\pm$ 0.03         | **0.55 $\\pm$ 0.01**      | 0.52 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.02          | 0.60 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.02     |\n| en         | EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.01             | 0.55 $\\pm$ 0.03         | 0.55 $\\pm$ 0.00          | 0.59 $\\pm$ 0.03                           | 0.62 $\\pm$ 0.04     |\n| en         | bert-base-multilingual-cased       | 0.58 $\\pm$ 0.01 | 0.63 $\\pm$ 0.03             | 0.64 $\\pm$ 0.02         | 0.65 $\\pm$ 0.01          | 0.64 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.03     |\n| en         | distilbert-base-multilingual-cased | 0.56 $\\pm$ 0.03 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.63 $\\pm$ 0.03          | 0.60 $\\pm$ 0.01                           | 0.64 $\\pm$ 0.01     |\n| en         | facebook-mbart-large-50            | 0.60 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.66 $\\pm$ 0.01          | 0.66 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.02     |\n| en         | gpt2                               | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.65 $\\pm$ 0.02         | 0.67 $\\pm$ 0.04          | 0.65 $\\pm$ 0.04                           | 0.66 $\\pm$ 0.03     |\n| en         | xlm-roberta-large                  | 0.60 $\\pm$ 0.01 | 0.66 $\\pm$ 0.01             | 0.66 $\\pm$ 0.02         | 0.67 $\\pm$ 0.01          | **0.68 $\\pm$ 0.02**                       | **0.68 $\\pm$ 0.01** |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.32 $\\pm$ 0.05 | 0.37 $\\pm$ 0.01             | 0.45 $\\pm$ 0.04         | 0.42 $\\pm$ 0.03          | 0.44 $\\pm$ 0.04                           | 0.50 $\\pm$ 0.05     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.24 $\\pm$ 0.01 | 0.32 $\\pm$ 0.04             | 0.32 $\\pm$ 0.04         | 0.31 $\\pm$ 0.03          | 0.38 $\\pm$ 0.04                           | 0.38 $\\pm$ 0.07     |\n| fr         | bert-base-multilingual-cased       | 0.38 $\\pm$ 0.04 | 0.46 $\\pm$ 0.03             | 0.49 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.51 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.03     |\n| fr         | distilbert-base-multilingual-cased | 0.37 $\\pm$ 0.06 | 0.44 $\\pm$ 0.02             | 0.48 $\\pm$ 0.04         | 0.48 $\\pm$ 0.02          | 0.46 $\\pm$ 0.03                           | 0.49 $\\pm$ 0.02     |\n| fr         | facebook-mbart-large-50            | 0.44 $\\pm$ 0.03 | 0.47 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.52 $\\pm$ 0.03          | 0.53 $\\pm$ 0.03                           | **0.55 $\\pm$ 0.04** |\n| fr         | gpt2                               | 0.36 $\\pm$ 0.06 | 0.36 $\\pm$ 0.02             | 0.43 $\\pm$ 0.02         | 0.43 $\\pm$ 0.02          | 0.43 $\\pm$ 0.06                           | 0.48 $\\pm$ 0.01     |\n| fr         | xlm-roberta-large                  | 0.44 $\\pm$ 0.02 | 0.51 $\\pm$ 0.02             | 0.52 $\\pm$ 0.04         | **0.55 $\\pm$ 0.04**      | **0.55 $\\pm$ 0.02**                       | 0.54 $\\pm$ 0.07     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.03 | 0.49 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.51 $\\pm$ 0.02                           | 0.59 $\\pm$ 0.04     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.33 $\\pm$ 0.03 | 0.41 $\\pm$ 0.02             | 0.41 $\\pm$ 0.01         | 0.43 $\\pm$ 0.02          | 0.45 $\\pm$ 0.03                           | 0.51 $\\pm$ 0.02     |\n| ge         | bert-base-multilingual-cased       | 0.46 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.54 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.54 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.06     |\n| ge         | distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.01 | 0.51 $\\pm$ 0.02             | 0.49 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.52 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.05     |\n| ge         | facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.55 $\\pm$ 0.02             | 0.56 $\\pm$ 0.03         | 0.59 $\\pm$ 0.01          | 0.59 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.05     |\n| ge         | gpt2                               | 0.46 $\\pm$ 0.05 | 0.49 $\\pm$ 0.05             | 0.50 $\\pm$ 0.05         | 0.52 $\\pm$ 0.05          | 0.52 $\\pm$ 0.04                           | 0.56 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.50 $\\pm$ 0.02 | 0.57 $\\pm$ 0.02             | 0.57 $\\pm$ 0.02         | 0.58 $\\pm$ 0.01          | 0.63 $\\pm$ 0.07                           | **0.66 $\\pm$ 0.04** |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.35 $\\pm$ 0.04 | 0.38 $\\pm$ 0.06             | 0.44 $\\pm$ 0.02         | 0.45 $\\pm$ 0.07          | 0.45 $\\pm$ 0.09                           | 0.51 $\\pm$ 0.02     |\n| it         | EleutherAI-gpt-neo-125M            | 0.28 $\\pm$ 0.04 | 0.36 $\\pm$ 0.04             | 0.36 $\\pm$ 0.02         | 0.35 $\\pm$ 0.02          | 0.41 $\\pm$ 0.06                           | 0.42 $\\pm$ 0.03     |\n| it         | bert-base-multilingual-cased       | 0.40 $\\pm$ 0.00 | 0.48 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | 0.51 $\\pm$ 0.03          | 0.52 $\\pm$ 0.01                           | 0.54 $\\pm$ 0.02     |\n| it         | distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.01 | 0.44 $\\pm$ 0.04             | 0.46 $\\pm$ 0.03         | 0.50 $\\pm$ 0.06          | 0.46 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.00     |\n| it         | facebook-mbart-large-50            | 0.42 $\\pm$ 0.01 | 0.47 $\\pm$ 0.06             | 0.49 $\\pm$ 0.02         | 0.51 $\\pm$ 0.03          | 0.51 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.04     |\n| it         | gpt2                               | 0.37 $\\pm$ 0.03 | 0.40 $\\pm$ 0.01             | 0.45 $\\pm$ 0.03         | 0.49 $\\pm$ 0.03          | 0.47 $\\pm$ 0.00                           | 0.52 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.43 $\\pm$ 0.05 | 0.49 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | 0.54 $\\pm$ 0.02          | 0.54 $\\pm$ 0.02                           | **0.57 $\\pm$ 0.04** |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.03 | 0.49 $\\pm$ 0.02             | 0.58 $\\pm$ 0.02         | 0.53 $\\pm$ 0.04          | 0.58 $\\pm$ 0.05                           | 0.62 $\\pm$ 0.04     |\n| po         | EleutherAI-gpt-neo-125M            | 0.34 $\\pm$ 0.05 | 0.42 $\\pm$ 0.04             | 0.50 $\\pm$ 0.02         | 0.44 $\\pm$ 0.06          | 0.52 $\\pm$ 0.01                           | 0.56 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.50 $\\pm$ 0.04 | 0.56 $\\pm$ 0.04             | 0.56 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.63 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.03     |\n| po         | distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.05         | 0.55 $\\pm$ 0.03          | 0.56 $\\pm$ 0.05                           | 0.62 $\\pm$ 0.02     |\n| po         | facebook-mbart-large-50            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.59 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.61 $\\pm$ 0.03                           | 0.64 $\\pm$ 0.05     |\n| po         | gpt2                               | 0.48 $\\pm$ 0.03 | 0.52 $\\pm$ 0.01             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.03          | 0.59 $\\pm$ 0.06                           | 0.63 $\\pm$ 0.04     |\n| po         | xlm-roberta-large                  | 0.54 $\\pm$ 0.02 | 0.56 $\\pm$ 0.05             | 0.60 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.64 $\\pm$ 0.03                           | **0.73 $\\pm$ 0.05** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.14 $\\pm$ 0.01 | 0.16 $\\pm$ 0.02             | 0.31 $\\pm$ 0.05         | 0.29 $\\pm$ 0.05          | 0.32 $\\pm$ 0.02                           | 0.35 $\\pm$ 0.06     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.16 $\\pm$ 0.01 | 0.17 $\\pm$ 0.03             | 0.15 $\\pm$ 0.02         | 0.16 $\\pm$ 0.01          | 0.15 $\\pm$ 0.02                           | 0.15 $\\pm$ 0.04     |\n| ru         | bert-base-multilingual-cased       | 0.31 $\\pm$ 0.03 | 0.40 $\\pm$ 0.05             | 0.44 $\\pm$ 0.05         | 0.45 $\\pm$ 0.10          | 0.45 $\\pm$ 0.04                           | 0.49 $\\pm$ 0.02     |\n| ru         | distilbert-base-multilingual-cased | 0.23 $\\pm$ 0.01 | 0.33 $\\pm$ 0.02             | 0.35 $\\pm$ 0.04         | 0.39 $\\pm$ 0.06          | 0.39 $\\pm$ 0.06                           | 0.38 $\\pm$ 0.03     |\n| ru         | facebook-mbart-large-50            | 0.31 $\\pm$ 0.04 | 0.42 $\\pm$ 0.03             | 0.44 $\\pm$ 0.05         | 0.47 $\\pm$ 0.06          | 0.44 $\\pm$ 0.03                           | 0.48 $\\pm$ 0.01     |\n| ru         | gpt2                               | 0.11 $\\pm$ 0.06 | 0.09 $\\pm$ 0.04             | 0.04 $\\pm$ 0.05         | 0.09 $\\pm$ 0.06          | 0.10 $\\pm$ 0.04                           | 0.10 $\\pm$ 0.08     |\n| ru         | xlm-roberta-large                  | 0.40 $\\pm$ 0.03 | 0.47 $\\pm$ 0.05             | 0.48 $\\pm$ 0.06         | **0.52 $\\pm$ 0.03**      | 0.46 $\\pm$ 0.05                           | 0.50 $\\pm$ 0.07     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.72 $\\pm$ 0.04 | 0.76 $\\pm$ 0.02             | 0.74 $\\pm$ 0.02         | 0.76 $\\pm$ 0.01          | **0.77 $\\pm$ 0.03**                       | **0.77 $\\pm$ 0.03** |\n| en         | EleutherAI-gpt-neo-125M            | 0.63 $\\pm$ 0.04 | 0.67 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | 0.71 $\\pm$ 0.03          | 0.73 $\\pm$ 0.04                           | 0.75 $\\pm$ 0.03     |\n| en         | bert-base-multilingual-cased       | 0.70 $\\pm$ 0.03 | 0.74 $\\pm$ 0.03             | 0.76 $\\pm$ 0.02         | 0.74 $\\pm$ 0.04          | 0.75 $\\pm$ 0.03                           | 0.75 $\\pm$ 0.02     |\n| en         | distilbert-base-multilingual-cased | 0.68 $\\pm$ 0.05 | 0.74 $\\pm$ 0.03             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.03          | 0.75 $\\pm$ 0.02                           | 0.75 $\\pm$ 0.03     |\n| en         | facebook-mbart-large-50            | 0.73 $\\pm$ 0.01 | 0.76 $\\pm$ 0.01             | 0.74 $\\pm$ 0.03         | 0.76 $\\pm$ 0.01          | **0.77 $\\pm$ 0.02**                       | 0.74 $\\pm$ 0.02     |\n| en         | gpt2                               | 0.67 $\\pm$ 0.06 | 0.72 $\\pm$ 0.02             | 0.70 $\\pm$ 0.03         | 0.67 $\\pm$ 0.03          | 0.71 $\\pm$ 0.05                           | 0.72 $\\pm$ 0.03     |\n| en         | xlm-roberta-large                  | 0.73 $\\pm$ 0.03 | 0.75 $\\pm$ 0.03             | 0.75 $\\pm$ 0.01         | 0.76 $\\pm$ 0.03          | 0.74 $\\pm$ 0.01                           | 0.74 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.03 | 0.63 $\\pm$ 0.04             | 0.57 $\\pm$ 0.04         | 0.62 $\\pm$ 0.06          | 0.65 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.42 $\\pm$ 0.05 | 0.48 $\\pm$ 0.03             | 0.51 $\\pm$ 0.02         | 0.55 $\\pm$ 0.06          | 0.53 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.04     |\n| fr         | bert-base-multilingual-cased       | 0.59 $\\pm$ 0.05 | 0.60 $\\pm$ 0.02             | 0.59 $\\pm$ 0.03         | 0.61 $\\pm$ 0.05          | 0.62 $\\pm$ 0.01                           | 0.64 $\\pm$ 0.03     |\n| fr         | distilbert-base-multilingual-cased | 0.53 $\\pm$ 0.05 | 0.59 $\\pm$ 0.05             | 0.59 $\\pm$ 0.03         | 0.59 $\\pm$ 0.03          | 0.62 $\\pm$ 0.01                           | 0.61 $\\pm$ 0.00     |\n| fr         | facebook-mbart-large-50            | 0.60 $\\pm$ 0.06 | 0.62 $\\pm$ 0.01             | 0.62 $\\pm$ 0.04         | 0.63 $\\pm$ 0.02          | 0.64 $\\pm$ 0.03                           | **0.67 $\\pm$ 0.05** |\n| fr         | gpt2                               | 0.45 $\\pm$ 0.10 | 0.51 $\\pm$ 0.04             | 0.53 $\\pm$ 0.02         | 0.57 $\\pm$ 0.04          | 0.59 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.02     |\n| fr         | xlm-roberta-large                  | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.62 $\\pm$ 0.01          | 0.61 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.01     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.57 $\\pm$ 0.05 | 0.65 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.70 $\\pm$ 0.06          | 0.66 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.02     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.02 | 0.51 $\\pm$ 0.04             | 0.57 $\\pm$ 0.01         | 0.58 $\\pm$ 0.02          | 0.60 $\\pm$ 0.05                           | 0.62 $\\pm$ 0.01     |\n| ge         | bert-base-multilingual-cased       | 0.60 $\\pm$ 0.02 | 0.62 $\\pm$ 0.02             | 0.66 $\\pm$ 0.03         | 0.68 $\\pm$ 0.06          | 0.66 $\\pm$ 0.01                           | 0.65 $\\pm$ 0.03     |\n| ge         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.63 $\\pm$ 0.02             | 0.63 $\\pm$ 0.04         | 0.64 $\\pm$ 0.05          | 0.66 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.01     |\n| ge         | facebook-mbart-large-50            | 0.62 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.68 $\\pm$ 0.00         | 0.68 $\\pm$ 0.02          | 0.70 $\\pm$ 0.04                           | 0.70 $\\pm$ 0.02     |\n| ge         | gpt2                               | 0.49 $\\pm$ 0.01 | 0.56 $\\pm$ 0.03             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.00          | 0.55 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| ge         | xlm-roberta-large                  | 0.62 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.04         | **0.71 $\\pm$ 0.01**      | 0.68 $\\pm$ 0.04                           | 0.64 $\\pm$ 0.06     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.53 $\\pm$ 0.03 | 0.58 $\\pm$ 0.03             | 0.58 $\\pm$ 0.05         | 0.63 $\\pm$ 0.05          | 0.63 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.05     |\n| it         | EleutherAI-gpt-neo-125M            | 0.47 $\\pm$ 0.05 | 0.49 $\\pm$ 0.01             | 0.55 $\\pm$ 0.01         | 0.57 $\\pm$ 0.01          | 0.59 $\\pm$ 0.04                           | 0.64 $\\pm$ 0.03     |\n| it         | bert-base-multilingual-cased       | 0.57 $\\pm$ 0.02 | 0.59 $\\pm$ 0.04             | 0.61 $\\pm$ 0.04         | 0.62 $\\pm$ 0.05          | 0.62 $\\pm$ 0.07                           | 0.65 $\\pm$ 0.06     |\n| it         | distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.02 | 0.57 $\\pm$ 0.05             | 0.58 $\\pm$ 0.03         | 0.62 $\\pm$ 0.03          | 0.60 $\\pm$ 0.04                           | 0.62 $\\pm$ 0.05     |\n| it         | facebook-mbart-large-50            | 0.57 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.03         | 0.64 $\\pm$ 0.03          | 0.65 $\\pm$ 0.08                           | **0.68 $\\pm$ 0.05** |\n| it         | gpt2                               | 0.47 $\\pm$ 0.01 | 0.52 $\\pm$ 0.02             | 0.52 $\\pm$ 0.03         | 0.55 $\\pm$ 0.01          | 0.55 $\\pm$ 0.06                           | 0.61 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.58 $\\pm$ 0.03 | 0.59 $\\pm$ 0.03             | 0.61 $\\pm$ 0.02         | 0.64 $\\pm$ 0.05          | 0.61 $\\pm$ 0.04                           | 0.61 $\\pm$ 0.05     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.10 | 0.63 $\\pm$ 0.05             | 0.63 $\\pm$ 0.06         | 0.64 $\\pm$ 0.06          | 0.69 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-125M            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.03             | 0.60 $\\pm$ 0.06         | 0.61 $\\pm$ 0.07          | 0.59 $\\pm$ 0.05                           | 0.63 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.61 $\\pm$ 0.07 | 0.63 $\\pm$ 0.03             | 0.67 $\\pm$ 0.05         | 0.69 $\\pm$ 0.04          | 0.67 $\\pm$ 0.06                           | 0.66 $\\pm$ 0.08     |\n| po         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.64 $\\pm$ 0.06             | 0.68 $\\pm$ 0.07         | 0.67 $\\pm$ 0.08          | 0.67 $\\pm$ 0.08                           | 0.65 $\\pm$ 0.04     |\n| po         | facebook-mbart-large-50            | 0.61 $\\pm$ 0.06 | 0.67 $\\pm$ 0.04             | 0.67 $\\pm$ 0.07         | 0.70 $\\pm$ 0.02          | 0.69 $\\pm$ 0.05                           | **0.71 $\\pm$ 0.02** |\n| po         | gpt2                               | 0.51 $\\pm$ 0.07 | 0.59 $\\pm$ 0.10             | 0.59 $\\pm$ 0.05         | 0.58 $\\pm$ 0.06          | 0.61 $\\pm$ 0.06                           | 0.61 $\\pm$ 0.05     |\n| po         | xlm-roberta-large                  | 0.62 $\\pm$ 0.11 | 0.64 $\\pm$ 0.03             | 0.67 $\\pm$ 0.04         | 0.68 $\\pm$ 0.05          | 0.68 $\\pm$ 0.06                           | 0.63 $\\pm$ 0.01     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.06 | 0.59 $\\pm$ 0.14             | 0.50 $\\pm$ 0.04         | 0.52 $\\pm$ 0.09          | 0.58 $\\pm$ 0.15                           | 0.52 $\\pm$ 0.05     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.03 | 0.33 $\\pm$ 0.08             | 0.36 $\\pm$ 0.05         | 0.37 $\\pm$ 0.03          | 0.37 $\\pm$ 0.10                           | 0.40 $\\pm$ 0.11     |\n| ru         | bert-base-multilingual-cased       | 0.53 $\\pm$ 0.02 | 0.53 $\\pm$ 0.05             | 0.59 $\\pm$ 0.04         | 0.60 $\\pm$ 0.02          | 0.60 $\\pm$ 0.04                           | 0.59 $\\pm$ 0.02     |\n| ru         | distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.01         | 0.57 $\\pm$ 0.04          | 0.60 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.01     |\n| ru         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.06 | 0.60 $\\pm$ 0.03             | 0.61 $\\pm$ 0.04         | 0.63 $\\pm$ 0.04          | 0.62 $\\pm$ 0.02                           | **0.66 $\\pm$ 0.07** |\n| ru         | gpt2                               | 0.30 $\\pm$ 0.03 | 0.27 $\\pm$ 0.09             | 0.25 $\\pm$ 0.04         | 0.37 $\\pm$ 0.02          | 0.41 $\\pm$ 0.00                           | 0.46 $\\pm$ 0.15     |\n| ru         | xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.61 $\\pm$ 0.01             | 0.61 $\\pm$ 0.06         | 0.58 $\\pm$ 0.03          | 0.58 $\\pm$ 0.02                           | 0.59 $\\pm$ 0.08     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.73 $\\pm$ 0.00 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.76 $\\pm$ 0.00          | 0.77 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.00     |\n| en         | EleutherAI-gpt-neo-125M            | 0.69 $\\pm$ 0.01 | 0.72 $\\pm$ 0.01             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.00          | 0.75 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| en         | bert-base-multilingual-cased       | 0.74 $\\pm$ 0.01 | 0.77 $\\pm$ 0.01             | 0.78 $\\pm$ 0.01         | 0.78 $\\pm$ 0.01          | 0.78 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.73 $\\pm$ 0.02 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.77 $\\pm$ 0.02          | 0.76 $\\pm$ 0.00                           | 0.78 $\\pm$ 0.01     |\n| en         | facebook-mbart-large-50            | 0.76 $\\pm$ 0.01 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | 0.78 $\\pm$ 0.01     |\n| en         | gpt2                               | 0.74 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.77 $\\pm$ 0.00         | 0.77 $\\pm$ 0.02          | 0.77 $\\pm$ 0.01                           | 0.78 $\\pm$ 0.01     |\n| en         | xlm-roberta-large                  | 0.76 $\\pm$ 0.00 | **0.79 $\\pm$ 0.00**         | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | **0.79 $\\pm$ 0.01** |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.68 $\\pm$ 0.00          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.01         | 0.62 $\\pm$ 0.01          | 0.64 $\\pm$ 0.02                           | 0.66 $\\pm$ 0.02     |\n| fr         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.70 $\\pm$ 0.02         | 0.71 $\\pm$ 0.01          | 0.71 $\\pm$ 0.02                           | 0.73 $\\pm$ 0.01     |\n| fr         | distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| fr         | facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.00         | 0.72 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | **0.74 $\\pm$ 0.01** |\n| fr         | gpt2                               | 0.62 $\\pm$ 0.04 | 0.64 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.01     |\n| fr         | xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.01         | 0.73 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.03     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.64 $\\pm$ 0.00 | 0.69 $\\pm$ 0.02             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.00          | 0.69 $\\pm$ 0.01                           | 0.73 $\\pm$ 0.01     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.01             | 0.64 $\\pm$ 0.00         | 0.65 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.00     |\n| ge         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.01         | 0.73 $\\pm$ 0.01          | 0.71 $\\pm$ 0.00                           | 0.73 $\\pm$ 0.03     |\n| ge         | distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.69 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.00                           | 0.72 $\\pm$ 0.02     |\n| ge         | facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.01          | **0.74 $\\pm$ 0.01**                       | **0.74 $\\pm$ 0.01** |\n| ge         | gpt2                               | 0.62 $\\pm$ 0.02 | 0.66 $\\pm$ 0.03             | 0.66 $\\pm$ 0.03         | 0.67 $\\pm$ 0.02          | 0.66 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.72 $\\pm$ 0.00             | 0.72 $\\pm$ 0.02         | **0.74 $\\pm$ 0.01**      | **0.74 $\\pm$ 0.01**                       | **0.74 $\\pm$ 0.02** |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.61 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.66 $\\pm$ 0.02         | 0.67 $\\pm$ 0.02          | 0.68 $\\pm$ 0.03                           | 0.71 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.01     |\n| it         | bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.02     |\n| it         | distilbert-base-multilingual-cased | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.69 $\\pm$ 0.03          | 0.67 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.02     |\n| it         | facebook-mbart-large-50            | 0.65 $\\pm$ 0.00 | 0.68 $\\pm$ 0.03             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.02          | 0.70 $\\pm$ 0.03                           | **0.73 $\\pm$ 0.02** |\n| it         | gpt2                               | 0.61 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.65 $\\pm$ 0.02         | 0.67 $\\pm$ 0.01          | 0.66 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.01         | 0.71 $\\pm$ 0.02          | 0.70 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.69 $\\pm$ 0.01         | 0.69 $\\pm$ 0.02          | 0.71 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| po         | EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.64 $\\pm$ 0.03          | 0.66 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.01     |\n| po         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.69 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.72 $\\pm$ 0.01          | 0.73 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| po         | distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.72 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.00             | 0.72 $\\pm$ 0.01         | 0.74 $\\pm$ 0.01          | 0.73 $\\pm$ 0.00                           | **0.75 $\\pm$ 0.01** |\n| po         | gpt2                               | 0.61 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.68 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| po         | xlm-roberta-large                  | 0.68 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | 0.74 $\\pm$ 0.00     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.57 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.63 $\\pm$ 0.01                           | 0.64 $\\pm$ 0.02     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.01 | 0.55 $\\pm$ 0.02             | 0.55 $\\pm$ 0.01         | 0.55 $\\pm$ 0.01          | 0.55 $\\pm$ 0.02                           | 0.55 $\\pm$ 0.00     |\n| ru         | bert-base-multilingual-cased       | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.69 $\\pm$ 0.03         | 0.69 $\\pm$ 0.04          | 0.69 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.01     |\n| ru         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.01             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.03          | 0.67 $\\pm$ 0.03                           | 0.66 $\\pm$ 0.01     |\n| ru         | facebook-mbart-large-50            | 0.63 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.71 $\\pm$ 0.03          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| ru         | gpt2                               | 0.53 $\\pm$ 0.02 | 0.52 $\\pm$ 0.02             | 0.51 $\\pm$ 0.01         | 0.53 $\\pm$ 0.02          | 0.54 $\\pm$ 0.01                           | 0.53 $\\pm$ 0.03     |\n| ru         | xlm-roberta-large                  | 0.67 $\\pm$ 0.01 | 0.70 $\\pm$ 0.03             | 0.71 $\\pm$ 0.02         | **0.72 $\\pm$ 0.01**      | 0.70 $\\pm$ 0.03                           | 0.71 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.08 $\\pm$ 0.02 | 0.12 $\\pm$ 0.02             | 0.08 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.11 $\\pm$ 0.02                           | 0.12 $\\pm$ 0.02     |\n| en         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.05 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.01          | 0.09 $\\pm$ 0.01                           | 0.09 $\\pm$ 0.01     |\n| en         | bert-base-multilingual-cased       | 0.07 $\\pm$ 0.01 | 0.10 $\\pm$ 0.03             | **0.13 $\\pm$ 0.03**     | 0.09 $\\pm$ 0.00          | 0.10 $\\pm$ 0.03                           | 0.10 $\\pm$ 0.00     |\n| en         | distilbert-base-multilingual-cased | 0.06 $\\pm$ 0.01 | 0.09 $\\pm$ 0.02             | 0.10 $\\pm$ 0.02         | 0.09 $\\pm$ 0.01          | 0.09 $\\pm$ 0.00                           | 0.11 $\\pm$ 0.02     |\n| en         | facebook-mbart-large-50            | 0.07 $\\pm$ 0.04 | 0.10 $\\pm$ 0.03             | 0.11 $\\pm$ 0.03         | 0.12 $\\pm$ 0.03          | 0.12 $\\pm$ 0.02                           | 0.10 $\\pm$ 0.00     |\n| en         | gpt2                               | 0.06 $\\pm$ 0.01 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.06 $\\pm$ 0.03          | 0.08 $\\pm$ 0.01                           | 0.09 $\\pm$ 0.01     |\n| en         | xlm-roberta-large                  | 0.10 $\\pm$ 0.01 | 0.12 $\\pm$ 0.01             | 0.10 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.11 $\\pm$ 0.02                           | 0.11 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.02 | 0.07 $\\pm$ 0.01             | 0.06 $\\pm$ 0.03         | 0.08 $\\pm$ 0.02          | 0.09 $\\pm$ 0.03                           | 0.11 $\\pm$ 0.04     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.01 $\\pm$ 0.01             | 0.03 $\\pm$ 0.01         | 0.04 $\\pm$ 0.01          | 0.04 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.03     |\n| fr         | bert-base-multilingual-cased       | 0.07 $\\pm$ 0.02 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.09 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | 0.11 $\\pm$ 0.03     |\n| fr         | distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.06 $\\pm$ 0.04          | 0.09 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.02     |\n| fr         | facebook-mbart-large-50            | 0.08 $\\pm$ 0.04 | 0.11 $\\pm$ 0.02             | 0.09 $\\pm$ 0.01         | 0.10 $\\pm$ 0.03          | 0.11 $\\pm$ 0.03                           | 0.11 $\\pm$ 0.01     |\n| fr         | gpt2                               | 0.03 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.04         | 0.07 $\\pm$ 0.02          | 0.06 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.06     |\n| fr         | xlm-roberta-large                  | 0.07 $\\pm$ 0.03 | 0.07 $\\pm$ 0.04             | 0.10 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | **0.12 $\\pm$ 0.05**                       | 0.08 $\\pm$ 0.05     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.05 $\\pm$ 0.03         | 0.05 $\\pm$ 0.03          | 0.03 $\\pm$ 0.03                           | 0.06 $\\pm$ 0.04     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.03 $\\pm$ 0.02         | 0.02 $\\pm$ 0.01          | 0.04 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.03     |\n| ge         | bert-base-multilingual-cased       | 0.05 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.03         | 0.09 $\\pm$ 0.05          | 0.06 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.07     |\n| ge         | distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.02             | 0.05 $\\pm$ 0.04         | 0.05 $\\pm$ 0.03          | 0.05 $\\pm$ 0.04                           | 0.04 $\\pm$ 0.04     |\n| ge         | facebook-mbart-large-50            | 0.05 $\\pm$ 0.03 | 0.06 $\\pm$ 0.04             | 0.06 $\\pm$ 0.03         | 0.05 $\\pm$ 0.02          | **0.11 $\\pm$ 0.03**                       | 0.07 $\\pm$ 0.04     |\n| ge         | gpt2                               | 0.01 $\\pm$ 0.01 | 0.01 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.03          | 0.03 $\\pm$ 0.01                           | 0.02 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.03 $\\pm$ 0.02 | 0.07 $\\pm$ 0.03             | 0.09 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | 0.07 $\\pm$ 0.04                           | 0.05 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.04             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.03          | 0.06 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.04 $\\pm$ 0.01             | 0.04 $\\pm$ 0.04         | 0.01 $\\pm$ 0.01          | 0.05 $\\pm$ 0.01                           | 0.08 $\\pm$ 0.02     |\n| it         | bert-base-multilingual-cased       | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.06 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | 0.09 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.04     |\n| it         | distilbert-base-multilingual-cased | 0.04 $\\pm$ 0.02 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.02         | 0.06 $\\pm$ 0.04          | 0.08 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| it         | facebook-mbart-large-50            | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.04             | 0.05 $\\pm$ 0.02         | 0.09 $\\pm$ 0.02          | 0.08 $\\pm$ 0.03                           | **0.10 $\\pm$ 0.02** |\n| it         | gpt2                               | 0.02 $\\pm$ 0.02 | 0.02 $\\pm$ 0.02             | 0.04 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.03 $\\pm$ 0.00                           | 0.04 $\\pm$ 0.02     |\n| it         | xlm-roberta-large                  | 0.06 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.07 $\\pm$ 0.04                           | 0.07 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.01 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.02                           | 0.05 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.02 $\\pm$ 0.00         | 0.02 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.04     |\n| po         | bert-base-multilingual-cased       | 0.04 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.06 $\\pm$ 0.02          | 0.07 $\\pm$ 0.04                           | 0.05 $\\pm$ 0.02     |\n| po         | distilbert-base-multilingual-cased | 0.03 $\\pm$ 0.02 | 0.04 $\\pm$ 0.01             | 0.05 $\\pm$ 0.03         | 0.04 $\\pm$ 0.01          | 0.05 $\\pm$ 0.02                           | 0.04 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.03          | 0.05 $\\pm$ 0.03                           | **0.11 $\\pm$ 0.03** |\n| po         | gpt2                               | 0.00 $\\pm$ 0.00 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.03         | 0.04 $\\pm$ 0.06          | 0.02 $\\pm$ 0.02                           | 0.03 $\\pm$ 0.02     |\n| po         | xlm-roberta-large                  | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.07 $\\pm$ 0.04          | 0.06 $\\pm$ 0.03                           | 0.06 $\\pm$ 0.02     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.03 | 0.06 $\\pm$ 0.03             | 0.10 $\\pm$ 0.01         | 0.08 $\\pm$ 0.03          | 0.13 $\\pm$ 0.05                           | 0.06 $\\pm$ 0.03     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.03 | 0.04 $\\pm$ 0.05             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.04          | 0.03 $\\pm$ 0.03                           | 0.03 $\\pm$ 0.05     |\n| ru         | bert-base-multilingual-cased       | 0.09 $\\pm$ 0.03 | 0.12 $\\pm$ 0.04             | **0.17 $\\pm$ 0.03**     | 0.15 $\\pm$ 0.08          | **0.17 $\\pm$ 0.03**                       | 0.14 $\\pm$ 0.05     |\n| ru         | distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.01 | 0.11 $\\pm$ 0.06             | 0.08 $\\pm$ 0.03         | 0.13 $\\pm$ 0.03          | 0.13 $\\pm$ 0.07                           | 0.12 $\\pm$ 0.02     |\n| ru         | facebook-mbart-large-50            | 0.11 $\\pm$ 0.03 | 0.16 $\\pm$ 0.03             | 0.14 $\\pm$ 0.03         | **0.17 $\\pm$ 0.05**      | 0.15 $\\pm$ 0.03                           | 0.13 $\\pm$ 0.03     |\n| ru         | gpt2                               | 0.02 $\\pm$ 0.02 | 0.03 $\\pm$ 0.02             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.00          | 0.02 $\\pm$ 0.01                           | 0.02 $\\pm$ 0.01     |\n| ru         | xlm-roberta-large                  | 0.12 $\\pm$ 0.05 | **0.17 $\\pm$ 0.04**         | 0.14 $\\pm$ 0.06         | 0.14 $\\pm$ 0.08          | 0.13 $\\pm$ 0.03                           | 0.14 $\\pm$ 0.06     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    }
   ],
   "source": [
    "display_metrics_and_write_to_file(df=results_mean_pred_df, grouping_criterion=['model_name'], output_dir='per_model_name_tables_mean_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# English"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.01 | 0.67 $\\pm$ 0.01             | 0.68 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.00 |\n| EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.03 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.01          | 0.64 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01 |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.69 $\\pm$ 0.02          | 0.69 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.01 |\n| distilbert-base-multilingual-cased | 0.61 $\\pm$ 0.03 | 0.66 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.03          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.02 |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | **0.71 $\\pm$ 0.02**                       | 0.69 $\\pm$ 0.02 |\n| gpt2                               | 0.63 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02 |\n| xlm-roberta-large                  | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | **0.71 $\\pm$ 0.02**      | 0.70 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.02          | 0.59 $\\pm$ 0.03                           | 0.61 $\\pm$ 0.01 |\n| EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.01             | 0.55 $\\pm$ 0.03         | 0.55 $\\pm$ 0.00          | 0.56 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.03 |\n| bert-base-multilingual-cased       | 0.58 $\\pm$ 0.01 | 0.63 $\\pm$ 0.03             | 0.64 $\\pm$ 0.02         | 0.65 $\\pm$ 0.01          | 0.63 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.03 |\n| distilbert-base-multilingual-cased | 0.56 $\\pm$ 0.03 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.63 $\\pm$ 0.03          | 0.59 $\\pm$ 0.00                           | 0.61 $\\pm$ 0.01 |\n| facebook-mbart-large-50            | 0.60 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.66 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.02 |\n| gpt2                               | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.65 $\\pm$ 0.02         | **0.67 $\\pm$ 0.04**      | 0.63 $\\pm$ 0.04                           | 0.63 $\\pm$ 0.03 |\n| xlm-roberta-large                  | 0.60 $\\pm$ 0.01 | 0.66 $\\pm$ 0.01             | 0.66 $\\pm$ 0.02         | **0.67 $\\pm$ 0.01**      | 0.66 $\\pm$ 0.02                           | 0.65 $\\pm$ 0.01 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.72 $\\pm$ 0.04 | 0.76 $\\pm$ 0.02             | 0.74 $\\pm$ 0.02         | 0.76 $\\pm$ 0.01          | 0.78 $\\pm$ 0.04                           | **0.79 $\\pm$ 0.02** |\n| EleutherAI-gpt-neo-125M            | 0.63 $\\pm$ 0.04 | 0.67 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | 0.71 $\\pm$ 0.03          | 0.74 $\\pm$ 0.04                           | 0.77 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.70 $\\pm$ 0.03 | 0.74 $\\pm$ 0.03             | 0.76 $\\pm$ 0.02         | 0.74 $\\pm$ 0.04          | 0.76 $\\pm$ 0.04                           | 0.76 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.68 $\\pm$ 0.05 | 0.74 $\\pm$ 0.03             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.03          | 0.76 $\\pm$ 0.02                           | 0.76 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.73 $\\pm$ 0.01 | 0.76 $\\pm$ 0.01             | 0.74 $\\pm$ 0.03         | 0.76 $\\pm$ 0.01          | 0.78 $\\pm$ 0.02                           | 0.76 $\\pm$ 0.02     |\n| gpt2                               | 0.67 $\\pm$ 0.06 | 0.72 $\\pm$ 0.02             | 0.70 $\\pm$ 0.03         | 0.67 $\\pm$ 0.03          | 0.72 $\\pm$ 0.06                           | 0.74 $\\pm$ 0.03     |\n| xlm-roberta-large                  | 0.73 $\\pm$ 0.03 | 0.75 $\\pm$ 0.03             | 0.75 $\\pm$ 0.01         | 0.76 $\\pm$ 0.03          | 0.74 $\\pm$ 0.01                           | 0.76 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.73 $\\pm$ 0.00 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.76 $\\pm$ 0.00          | 0.76 $\\pm$ 0.02                           | 0.77 $\\pm$ 0.00 |\n| EleutherAI-gpt-neo-125M            | 0.69 $\\pm$ 0.01 | 0.72 $\\pm$ 0.01             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.00          | 0.74 $\\pm$ 0.01                           | 0.76 $\\pm$ 0.01 |\n| bert-base-multilingual-cased       | 0.74 $\\pm$ 0.01 | 0.77 $\\pm$ 0.01             | 0.78 $\\pm$ 0.01         | 0.78 $\\pm$ 0.01          | 0.78 $\\pm$ 0.02                           | 0.78 $\\pm$ 0.01 |\n| distilbert-base-multilingual-cased | 0.73 $\\pm$ 0.02 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.77 $\\pm$ 0.02          | 0.76 $\\pm$ 0.00                           | 0.77 $\\pm$ 0.01 |\n| facebook-mbart-large-50            | 0.76 $\\pm$ 0.01 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | 0.78 $\\pm$ 0.01 |\n| gpt2                               | 0.74 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.77 $\\pm$ 0.00         | 0.77 $\\pm$ 0.02          | 0.77 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.76 $\\pm$ 0.00 | **0.79 $\\pm$ 0.00**         | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | 0.78 $\\pm$ 0.01 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.08 $\\pm$ 0.02 | 0.12 $\\pm$ 0.02             | 0.08 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.10 $\\pm$ 0.02                           | **0.13 $\\pm$ 0.01** |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.05 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.01          | 0.08 $\\pm$ 0.01                           | 0.09 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.07 $\\pm$ 0.01 | 0.10 $\\pm$ 0.03             | **0.13 $\\pm$ 0.03**     | 0.09 $\\pm$ 0.00          | 0.10 $\\pm$ 0.04                           | 0.11 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.06 $\\pm$ 0.01 | 0.09 $\\pm$ 0.02             | 0.10 $\\pm$ 0.02         | 0.09 $\\pm$ 0.01          | 0.08 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.07 $\\pm$ 0.04 | 0.10 $\\pm$ 0.03             | 0.11 $\\pm$ 0.03         | 0.12 $\\pm$ 0.03          | 0.12 $\\pm$ 0.02                           | 0.11 $\\pm$ 0.01     |\n| gpt2                               | 0.06 $\\pm$ 0.01 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.06 $\\pm$ 0.03          | 0.08 $\\pm$ 0.00                           | 0.10 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.10 $\\pm$ 0.01 | 0.12 $\\pm$ 0.01             | 0.10 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | 0.11 $\\pm$ 0.01     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# French"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.04 | 0.47 $\\pm$ 0.01             | 0.50 $\\pm$ 0.04         | 0.50 $\\pm$ 0.01          | 0.52 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.31 $\\pm$ 0.01 | 0.38 $\\pm$ 0.03             | 0.39 $\\pm$ 0.03         | 0.39 $\\pm$ 0.01          | 0.42 $\\pm$ 0.04                           | 0.44 $\\pm$ 0.05     |\n| bert-base-multilingual-cased       | 0.47 $\\pm$ 0.04 | 0.52 $\\pm$ 0.03             | 0.53 $\\pm$ 0.02         | 0.55 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.05 | 0.50 $\\pm$ 0.02             | 0.53 $\\pm$ 0.03         | 0.53 $\\pm$ 0.02          | 0.52 $\\pm$ 0.02                           | 0.52 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.53 $\\pm$ 0.02             | 0.56 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | **0.58 $\\pm$ 0.02**                       | **0.58 $\\pm$ 0.03** |\n| gpt2                               | 0.40 $\\pm$ 0.07 | 0.43 $\\pm$ 0.02             | 0.47 $\\pm$ 0.02         | 0.49 $\\pm$ 0.02          | 0.48 $\\pm$ 0.04                           | 0.50 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.51 $\\pm$ 0.02 | 0.56 $\\pm$ 0.03             | 0.57 $\\pm$ 0.01         | **0.58 $\\pm$ 0.03**      | 0.57 $\\pm$ 0.01                           | 0.55 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.32 $\\pm$ 0.05 | 0.37 $\\pm$ 0.01             | 0.45 $\\pm$ 0.04         | 0.42 $\\pm$ 0.03          | 0.43 $\\pm$ 0.05                           | 0.46 $\\pm$ 0.06 |\n| EleutherAI-gpt-neo-125M            | 0.24 $\\pm$ 0.01 | 0.32 $\\pm$ 0.04             | 0.32 $\\pm$ 0.04         | 0.31 $\\pm$ 0.03          | 0.35 $\\pm$ 0.04                           | 0.33 $\\pm$ 0.07 |\n| bert-base-multilingual-cased       | 0.38 $\\pm$ 0.04 | 0.46 $\\pm$ 0.03             | 0.49 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.50 $\\pm$ 0.05                           | 0.50 $\\pm$ 0.03 |\n| distilbert-base-multilingual-cased | 0.37 $\\pm$ 0.06 | 0.44 $\\pm$ 0.02             | 0.48 $\\pm$ 0.04         | 0.48 $\\pm$ 0.02          | 0.44 $\\pm$ 0.04                           | 0.45 $\\pm$ 0.04 |\n| facebook-mbart-large-50            | 0.44 $\\pm$ 0.03 | 0.47 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.52 $\\pm$ 0.03          | 0.52 $\\pm$ 0.03                           | 0.51 $\\pm$ 0.06 |\n| gpt2                               | 0.36 $\\pm$ 0.06 | 0.36 $\\pm$ 0.02             | 0.43 $\\pm$ 0.02         | 0.43 $\\pm$ 0.02          | 0.42 $\\pm$ 0.06                           | 0.44 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.44 $\\pm$ 0.02 | 0.51 $\\pm$ 0.02             | 0.52 $\\pm$ 0.04         | **0.55 $\\pm$ 0.04**      | 0.54 $\\pm$ 0.02                           | 0.49 $\\pm$ 0.07 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.03 | 0.63 $\\pm$ 0.04             | 0.57 $\\pm$ 0.04         | 0.62 $\\pm$ 0.06          | 0.65 $\\pm$ 0.05                           | 0.66 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.42 $\\pm$ 0.05 | 0.48 $\\pm$ 0.03             | 0.51 $\\pm$ 0.02         | 0.55 $\\pm$ 0.06          | 0.52 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.59 $\\pm$ 0.05 | 0.60 $\\pm$ 0.02             | 0.59 $\\pm$ 0.03         | 0.61 $\\pm$ 0.05          | 0.63 $\\pm$ 0.01                           | 0.67 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.53 $\\pm$ 0.05 | 0.59 $\\pm$ 0.05             | 0.59 $\\pm$ 0.03         | 0.59 $\\pm$ 0.03          | 0.63 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.60 $\\pm$ 0.06 | 0.62 $\\pm$ 0.01             | 0.62 $\\pm$ 0.04         | 0.63 $\\pm$ 0.02          | 0.65 $\\pm$ 0.02                           | **0.68 $\\pm$ 0.04** |\n| gpt2                               | 0.45 $\\pm$ 0.10 | 0.51 $\\pm$ 0.04             | 0.53 $\\pm$ 0.02         | 0.57 $\\pm$ 0.04          | 0.58 $\\pm$ 0.03                           | 0.60 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.62 $\\pm$ 0.01          | 0.61 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.68 $\\pm$ 0.00          | 0.68 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.03 |\n| EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.01         | 0.62 $\\pm$ 0.01          | 0.63 $\\pm$ 0.02                           | 0.64 $\\pm$ 0.03 |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.70 $\\pm$ 0.02         | 0.71 $\\pm$ 0.01          | 0.71 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.01 |\n| distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02 |\n| facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.00         | 0.72 $\\pm$ 0.02          | 0.72 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.02 |\n| gpt2                               | 0.62 $\\pm$ 0.04 | 0.64 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.00 |\n| xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.01         | **0.73 $\\pm$ 0.02**      | 0.72 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.03 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.02 | 0.07 $\\pm$ 0.01             | 0.06 $\\pm$ 0.03         | 0.08 $\\pm$ 0.02          | 0.09 $\\pm$ 0.04                           | 0.09 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.01 $\\pm$ 0.01             | 0.03 $\\pm$ 0.01         | 0.04 $\\pm$ 0.01          | 0.04 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.05     |\n| bert-base-multilingual-cased       | 0.07 $\\pm$ 0.02 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.09 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | **0.11 $\\pm$ 0.02** |\n| distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.06 $\\pm$ 0.04          | 0.09 $\\pm$ 0.04                           | 0.09 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.08 $\\pm$ 0.04 | **0.11 $\\pm$ 0.02**         | 0.09 $\\pm$ 0.01         | 0.10 $\\pm$ 0.03          | **0.11 $\\pm$ 0.03**                       | **0.11 $\\pm$ 0.02** |\n| gpt2                               | 0.03 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.04         | 0.07 $\\pm$ 0.02          | 0.06 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.05     |\n| xlm-roberta-large                  | 0.07 $\\pm$ 0.03 | 0.07 $\\pm$ 0.04             | 0.10 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | **0.11 $\\pm$ 0.05**                       | 0.09 $\\pm$ 0.05     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# German"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.00 | 0.56 $\\pm$ 0.02             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.00          | 0.57 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.01 | 0.46 $\\pm$ 0.01             | 0.48 $\\pm$ 0.00         | 0.50 $\\pm$ 0.02          | 0.51 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.52 $\\pm$ 0.03 | 0.58 $\\pm$ 0.02             | 0.59 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.59 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.50 $\\pm$ 0.02 | 0.56 $\\pm$ 0.00             | 0.55 $\\pm$ 0.03         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.02             | 0.61 $\\pm$ 0.02         | 0.63 $\\pm$ 0.00          | **0.64 $\\pm$ 0.01**                       | **0.64 $\\pm$ 0.03** |\n| gpt2                               | 0.47 $\\pm$ 0.03 | 0.52 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.55 $\\pm$ 0.03          | 0.53 $\\pm$ 0.01                           | 0.56 $\\pm$ 0.00     |\n| xlm-roberta-large                  | 0.55 $\\pm$ 0.02 | 0.61 $\\pm$ 0.00             | 0.62 $\\pm$ 0.03         | **0.64 $\\pm$ 0.01**      | **0.64 $\\pm$ 0.02**                       | **0.64 $\\pm$ 0.02** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.03 | 0.49 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.49 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.03 |\n| EleutherAI-gpt-neo-125M            | 0.33 $\\pm$ 0.03 | 0.41 $\\pm$ 0.02             | 0.41 $\\pm$ 0.01         | 0.43 $\\pm$ 0.02          | 0.44 $\\pm$ 0.03                           | 0.45 $\\pm$ 0.02 |\n| bert-base-multilingual-cased       | 0.46 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.54 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.54 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.05 |\n| distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.01 | 0.51 $\\pm$ 0.02             | 0.49 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.52 $\\pm$ 0.01                           | 0.55 $\\pm$ 0.04 |\n| facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.55 $\\pm$ 0.02             | 0.56 $\\pm$ 0.03         | 0.59 $\\pm$ 0.01          | 0.59 $\\pm$ 0.00                           | 0.58 $\\pm$ 0.05 |\n| gpt2                               | 0.46 $\\pm$ 0.05 | 0.49 $\\pm$ 0.05             | 0.50 $\\pm$ 0.05         | 0.52 $\\pm$ 0.05          | 0.51 $\\pm$ 0.03                           | 0.50 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.50 $\\pm$ 0.02 | 0.57 $\\pm$ 0.02             | 0.57 $\\pm$ 0.02         | 0.58 $\\pm$ 0.01          | **0.62 $\\pm$ 0.07**                       | 0.61 $\\pm$ 0.03 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.57 $\\pm$ 0.05 | 0.65 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.70 $\\pm$ 0.06          | 0.66 $\\pm$ 0.03                           | **0.71 $\\pm$ 0.02** |\n| EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.02 | 0.51 $\\pm$ 0.04             | 0.57 $\\pm$ 0.01         | 0.58 $\\pm$ 0.02          | 0.60 $\\pm$ 0.05                           | 0.65 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.60 $\\pm$ 0.02 | 0.62 $\\pm$ 0.02             | 0.66 $\\pm$ 0.03         | 0.68 $\\pm$ 0.06          | 0.66 $\\pm$ 0.01                           | 0.67 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.63 $\\pm$ 0.02             | 0.63 $\\pm$ 0.04         | 0.64 $\\pm$ 0.05          | 0.66 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.62 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.68 $\\pm$ 0.00         | 0.68 $\\pm$ 0.02          | 0.70 $\\pm$ 0.03                           | **0.71 $\\pm$ 0.01** |\n| gpt2                               | 0.49 $\\pm$ 0.01 | 0.56 $\\pm$ 0.03             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.00          | 0.56 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.62 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.04         | **0.71 $\\pm$ 0.01**      | 0.68 $\\pm$ 0.04                           | 0.67 $\\pm$ 0.07     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.64 $\\pm$ 0.00 | 0.69 $\\pm$ 0.02             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.00          | 0.69 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.01             | 0.64 $\\pm$ 0.00         | 0.65 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.67 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.01         | 0.73 $\\pm$ 0.01          | 0.71 $\\pm$ 0.00                           | 0.72 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.69 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.00                           | 0.71 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.01          | **0.74 $\\pm$ 0.01**                       | **0.74 $\\pm$ 0.01** |\n| gpt2                               | 0.62 $\\pm$ 0.02 | 0.66 $\\pm$ 0.03             | 0.66 $\\pm$ 0.03         | 0.67 $\\pm$ 0.02          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.72 $\\pm$ 0.00             | 0.72 $\\pm$ 0.02         | **0.74 $\\pm$ 0.01**      | **0.74 $\\pm$ 0.02**                       | 0.73 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.05 $\\pm$ 0.03         | 0.05 $\\pm$ 0.03          | 0.04 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.03 |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.03 $\\pm$ 0.02         | 0.02 $\\pm$ 0.01          | 0.04 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.02 |\n| bert-base-multilingual-cased       | 0.05 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.03         | 0.09 $\\pm$ 0.05          | 0.06 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.05 |\n| distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.02             | 0.05 $\\pm$ 0.04         | 0.05 $\\pm$ 0.03          | 0.05 $\\pm$ 0.04                           | 0.04 $\\pm$ 0.04 |\n| facebook-mbart-large-50            | 0.05 $\\pm$ 0.03 | 0.06 $\\pm$ 0.04             | 0.06 $\\pm$ 0.03         | 0.05 $\\pm$ 0.02          | **0.11 $\\pm$ 0.03**                       | 0.08 $\\pm$ 0.04 |\n| gpt2                               | 0.01 $\\pm$ 0.01 | 0.01 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.03          | 0.03 $\\pm$ 0.01                           | 0.02 $\\pm$ 0.01 |\n| xlm-roberta-large                  | 0.03 $\\pm$ 0.02 | 0.07 $\\pm$ 0.03             | 0.09 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | 0.07 $\\pm$ 0.04                           | 0.07 $\\pm$ 0.04 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Italian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.42 $\\pm$ 0.02 | 0.46 $\\pm$ 0.05             | 0.50 $\\pm$ 0.03         | 0.52 $\\pm$ 0.04          | 0.52 $\\pm$ 0.05                           | 0.56 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.04 | 0.42 $\\pm$ 0.02             | 0.44 $\\pm$ 0.02         | 0.44 $\\pm$ 0.01          | 0.47 $\\pm$ 0.05                           | 0.48 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.47 $\\pm$ 0.01 | 0.53 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.56 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.56 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.01 | 0.50 $\\pm$ 0.05             | 0.52 $\\pm$ 0.03         | 0.56 $\\pm$ 0.05          | 0.52 $\\pm$ 0.03                           | 0.54 $\\pm$ 0.02     |\n| facebook-mbart-large-50            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.04             | 0.55 $\\pm$ 0.02         | 0.57 $\\pm$ 0.03          | 0.56 $\\pm$ 0.04                           | **0.60 $\\pm$ 0.04** |\n| gpt2                               | 0.41 $\\pm$ 0.02 | 0.45 $\\pm$ 0.01             | 0.48 $\\pm$ 0.03         | 0.52 $\\pm$ 0.02          | 0.50 $\\pm$ 0.03                           | 0.53 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.50 $\\pm$ 0.04 | 0.54 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.59 $\\pm$ 0.03          | 0.56 $\\pm$ 0.01                           | 0.58 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.35 $\\pm$ 0.04 | 0.38 $\\pm$ 0.06             | 0.44 $\\pm$ 0.02         | 0.45 $\\pm$ 0.07          | 0.44 $\\pm$ 0.09                           | 0.47 $\\pm$ 0.01 |\n| EleutherAI-gpt-neo-125M            | 0.28 $\\pm$ 0.04 | 0.36 $\\pm$ 0.04             | 0.36 $\\pm$ 0.02         | 0.35 $\\pm$ 0.02          | 0.40 $\\pm$ 0.06                           | 0.37 $\\pm$ 0.01 |\n| bert-base-multilingual-cased       | 0.40 $\\pm$ 0.00 | 0.48 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | 0.51 $\\pm$ 0.03          | 0.51 $\\pm$ 0.02                           | 0.49 $\\pm$ 0.02 |\n| distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.01 | 0.44 $\\pm$ 0.04             | 0.46 $\\pm$ 0.03         | 0.50 $\\pm$ 0.06          | 0.45 $\\pm$ 0.04                           | 0.47 $\\pm$ 0.01 |\n| facebook-mbart-large-50            | 0.42 $\\pm$ 0.01 | 0.47 $\\pm$ 0.06             | 0.49 $\\pm$ 0.02         | 0.51 $\\pm$ 0.03          | 0.50 $\\pm$ 0.03                           | 0.52 $\\pm$ 0.05 |\n| gpt2                               | 0.37 $\\pm$ 0.03 | 0.40 $\\pm$ 0.01             | 0.45 $\\pm$ 0.03         | 0.49 $\\pm$ 0.03          | 0.46 $\\pm$ 0.01                           | 0.47 $\\pm$ 0.02 |\n| xlm-roberta-large                  | 0.43 $\\pm$ 0.05 | 0.49 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | **0.54 $\\pm$ 0.02**      | 0.52 $\\pm$ 0.02                           | 0.53 $\\pm$ 0.04 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.53 $\\pm$ 0.03 | 0.58 $\\pm$ 0.03             | 0.58 $\\pm$ 0.05         | 0.63 $\\pm$ 0.05          | 0.63 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.47 $\\pm$ 0.05 | 0.49 $\\pm$ 0.01             | 0.55 $\\pm$ 0.01         | 0.57 $\\pm$ 0.01          | 0.59 $\\pm$ 0.04                           | 0.67 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.57 $\\pm$ 0.02 | 0.59 $\\pm$ 0.04             | 0.61 $\\pm$ 0.04         | 0.62 $\\pm$ 0.05          | 0.62 $\\pm$ 0.07                           | 0.65 $\\pm$ 0.07     |\n| distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.02 | 0.57 $\\pm$ 0.06             | 0.58 $\\pm$ 0.03         | 0.62 $\\pm$ 0.03          | 0.60 $\\pm$ 0.04                           | 0.64 $\\pm$ 0.04     |\n| facebook-mbart-large-50            | 0.57 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.03         | 0.64 $\\pm$ 0.03          | 0.65 $\\pm$ 0.08                           | **0.70 $\\pm$ 0.05** |\n| gpt2                               | 0.47 $\\pm$ 0.01 | 0.52 $\\pm$ 0.02             | 0.52 $\\pm$ 0.03         | 0.55 $\\pm$ 0.01          | 0.55 $\\pm$ 0.06                           | 0.62 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.58 $\\pm$ 0.03 | 0.59 $\\pm$ 0.03             | 0.61 $\\pm$ 0.02         | 0.64 $\\pm$ 0.05          | 0.61 $\\pm$ 0.05                           | 0.63 $\\pm$ 0.05     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.61 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.66 $\\pm$ 0.02         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.01     |\n| EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.01     |\n| bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.69 $\\pm$ 0.03          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.65 $\\pm$ 0.00 | 0.68 $\\pm$ 0.03             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.02          | 0.70 $\\pm$ 0.03                           | **0.72 $\\pm$ 0.03** |\n| gpt2                               | 0.61 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.65 $\\pm$ 0.02         | 0.67 $\\pm$ 0.01          | 0.66 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.01         | 0.71 $\\pm$ 0.02          | 0.70 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.04             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.03          | 0.07 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.04 $\\pm$ 0.01             | 0.04 $\\pm$ 0.04         | 0.01 $\\pm$ 0.01          | 0.04 $\\pm$ 0.01                           | 0.08 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.06 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | 0.09 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.04 $\\pm$ 0.02 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.02         | 0.06 $\\pm$ 0.04          | 0.07 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.04             | 0.05 $\\pm$ 0.02         | 0.09 $\\pm$ 0.02          | 0.08 $\\pm$ 0.03                           | **0.11 $\\pm$ 0.02** |\n| gpt2                               | 0.02 $\\pm$ 0.02 | 0.02 $\\pm$ 0.02             | 0.04 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.03 $\\pm$ 0.00                           | 0.07 $\\pm$ 0.03     |\n| xlm-roberta-large                  | 0.06 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.08 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Polish"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.60 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.61 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.04 | 0.47 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.51 $\\pm$ 0.06          | 0.54 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.55 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.65 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.03 | 0.58 $\\pm$ 0.04             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.01          | 0.59 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.04 | 0.60 $\\pm$ 0.01             | 0.63 $\\pm$ 0.03         | **0.66 $\\pm$ 0.02**      | 0.64 $\\pm$ 0.02                           | **0.66 $\\pm$ 0.03** |\n| gpt2                               | 0.49 $\\pm$ 0.01 | 0.55 $\\pm$ 0.05             | 0.57 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.58 $\\pm$ 0.02                           | 0.59 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.59 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.65 $\\pm$ 0.03          | 0.65 $\\pm$ 0.02                           | **0.66 $\\pm$ 0.03** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.03 | 0.49 $\\pm$ 0.02             | 0.58 $\\pm$ 0.02         | 0.53 $\\pm$ 0.04          | 0.56 $\\pm$ 0.05                           | 0.58 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.34 $\\pm$ 0.05 | 0.42 $\\pm$ 0.04             | 0.50 $\\pm$ 0.02         | 0.44 $\\pm$ 0.06          | 0.49 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.50 $\\pm$ 0.04 | 0.56 $\\pm$ 0.04             | 0.56 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.57 $\\pm$ 0.04     |\n| distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.05         | 0.55 $\\pm$ 0.03          | 0.54 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.59 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.59 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.05     |\n| gpt2                               | 0.48 $\\pm$ 0.03 | 0.52 $\\pm$ 0.01             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.03          | 0.57 $\\pm$ 0.06                           | 0.57 $\\pm$ 0.04     |\n| xlm-roberta-large                  | 0.54 $\\pm$ 0.02 | 0.55 $\\pm$ 0.05             | 0.60 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.63 $\\pm$ 0.03                           | **0.69 $\\pm$ 0.06** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.10 | 0.63 $\\pm$ 0.05             | 0.63 $\\pm$ 0.06         | 0.64 $\\pm$ 0.06          | 0.69 $\\pm$ 0.04                           | 0.68 $\\pm$ 0.04     |\n| EleutherAI-gpt-neo-125M            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.03             | 0.60 $\\pm$ 0.06         | 0.61 $\\pm$ 0.07          | 0.60 $\\pm$ 0.05                           | 0.65 $\\pm$ 0.05     |\n| bert-base-multilingual-cased       | 0.61 $\\pm$ 0.07 | 0.63 $\\pm$ 0.02             | 0.67 $\\pm$ 0.05         | 0.69 $\\pm$ 0.04          | 0.68 $\\pm$ 0.05                           | 0.67 $\\pm$ 0.08     |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.64 $\\pm$ 0.06             | 0.68 $\\pm$ 0.07         | 0.67 $\\pm$ 0.08          | 0.67 $\\pm$ 0.09                           | 0.67 $\\pm$ 0.05     |\n| facebook-mbart-large-50            | 0.61 $\\pm$ 0.06 | 0.67 $\\pm$ 0.04             | 0.67 $\\pm$ 0.07         | 0.70 $\\pm$ 0.02          | 0.69 $\\pm$ 0.05                           | **0.73 $\\pm$ 0.01** |\n| gpt2                               | 0.51 $\\pm$ 0.07 | 0.59 $\\pm$ 0.10             | 0.59 $\\pm$ 0.05         | 0.58 $\\pm$ 0.06          | 0.61 $\\pm$ 0.06                           | 0.62 $\\pm$ 0.06     |\n| xlm-roberta-large                  | 0.62 $\\pm$ 0.11 | 0.64 $\\pm$ 0.03             | 0.67 $\\pm$ 0.04         | 0.68 $\\pm$ 0.05          | 0.68 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.03     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.69 $\\pm$ 0.01         | 0.69 $\\pm$ 0.02          | 0.71 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.64 $\\pm$ 0.03          | 0.66 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.69 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.72 $\\pm$ 0.01          | 0.73 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.72 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.00             | 0.72 $\\pm$ 0.01         | **0.74 $\\pm$ 0.01**      | 0.72 $\\pm$ 0.01                           | **0.74 $\\pm$ 0.01** |\n| gpt2                               | 0.61 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.68 $\\pm$ 0.01          | 0.68 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.68 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | **0.74 $\\pm$ 0.01** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.01 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.02     |\n| EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.02 $\\pm$ 0.00         | 0.02 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.04 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.06 $\\pm$ 0.02          | 0.07 $\\pm$ 0.04                           | 0.06 $\\pm$ 0.02     |\n| distilbert-base-multilingual-cased | 0.03 $\\pm$ 0.02 | 0.04 $\\pm$ 0.01             | 0.05 $\\pm$ 0.03         | 0.04 $\\pm$ 0.01          | 0.05 $\\pm$ 0.02                           | 0.04 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.03          | 0.05 $\\pm$ 0.03                           | **0.11 $\\pm$ 0.03** |\n| gpt2                               | 0.00 $\\pm$ 0.00 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.03         | 0.04 $\\pm$ 0.06          | 0.02 $\\pm$ 0.02                           | 0.03 $\\pm$ 0.02     |\n| xlm-roberta-large                  | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.07 $\\pm$ 0.04          | 0.06 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Russian"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.21 $\\pm$ 0.02 | 0.25 $\\pm$ 0.03             | 0.38 $\\pm$ 0.03         | 0.37 $\\pm$ 0.04          | 0.40 $\\pm$ 0.03                           | 0.39 $\\pm$ 0.05     |\n| EleutherAI-gpt-neo-125M            | 0.22 $\\pm$ 0.01 | 0.22 $\\pm$ 0.04             | 0.21 $\\pm$ 0.02         | 0.22 $\\pm$ 0.02          | 0.20 $\\pm$ 0.04                           | 0.18 $\\pm$ 0.03     |\n| bert-base-multilingual-cased       | 0.39 $\\pm$ 0.01 | 0.45 $\\pm$ 0.02             | 0.50 $\\pm$ 0.05         | 0.51 $\\pm$ 0.06          | 0.51 $\\pm$ 0.03                           | 0.51 $\\pm$ 0.03     |\n| distilbert-base-multilingual-cased | 0.31 $\\pm$ 0.02 | 0.41 $\\pm$ 0.01             | 0.44 $\\pm$ 0.03         | 0.46 $\\pm$ 0.06          | 0.47 $\\pm$ 0.04                           | 0.44 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.40 $\\pm$ 0.04 | 0.49 $\\pm$ 0.03             | 0.51 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.51 $\\pm$ 0.01                           | **0.55 $\\pm$ 0.02** |\n| gpt2                               | 0.16 $\\pm$ 0.08 | 0.13 $\\pm$ 0.06             | 0.07 $\\pm$ 0.07         | 0.14 $\\pm$ 0.08          | 0.16 $\\pm$ 0.05                           | 0.14 $\\pm$ 0.10     |\n| xlm-roberta-large                  | 0.47 $\\pm$ 0.03 | 0.53 $\\pm$ 0.04             | 0.53 $\\pm$ 0.03         | **0.55 $\\pm$ 0.01**      | 0.52 $\\pm$ 0.03                           | 0.53 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.14 $\\pm$ 0.01 | 0.16 $\\pm$ 0.02             | 0.31 $\\pm$ 0.05         | 0.29 $\\pm$ 0.05          | 0.31 $\\pm$ 0.02                           | 0.32 $\\pm$ 0.07 |\n| EleutherAI-gpt-neo-125M            | 0.16 $\\pm$ 0.01 | 0.17 $\\pm$ 0.03             | 0.15 $\\pm$ 0.02         | 0.16 $\\pm$ 0.01          | 0.14 $\\pm$ 0.02                           | 0.12 $\\pm$ 0.04 |\n| bert-base-multilingual-cased       | 0.31 $\\pm$ 0.03 | 0.40 $\\pm$ 0.05             | 0.44 $\\pm$ 0.05         | 0.45 $\\pm$ 0.10          | 0.44 $\\pm$ 0.04                           | 0.45 $\\pm$ 0.04 |\n| distilbert-base-multilingual-cased | 0.23 $\\pm$ 0.01 | 0.33 $\\pm$ 0.02             | 0.35 $\\pm$ 0.04         | 0.39 $\\pm$ 0.06          | 0.39 $\\pm$ 0.06                           | 0.36 $\\pm$ 0.02 |\n| facebook-mbart-large-50            | 0.31 $\\pm$ 0.04 | 0.42 $\\pm$ 0.02             | 0.44 $\\pm$ 0.05         | 0.47 $\\pm$ 0.06          | 0.44 $\\pm$ 0.03                           | 0.47 $\\pm$ 0.01 |\n| gpt2                               | 0.11 $\\pm$ 0.06 | 0.09 $\\pm$ 0.04             | 0.04 $\\pm$ 0.05         | 0.09 $\\pm$ 0.06          | 0.10 $\\pm$ 0.04                           | 0.09 $\\pm$ 0.07 |\n| xlm-roberta-large                  | 0.40 $\\pm$ 0.03 | 0.47 $\\pm$ 0.05             | 0.48 $\\pm$ 0.06         | **0.52 $\\pm$ 0.03**      | 0.46 $\\pm$ 0.05                           | 0.47 $\\pm$ 0.07 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.06 | 0.59 $\\pm$ 0.14             | 0.50 $\\pm$ 0.04         | 0.52 $\\pm$ 0.09          | 0.59 $\\pm$ 0.15                           | 0.52 $\\pm$ 0.07     |\n| EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.03 | 0.32 $\\pm$ 0.08             | 0.36 $\\pm$ 0.05         | 0.37 $\\pm$ 0.03          | 0.36 $\\pm$ 0.11                           | 0.39 $\\pm$ 0.12     |\n| bert-base-multilingual-cased       | 0.53 $\\pm$ 0.02 | 0.53 $\\pm$ 0.05             | 0.59 $\\pm$ 0.04         | 0.60 $\\pm$ 0.02          | 0.60 $\\pm$ 0.04                           | 0.59 $\\pm$ 0.01     |\n| distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.01         | 0.57 $\\pm$ 0.04          | 0.61 $\\pm$ 0.02                           | 0.57 $\\pm$ 0.01     |\n| facebook-mbart-large-50            | 0.55 $\\pm$ 0.06 | 0.60 $\\pm$ 0.03             | 0.61 $\\pm$ 0.04         | 0.63 $\\pm$ 0.04          | 0.62 $\\pm$ 0.02                           | **0.67 $\\pm$ 0.07** |\n| gpt2                               | 0.30 $\\pm$ 0.03 | 0.26 $\\pm$ 0.10             | 0.25 $\\pm$ 0.04         | 0.37 $\\pm$ 0.02          | 0.41 $\\pm$ 0.02                           | 0.47 $\\pm$ 0.12     |\n| xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.61 $\\pm$ 0.01             | 0.61 $\\pm$ 0.06         | 0.58 $\\pm$ 0.03          | 0.59 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.07     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text        |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:----------------|\n| EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.57 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.63 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.03 |\n| EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.01 | 0.55 $\\pm$ 0.02             | 0.55 $\\pm$ 0.01         | 0.55 $\\pm$ 0.01          | 0.54 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.01 |\n| bert-base-multilingual-cased       | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.69 $\\pm$ 0.03         | 0.69 $\\pm$ 0.04          | 0.69 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.01 |\n| distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.01             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.03          | 0.67 $\\pm$ 0.02                           | 0.65 $\\pm$ 0.01 |\n| facebook-mbart-large-50            | 0.63 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.71 $\\pm$ 0.03          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.00 |\n| gpt2                               | 0.53 $\\pm$ 0.02 | 0.52 $\\pm$ 0.02             | 0.51 $\\pm$ 0.01         | 0.53 $\\pm$ 0.02          | 0.53 $\\pm$ 0.01                           | 0.53 $\\pm$ 0.02 |\n| xlm-roberta-large                  | 0.67 $\\pm$ 0.01 | 0.70 $\\pm$ 0.03             | 0.71 $\\pm$ 0.02         | **0.72 $\\pm$ 0.01**      | 0.70 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.02 |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.03 | 0.06 $\\pm$ 0.03             | 0.10 $\\pm$ 0.01         | 0.08 $\\pm$ 0.03          | 0.13 $\\pm$ 0.05                           | 0.08 $\\pm$ 0.03     |\n| EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.03 | 0.04 $\\pm$ 0.05             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.04          | 0.03 $\\pm$ 0.03                           | 0.04 $\\pm$ 0.04     |\n| bert-base-multilingual-cased       | 0.09 $\\pm$ 0.03 | 0.12 $\\pm$ 0.04             | **0.17 $\\pm$ 0.03**     | 0.15 $\\pm$ 0.08          | **0.17 $\\pm$ 0.03**                       | 0.13 $\\pm$ 0.06     |\n| distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.01 | 0.11 $\\pm$ 0.06             | 0.08 $\\pm$ 0.03         | 0.13 $\\pm$ 0.03          | 0.13 $\\pm$ 0.07                           | 0.12 $\\pm$ 0.03     |\n| facebook-mbart-large-50            | 0.11 $\\pm$ 0.03 | 0.16 $\\pm$ 0.03             | 0.14 $\\pm$ 0.03         | **0.17 $\\pm$ 0.05**      | 0.15 $\\pm$ 0.03                           | 0.14 $\\pm$ 0.02     |\n| gpt2                               | 0.02 $\\pm$ 0.02 | 0.03 $\\pm$ 0.02             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.00          | 0.02 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.01     |\n| xlm-roberta-large                  | 0.12 $\\pm$ 0.05 | **0.17 $\\pm$ 0.04**         | 0.14 $\\pm$ 0.06         | 0.14 $\\pm$ 0.08          | 0.13 $\\pm$ 0.02                           | **0.17 $\\pm$ 0.06** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:30: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  report_table.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# All 6 Languages"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## f1-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.62 $\\pm$ 0.01 | 0.67 $\\pm$ 0.01             | 0.68 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.00     |\n| en         | EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.03 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.01          | 0.64 $\\pm$ 0.01                           | 0.66 $\\pm$ 0.01     |\n| en         | bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.69 $\\pm$ 0.02          | 0.69 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.61 $\\pm$ 0.03 | 0.66 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.68 $\\pm$ 0.03          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.02     |\n| en         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | **0.71 $\\pm$ 0.02**                       | 0.69 $\\pm$ 0.02     |\n| en         | gpt2                               | 0.63 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.67 $\\pm$ 0.01         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| en         | xlm-roberta-large                  | 0.66 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | **0.71 $\\pm$ 0.02**      | 0.70 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.04 | 0.47 $\\pm$ 0.01             | 0.50 $\\pm$ 0.04         | 0.50 $\\pm$ 0.01          | 0.52 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.05     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.31 $\\pm$ 0.01 | 0.38 $\\pm$ 0.03             | 0.39 $\\pm$ 0.03         | 0.39 $\\pm$ 0.01          | 0.42 $\\pm$ 0.04                           | 0.44 $\\pm$ 0.05     |\n| fr         | bert-base-multilingual-cased       | 0.47 $\\pm$ 0.04 | 0.52 $\\pm$ 0.03             | 0.53 $\\pm$ 0.02         | 0.55 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.02     |\n| fr         | distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.05 | 0.50 $\\pm$ 0.02             | 0.53 $\\pm$ 0.03         | 0.53 $\\pm$ 0.02          | 0.52 $\\pm$ 0.02                           | 0.52 $\\pm$ 0.03     |\n| fr         | facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.53 $\\pm$ 0.02             | 0.56 $\\pm$ 0.01         | 0.57 $\\pm$ 0.02          | **0.58 $\\pm$ 0.02**                       | **0.58 $\\pm$ 0.03** |\n| fr         | gpt2                               | 0.40 $\\pm$ 0.07 | 0.43 $\\pm$ 0.02             | 0.47 $\\pm$ 0.02         | 0.49 $\\pm$ 0.02          | 0.48 $\\pm$ 0.04                           | 0.50 $\\pm$ 0.01     |\n| fr         | xlm-roberta-large                  | 0.51 $\\pm$ 0.02 | 0.56 $\\pm$ 0.03             | 0.57 $\\pm$ 0.01         | **0.58 $\\pm$ 0.03**      | 0.57 $\\pm$ 0.01                           | 0.55 $\\pm$ 0.03     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.00 | 0.56 $\\pm$ 0.02             | 0.56 $\\pm$ 0.02         | 0.58 $\\pm$ 0.00          | 0.57 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.02     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.01 | 0.46 $\\pm$ 0.01             | 0.48 $\\pm$ 0.00         | 0.50 $\\pm$ 0.02          | 0.51 $\\pm$ 0.04                           | 0.53 $\\pm$ 0.02     |\n| ge         | bert-base-multilingual-cased       | 0.52 $\\pm$ 0.03 | 0.58 $\\pm$ 0.02             | 0.59 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.59 $\\pm$ 0.01                           | 0.62 $\\pm$ 0.03     |\n| ge         | distilbert-base-multilingual-cased | 0.50 $\\pm$ 0.02 | 0.56 $\\pm$ 0.00             | 0.55 $\\pm$ 0.03         | 0.59 $\\pm$ 0.02          | 0.58 $\\pm$ 0.01                           | 0.60 $\\pm$ 0.02     |\n| ge         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.02             | 0.61 $\\pm$ 0.02         | 0.63 $\\pm$ 0.00          | **0.64 $\\pm$ 0.01**                       | **0.64 $\\pm$ 0.03** |\n| ge         | gpt2                               | 0.47 $\\pm$ 0.03 | 0.52 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.55 $\\pm$ 0.03          | 0.53 $\\pm$ 0.01                           | 0.56 $\\pm$ 0.00     |\n| ge         | xlm-roberta-large                  | 0.55 $\\pm$ 0.02 | 0.61 $\\pm$ 0.00             | 0.62 $\\pm$ 0.03         | **0.64 $\\pm$ 0.01**      | **0.64 $\\pm$ 0.02**                       | **0.64 $\\pm$ 0.02** |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.42 $\\pm$ 0.02 | 0.46 $\\pm$ 0.05             | 0.50 $\\pm$ 0.03         | 0.52 $\\pm$ 0.04          | 0.52 $\\pm$ 0.05                           | 0.56 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.04 | 0.42 $\\pm$ 0.02             | 0.44 $\\pm$ 0.02         | 0.44 $\\pm$ 0.01          | 0.47 $\\pm$ 0.05                           | 0.48 $\\pm$ 0.02     |\n| it         | bert-base-multilingual-cased       | 0.47 $\\pm$ 0.01 | 0.53 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.56 $\\pm$ 0.02          | 0.56 $\\pm$ 0.03                           | 0.56 $\\pm$ 0.03     |\n| it         | distilbert-base-multilingual-cased | 0.45 $\\pm$ 0.01 | 0.50 $\\pm$ 0.05             | 0.52 $\\pm$ 0.03         | 0.56 $\\pm$ 0.05          | 0.52 $\\pm$ 0.03                           | 0.54 $\\pm$ 0.02     |\n| it         | facebook-mbart-large-50            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.04             | 0.55 $\\pm$ 0.02         | 0.57 $\\pm$ 0.03          | 0.56 $\\pm$ 0.04                           | **0.60 $\\pm$ 0.04** |\n| it         | gpt2                               | 0.41 $\\pm$ 0.02 | 0.45 $\\pm$ 0.01             | 0.48 $\\pm$ 0.03         | 0.52 $\\pm$ 0.02          | 0.50 $\\pm$ 0.03                           | 0.53 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.50 $\\pm$ 0.04 | 0.54 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.59 $\\pm$ 0.03          | 0.56 $\\pm$ 0.01                           | 0.58 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.60 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.61 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.04     |\n| po         | EleutherAI-gpt-neo-125M            | 0.39 $\\pm$ 0.04 | 0.47 $\\pm$ 0.02             | 0.55 $\\pm$ 0.02         | 0.51 $\\pm$ 0.06          | 0.54 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.03     |\n| po         | bert-base-multilingual-cased       | 0.55 $\\pm$ 0.02 | 0.59 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.65 $\\pm$ 0.02                           | 0.62 $\\pm$ 0.01     |\n| po         | distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.03 | 0.58 $\\pm$ 0.04             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.01          | 0.59 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.04 | 0.60 $\\pm$ 0.01             | 0.63 $\\pm$ 0.03         | **0.66 $\\pm$ 0.02**      | 0.64 $\\pm$ 0.02                           | **0.66 $\\pm$ 0.03** |\n| po         | gpt2                               | 0.49 $\\pm$ 0.01 | 0.55 $\\pm$ 0.05             | 0.57 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.58 $\\pm$ 0.02                           | 0.59 $\\pm$ 0.04     |\n| po         | xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.59 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.65 $\\pm$ 0.03          | 0.65 $\\pm$ 0.02                           | **0.66 $\\pm$ 0.03** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.21 $\\pm$ 0.02 | 0.25 $\\pm$ 0.03             | 0.38 $\\pm$ 0.03         | 0.37 $\\pm$ 0.04          | 0.40 $\\pm$ 0.03                           | 0.39 $\\pm$ 0.05     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.22 $\\pm$ 0.01 | 0.22 $\\pm$ 0.04             | 0.21 $\\pm$ 0.02         | 0.22 $\\pm$ 0.02          | 0.20 $\\pm$ 0.04                           | 0.18 $\\pm$ 0.03     |\n| ru         | bert-base-multilingual-cased       | 0.39 $\\pm$ 0.01 | 0.45 $\\pm$ 0.02             | 0.50 $\\pm$ 0.05         | 0.51 $\\pm$ 0.06          | 0.51 $\\pm$ 0.03                           | 0.51 $\\pm$ 0.03     |\n| ru         | distilbert-base-multilingual-cased | 0.31 $\\pm$ 0.02 | 0.41 $\\pm$ 0.01             | 0.44 $\\pm$ 0.03         | 0.46 $\\pm$ 0.06          | 0.47 $\\pm$ 0.04                           | 0.44 $\\pm$ 0.01     |\n| ru         | facebook-mbart-large-50            | 0.40 $\\pm$ 0.04 | 0.49 $\\pm$ 0.03             | 0.51 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.51 $\\pm$ 0.01                           | **0.55 $\\pm$ 0.02** |\n| ru         | gpt2                               | 0.16 $\\pm$ 0.08 | 0.13 $\\pm$ 0.06             | 0.07 $\\pm$ 0.07         | 0.14 $\\pm$ 0.08          | 0.16 $\\pm$ 0.05                           | 0.14 $\\pm$ 0.10     |\n| ru         | xlm-roberta-large                  | 0.47 $\\pm$ 0.03 | 0.53 $\\pm$ 0.04             | 0.53 $\\pm$ 0.03         | **0.55 $\\pm$ 0.01**      | 0.52 $\\pm$ 0.03                           | 0.53 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## recall-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.60 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.60 $\\pm$ 0.02          | 0.59 $\\pm$ 0.03                           | 0.61 $\\pm$ 0.01     |\n| en         | EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.01             | 0.55 $\\pm$ 0.03         | 0.55 $\\pm$ 0.00          | 0.56 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.03     |\n| en         | bert-base-multilingual-cased       | 0.58 $\\pm$ 0.01 | 0.63 $\\pm$ 0.03             | 0.64 $\\pm$ 0.02         | 0.65 $\\pm$ 0.01          | 0.63 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.03     |\n| en         | distilbert-base-multilingual-cased | 0.56 $\\pm$ 0.03 | 0.59 $\\pm$ 0.02             | 0.62 $\\pm$ 0.02         | 0.63 $\\pm$ 0.03          | 0.59 $\\pm$ 0.00                           | 0.61 $\\pm$ 0.01     |\n| en         | facebook-mbart-large-50            | 0.60 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.66 $\\pm$ 0.02         | 0.66 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.63 $\\pm$ 0.02     |\n| en         | gpt2                               | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.65 $\\pm$ 0.02         | **0.67 $\\pm$ 0.04**      | 0.63 $\\pm$ 0.04                           | 0.63 $\\pm$ 0.03     |\n| en         | xlm-roberta-large                  | 0.60 $\\pm$ 0.01 | 0.66 $\\pm$ 0.01             | 0.66 $\\pm$ 0.02         | **0.67 $\\pm$ 0.01**      | 0.66 $\\pm$ 0.02                           | 0.65 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.32 $\\pm$ 0.05 | 0.37 $\\pm$ 0.01             | 0.45 $\\pm$ 0.04         | 0.42 $\\pm$ 0.03          | 0.43 $\\pm$ 0.05                           | 0.46 $\\pm$ 0.06     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.24 $\\pm$ 0.01 | 0.32 $\\pm$ 0.04             | 0.32 $\\pm$ 0.04         | 0.31 $\\pm$ 0.03          | 0.35 $\\pm$ 0.04                           | 0.33 $\\pm$ 0.07     |\n| fr         | bert-base-multilingual-cased       | 0.38 $\\pm$ 0.04 | 0.46 $\\pm$ 0.03             | 0.49 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.50 $\\pm$ 0.05                           | 0.50 $\\pm$ 0.03     |\n| fr         | distilbert-base-multilingual-cased | 0.37 $\\pm$ 0.06 | 0.44 $\\pm$ 0.02             | 0.48 $\\pm$ 0.04         | 0.48 $\\pm$ 0.02          | 0.44 $\\pm$ 0.04                           | 0.45 $\\pm$ 0.04     |\n| fr         | facebook-mbart-large-50            | 0.44 $\\pm$ 0.03 | 0.47 $\\pm$ 0.02             | 0.51 $\\pm$ 0.02         | 0.52 $\\pm$ 0.03          | 0.52 $\\pm$ 0.03                           | 0.51 $\\pm$ 0.06     |\n| fr         | gpt2                               | 0.36 $\\pm$ 0.06 | 0.36 $\\pm$ 0.02             | 0.43 $\\pm$ 0.02         | 0.43 $\\pm$ 0.02          | 0.42 $\\pm$ 0.06                           | 0.44 $\\pm$ 0.01     |\n| fr         | xlm-roberta-large                  | 0.44 $\\pm$ 0.02 | 0.51 $\\pm$ 0.02             | 0.52 $\\pm$ 0.04         | **0.55 $\\pm$ 0.04**      | 0.54 $\\pm$ 0.02                           | 0.49 $\\pm$ 0.07     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.41 $\\pm$ 0.03 | 0.49 $\\pm$ 0.04             | 0.52 $\\pm$ 0.04         | 0.50 $\\pm$ 0.03          | 0.49 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.03     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.33 $\\pm$ 0.03 | 0.41 $\\pm$ 0.02             | 0.41 $\\pm$ 0.01         | 0.43 $\\pm$ 0.02          | 0.44 $\\pm$ 0.03                           | 0.45 $\\pm$ 0.02     |\n| ge         | bert-base-multilingual-cased       | 0.46 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.54 $\\pm$ 0.02         | 0.58 $\\pm$ 0.04          | 0.54 $\\pm$ 0.02                           | 0.58 $\\pm$ 0.05     |\n| ge         | distilbert-base-multilingual-cased | 0.44 $\\pm$ 0.01 | 0.51 $\\pm$ 0.02             | 0.49 $\\pm$ 0.03         | 0.54 $\\pm$ 0.03          | 0.52 $\\pm$ 0.01                           | 0.55 $\\pm$ 0.04     |\n| ge         | facebook-mbart-large-50            | 0.50 $\\pm$ 0.02 | 0.55 $\\pm$ 0.02             | 0.56 $\\pm$ 0.03         | 0.59 $\\pm$ 0.01          | 0.59 $\\pm$ 0.00                           | 0.58 $\\pm$ 0.05     |\n| ge         | gpt2                               | 0.46 $\\pm$ 0.05 | 0.49 $\\pm$ 0.05             | 0.50 $\\pm$ 0.05         | 0.52 $\\pm$ 0.05          | 0.51 $\\pm$ 0.03                           | 0.50 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.50 $\\pm$ 0.02 | 0.57 $\\pm$ 0.02             | 0.57 $\\pm$ 0.02         | 0.58 $\\pm$ 0.01          | **0.62 $\\pm$ 0.07**                       | 0.61 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.35 $\\pm$ 0.04 | 0.38 $\\pm$ 0.06             | 0.44 $\\pm$ 0.02         | 0.45 $\\pm$ 0.07          | 0.44 $\\pm$ 0.09                           | 0.47 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-125M            | 0.28 $\\pm$ 0.04 | 0.36 $\\pm$ 0.04             | 0.36 $\\pm$ 0.02         | 0.35 $\\pm$ 0.02          | 0.40 $\\pm$ 0.06                           | 0.37 $\\pm$ 0.01     |\n| it         | bert-base-multilingual-cased       | 0.40 $\\pm$ 0.00 | 0.48 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | 0.51 $\\pm$ 0.03          | 0.51 $\\pm$ 0.02                           | 0.49 $\\pm$ 0.02     |\n| it         | distilbert-base-multilingual-cased | 0.39 $\\pm$ 0.01 | 0.44 $\\pm$ 0.04             | 0.46 $\\pm$ 0.03         | 0.50 $\\pm$ 0.06          | 0.45 $\\pm$ 0.04                           | 0.47 $\\pm$ 0.01     |\n| it         | facebook-mbart-large-50            | 0.42 $\\pm$ 0.01 | 0.47 $\\pm$ 0.06             | 0.49 $\\pm$ 0.02         | 0.51 $\\pm$ 0.03          | 0.50 $\\pm$ 0.03                           | 0.52 $\\pm$ 0.05     |\n| it         | gpt2                               | 0.37 $\\pm$ 0.03 | 0.40 $\\pm$ 0.01             | 0.45 $\\pm$ 0.03         | 0.49 $\\pm$ 0.03          | 0.46 $\\pm$ 0.01                           | 0.47 $\\pm$ 0.02     |\n| it         | xlm-roberta-large                  | 0.43 $\\pm$ 0.05 | 0.49 $\\pm$ 0.02             | 0.50 $\\pm$ 0.03         | **0.54 $\\pm$ 0.02**      | 0.52 $\\pm$ 0.02                           | 0.53 $\\pm$ 0.04     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.48 $\\pm$ 0.03 | 0.49 $\\pm$ 0.02             | 0.58 $\\pm$ 0.02         | 0.53 $\\pm$ 0.04          | 0.56 $\\pm$ 0.05                           | 0.58 $\\pm$ 0.05     |\n| po         | EleutherAI-gpt-neo-125M            | 0.34 $\\pm$ 0.05 | 0.42 $\\pm$ 0.04             | 0.50 $\\pm$ 0.02         | 0.44 $\\pm$ 0.06          | 0.49 $\\pm$ 0.02                           | 0.50 $\\pm$ 0.02     |\n| po         | bert-base-multilingual-cased       | 0.50 $\\pm$ 0.04 | 0.56 $\\pm$ 0.04             | 0.56 $\\pm$ 0.04         | 0.58 $\\pm$ 0.02          | 0.62 $\\pm$ 0.01                           | 0.57 $\\pm$ 0.04     |\n| po         | distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.03 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.05         | 0.55 $\\pm$ 0.03          | 0.54 $\\pm$ 0.03                           | 0.57 $\\pm$ 0.03     |\n| po         | facebook-mbart-large-50            | 0.51 $\\pm$ 0.02 | 0.55 $\\pm$ 0.01             | 0.59 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.59 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.05     |\n| po         | gpt2                               | 0.48 $\\pm$ 0.03 | 0.52 $\\pm$ 0.01             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.03          | 0.57 $\\pm$ 0.06                           | 0.57 $\\pm$ 0.04     |\n| po         | xlm-roberta-large                  | 0.54 $\\pm$ 0.02 | 0.55 $\\pm$ 0.05             | 0.60 $\\pm$ 0.03         | 0.63 $\\pm$ 0.02          | 0.63 $\\pm$ 0.03                           | **0.69 $\\pm$ 0.06** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.14 $\\pm$ 0.01 | 0.16 $\\pm$ 0.02             | 0.31 $\\pm$ 0.05         | 0.29 $\\pm$ 0.05          | 0.31 $\\pm$ 0.02                           | 0.32 $\\pm$ 0.07     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.16 $\\pm$ 0.01 | 0.17 $\\pm$ 0.03             | 0.15 $\\pm$ 0.02         | 0.16 $\\pm$ 0.01          | 0.14 $\\pm$ 0.02                           | 0.12 $\\pm$ 0.04     |\n| ru         | bert-base-multilingual-cased       | 0.31 $\\pm$ 0.03 | 0.40 $\\pm$ 0.05             | 0.44 $\\pm$ 0.05         | 0.45 $\\pm$ 0.10          | 0.44 $\\pm$ 0.04                           | 0.45 $\\pm$ 0.04     |\n| ru         | distilbert-base-multilingual-cased | 0.23 $\\pm$ 0.01 | 0.33 $\\pm$ 0.02             | 0.35 $\\pm$ 0.04         | 0.39 $\\pm$ 0.06          | 0.39 $\\pm$ 0.06                           | 0.36 $\\pm$ 0.02     |\n| ru         | facebook-mbart-large-50            | 0.31 $\\pm$ 0.04 | 0.42 $\\pm$ 0.02             | 0.44 $\\pm$ 0.05         | 0.47 $\\pm$ 0.06          | 0.44 $\\pm$ 0.03                           | 0.47 $\\pm$ 0.01     |\n| ru         | gpt2                               | 0.11 $\\pm$ 0.06 | 0.09 $\\pm$ 0.04             | 0.04 $\\pm$ 0.05         | 0.09 $\\pm$ 0.06          | 0.10 $\\pm$ 0.04                           | 0.09 $\\pm$ 0.07     |\n| ru         | xlm-roberta-large                  | 0.40 $\\pm$ 0.03 | 0.47 $\\pm$ 0.05             | 0.48 $\\pm$ 0.06         | **0.52 $\\pm$ 0.03**      | 0.46 $\\pm$ 0.05                           | 0.47 $\\pm$ 0.07     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## precision-micro"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.72 $\\pm$ 0.04 | 0.76 $\\pm$ 0.02             | 0.74 $\\pm$ 0.02         | 0.76 $\\pm$ 0.01          | 0.78 $\\pm$ 0.04                           | **0.79 $\\pm$ 0.02** |\n| en         | EleutherAI-gpt-neo-125M            | 0.63 $\\pm$ 0.04 | 0.67 $\\pm$ 0.01             | 0.70 $\\pm$ 0.01         | 0.71 $\\pm$ 0.03          | 0.74 $\\pm$ 0.04                           | 0.77 $\\pm$ 0.03     |\n| en         | bert-base-multilingual-cased       | 0.70 $\\pm$ 0.03 | 0.74 $\\pm$ 0.03             | 0.76 $\\pm$ 0.02         | 0.74 $\\pm$ 0.04          | 0.76 $\\pm$ 0.04                           | 0.76 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.68 $\\pm$ 0.05 | 0.74 $\\pm$ 0.03             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.03          | 0.76 $\\pm$ 0.02                           | 0.76 $\\pm$ 0.03     |\n| en         | facebook-mbart-large-50            | 0.73 $\\pm$ 0.01 | 0.76 $\\pm$ 0.01             | 0.74 $\\pm$ 0.03         | 0.76 $\\pm$ 0.01          | 0.78 $\\pm$ 0.02                           | 0.76 $\\pm$ 0.02     |\n| en         | gpt2                               | 0.67 $\\pm$ 0.06 | 0.72 $\\pm$ 0.02             | 0.70 $\\pm$ 0.03         | 0.67 $\\pm$ 0.03          | 0.72 $\\pm$ 0.06                           | 0.74 $\\pm$ 0.03     |\n| en         | xlm-roberta-large                  | 0.73 $\\pm$ 0.03 | 0.75 $\\pm$ 0.03             | 0.75 $\\pm$ 0.01         | 0.76 $\\pm$ 0.03          | 0.74 $\\pm$ 0.01                           | 0.76 $\\pm$ 0.02     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.03 | 0.63 $\\pm$ 0.04             | 0.57 $\\pm$ 0.04         | 0.62 $\\pm$ 0.06          | 0.65 $\\pm$ 0.05                           | 0.66 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.42 $\\pm$ 0.05 | 0.48 $\\pm$ 0.03             | 0.51 $\\pm$ 0.02         | 0.55 $\\pm$ 0.06          | 0.52 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.04     |\n| fr         | bert-base-multilingual-cased       | 0.59 $\\pm$ 0.05 | 0.60 $\\pm$ 0.02             | 0.59 $\\pm$ 0.03         | 0.61 $\\pm$ 0.05          | 0.63 $\\pm$ 0.01                           | 0.67 $\\pm$ 0.03     |\n| fr         | distilbert-base-multilingual-cased | 0.53 $\\pm$ 0.05 | 0.59 $\\pm$ 0.05             | 0.59 $\\pm$ 0.03         | 0.59 $\\pm$ 0.03          | 0.63 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.02     |\n| fr         | facebook-mbart-large-50            | 0.60 $\\pm$ 0.06 | 0.62 $\\pm$ 0.01             | 0.62 $\\pm$ 0.04         | 0.63 $\\pm$ 0.02          | 0.65 $\\pm$ 0.02                           | **0.68 $\\pm$ 0.04** |\n| fr         | gpt2                               | 0.45 $\\pm$ 0.10 | 0.51 $\\pm$ 0.04             | 0.53 $\\pm$ 0.02         | 0.57 $\\pm$ 0.04          | 0.58 $\\pm$ 0.03                           | 0.60 $\\pm$ 0.02     |\n| fr         | xlm-roberta-large                  | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.04             | 0.63 $\\pm$ 0.03         | 0.62 $\\pm$ 0.01          | 0.61 $\\pm$ 0.02                           | 0.63 $\\pm$ 0.02     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.57 $\\pm$ 0.05 | 0.65 $\\pm$ 0.02             | 0.61 $\\pm$ 0.03         | 0.70 $\\pm$ 0.06          | 0.66 $\\pm$ 0.03                           | **0.71 $\\pm$ 0.02** |\n| ge         | EleutherAI-gpt-neo-125M            | 0.49 $\\pm$ 0.02 | 0.51 $\\pm$ 0.04             | 0.57 $\\pm$ 0.01         | 0.58 $\\pm$ 0.02          | 0.60 $\\pm$ 0.05                           | 0.65 $\\pm$ 0.02     |\n| ge         | bert-base-multilingual-cased       | 0.60 $\\pm$ 0.02 | 0.62 $\\pm$ 0.02             | 0.66 $\\pm$ 0.03         | 0.68 $\\pm$ 0.06          | 0.66 $\\pm$ 0.01                           | 0.67 $\\pm$ 0.03     |\n| ge         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.63 $\\pm$ 0.02             | 0.63 $\\pm$ 0.04         | 0.64 $\\pm$ 0.05          | 0.66 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.01     |\n| ge         | facebook-mbart-large-50            | 0.62 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.68 $\\pm$ 0.00         | 0.68 $\\pm$ 0.02          | 0.70 $\\pm$ 0.03                           | **0.71 $\\pm$ 0.01** |\n| ge         | gpt2                               | 0.49 $\\pm$ 0.01 | 0.56 $\\pm$ 0.03             | 0.55 $\\pm$ 0.04         | 0.58 $\\pm$ 0.00          | 0.56 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.62 $\\pm$ 0.03 | 0.65 $\\pm$ 0.02             | 0.67 $\\pm$ 0.04         | **0.71 $\\pm$ 0.01**      | 0.68 $\\pm$ 0.04                           | 0.67 $\\pm$ 0.07     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.53 $\\pm$ 0.03 | 0.58 $\\pm$ 0.03             | 0.58 $\\pm$ 0.05         | 0.63 $\\pm$ 0.05          | 0.63 $\\pm$ 0.03                           | 0.68 $\\pm$ 0.05     |\n| it         | EleutherAI-gpt-neo-125M            | 0.47 $\\pm$ 0.05 | 0.49 $\\pm$ 0.01             | 0.55 $\\pm$ 0.01         | 0.57 $\\pm$ 0.01          | 0.59 $\\pm$ 0.04                           | 0.67 $\\pm$ 0.03     |\n| it         | bert-base-multilingual-cased       | 0.57 $\\pm$ 0.02 | 0.59 $\\pm$ 0.04             | 0.61 $\\pm$ 0.04         | 0.62 $\\pm$ 0.05          | 0.62 $\\pm$ 0.07                           | 0.65 $\\pm$ 0.07     |\n| it         | distilbert-base-multilingual-cased | 0.54 $\\pm$ 0.02 | 0.57 $\\pm$ 0.06             | 0.58 $\\pm$ 0.03         | 0.62 $\\pm$ 0.03          | 0.60 $\\pm$ 0.04                           | 0.64 $\\pm$ 0.04     |\n| it         | facebook-mbart-large-50            | 0.57 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.03         | 0.64 $\\pm$ 0.03          | 0.65 $\\pm$ 0.08                           | **0.70 $\\pm$ 0.05** |\n| it         | gpt2                               | 0.47 $\\pm$ 0.01 | 0.52 $\\pm$ 0.02             | 0.52 $\\pm$ 0.03         | 0.55 $\\pm$ 0.01          | 0.55 $\\pm$ 0.06                           | 0.62 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.58 $\\pm$ 0.03 | 0.59 $\\pm$ 0.03             | 0.61 $\\pm$ 0.02         | 0.64 $\\pm$ 0.05          | 0.61 $\\pm$ 0.05                           | 0.63 $\\pm$ 0.05     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.56 $\\pm$ 0.10 | 0.63 $\\pm$ 0.05             | 0.63 $\\pm$ 0.06         | 0.64 $\\pm$ 0.06          | 0.69 $\\pm$ 0.04                           | 0.68 $\\pm$ 0.04     |\n| po         | EleutherAI-gpt-neo-125M            | 0.48 $\\pm$ 0.01 | 0.53 $\\pm$ 0.03             | 0.60 $\\pm$ 0.06         | 0.61 $\\pm$ 0.07          | 0.60 $\\pm$ 0.05                           | 0.65 $\\pm$ 0.05     |\n| po         | bert-base-multilingual-cased       | 0.61 $\\pm$ 0.07 | 0.63 $\\pm$ 0.02             | 0.67 $\\pm$ 0.05         | 0.69 $\\pm$ 0.04          | 0.68 $\\pm$ 0.05                           | 0.67 $\\pm$ 0.08     |\n| po         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.04 | 0.64 $\\pm$ 0.06             | 0.68 $\\pm$ 0.07         | 0.67 $\\pm$ 0.08          | 0.67 $\\pm$ 0.09                           | 0.67 $\\pm$ 0.05     |\n| po         | facebook-mbart-large-50            | 0.61 $\\pm$ 0.06 | 0.67 $\\pm$ 0.04             | 0.67 $\\pm$ 0.07         | 0.70 $\\pm$ 0.02          | 0.69 $\\pm$ 0.05                           | **0.73 $\\pm$ 0.01** |\n| po         | gpt2                               | 0.51 $\\pm$ 0.07 | 0.59 $\\pm$ 0.10             | 0.59 $\\pm$ 0.05         | 0.58 $\\pm$ 0.06          | 0.61 $\\pm$ 0.06                           | 0.62 $\\pm$ 0.06     |\n| po         | xlm-roberta-large                  | 0.62 $\\pm$ 0.11 | 0.64 $\\pm$ 0.03             | 0.67 $\\pm$ 0.04         | 0.68 $\\pm$ 0.05          | 0.68 $\\pm$ 0.06                           | 0.64 $\\pm$ 0.03     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.43 $\\pm$ 0.06 | 0.59 $\\pm$ 0.14             | 0.50 $\\pm$ 0.04         | 0.52 $\\pm$ 0.09          | 0.59 $\\pm$ 0.15                           | 0.52 $\\pm$ 0.07     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.35 $\\pm$ 0.03 | 0.32 $\\pm$ 0.08             | 0.36 $\\pm$ 0.05         | 0.37 $\\pm$ 0.03          | 0.36 $\\pm$ 0.11                           | 0.39 $\\pm$ 0.12     |\n| ru         | bert-base-multilingual-cased       | 0.53 $\\pm$ 0.02 | 0.53 $\\pm$ 0.05             | 0.59 $\\pm$ 0.04         | 0.60 $\\pm$ 0.02          | 0.60 $\\pm$ 0.04                           | 0.59 $\\pm$ 0.01     |\n| ru         | distilbert-base-multilingual-cased | 0.49 $\\pm$ 0.04 | 0.54 $\\pm$ 0.04             | 0.58 $\\pm$ 0.01         | 0.57 $\\pm$ 0.04          | 0.61 $\\pm$ 0.02                           | 0.57 $\\pm$ 0.01     |\n| ru         | facebook-mbart-large-50            | 0.55 $\\pm$ 0.06 | 0.60 $\\pm$ 0.03             | 0.61 $\\pm$ 0.04         | 0.63 $\\pm$ 0.04          | 0.62 $\\pm$ 0.02                           | **0.67 $\\pm$ 0.07** |\n| ru         | gpt2                               | 0.30 $\\pm$ 0.03 | 0.26 $\\pm$ 0.10             | 0.25 $\\pm$ 0.04         | 0.37 $\\pm$ 0.02          | 0.41 $\\pm$ 0.02                           | 0.47 $\\pm$ 0.12     |\n| ru         | xlm-roberta-large                  | 0.57 $\\pm$ 0.03 | 0.61 $\\pm$ 0.01             | 0.61 $\\pm$ 0.06         | 0.58 $\\pm$ 0.03          | 0.59 $\\pm$ 0.02                           | 0.61 $\\pm$ 0.07     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## roc-auc"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.73 $\\pm$ 0.00 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.76 $\\pm$ 0.00          | 0.76 $\\pm$ 0.02                           | 0.77 $\\pm$ 0.00     |\n| en         | EleutherAI-gpt-neo-125M            | 0.69 $\\pm$ 0.01 | 0.72 $\\pm$ 0.01             | 0.73 $\\pm$ 0.01         | 0.73 $\\pm$ 0.00          | 0.74 $\\pm$ 0.01                           | 0.76 $\\pm$ 0.01     |\n| en         | bert-base-multilingual-cased       | 0.74 $\\pm$ 0.01 | 0.77 $\\pm$ 0.01             | 0.78 $\\pm$ 0.01         | 0.78 $\\pm$ 0.01          | 0.78 $\\pm$ 0.02                           | 0.78 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.73 $\\pm$ 0.02 | 0.76 $\\pm$ 0.01             | 0.77 $\\pm$ 0.01         | 0.77 $\\pm$ 0.02          | 0.76 $\\pm$ 0.00                           | 0.77 $\\pm$ 0.01     |\n| en         | facebook-mbart-large-50            | 0.76 $\\pm$ 0.01 | 0.78 $\\pm$ 0.01             | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | 0.78 $\\pm$ 0.01     |\n| en         | gpt2                               | 0.74 $\\pm$ 0.02 | 0.77 $\\pm$ 0.01             | 0.77 $\\pm$ 0.00         | 0.77 $\\pm$ 0.02          | 0.77 $\\pm$ 0.01                           | 0.77 $\\pm$ 0.01     |\n| en         | xlm-roberta-large                  | 0.76 $\\pm$ 0.00 | **0.79 $\\pm$ 0.00**         | **0.79 $\\pm$ 0.01**     | **0.79 $\\pm$ 0.01**      | **0.79 $\\pm$ 0.01**                       | 0.78 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.68 $\\pm$ 0.00          | 0.68 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.03     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.01 | 0.61 $\\pm$ 0.01             | 0.62 $\\pm$ 0.01         | 0.62 $\\pm$ 0.01          | 0.63 $\\pm$ 0.02                           | 0.64 $\\pm$ 0.03     |\n| fr         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.70 $\\pm$ 0.02         | 0.71 $\\pm$ 0.01          | 0.71 $\\pm$ 0.02                           | 0.71 $\\pm$ 0.01     |\n| fr         | distilbert-base-multilingual-cased | 0.64 $\\pm$ 0.03 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.69 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| fr         | facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.00         | 0.72 $\\pm$ 0.02          | 0.72 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.02     |\n| fr         | gpt2                               | 0.62 $\\pm$ 0.04 | 0.64 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.67 $\\pm$ 0.01          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.00     |\n| fr         | xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.01         | **0.73 $\\pm$ 0.02**      | 0.72 $\\pm$ 0.01                           | 0.70 $\\pm$ 0.03     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.64 $\\pm$ 0.00 | 0.69 $\\pm$ 0.02             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.00          | 0.69 $\\pm$ 0.01                           | 0.72 $\\pm$ 0.01     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.59 $\\pm$ 0.01 | 0.62 $\\pm$ 0.01             | 0.64 $\\pm$ 0.00         | 0.65 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.67 $\\pm$ 0.01     |\n| ge         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.01             | 0.71 $\\pm$ 0.01         | 0.73 $\\pm$ 0.01          | 0.71 $\\pm$ 0.00                           | 0.72 $\\pm$ 0.02     |\n| ge         | distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.69 $\\pm$ 0.00             | 0.68 $\\pm$ 0.02         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.00                           | 0.71 $\\pm$ 0.02     |\n| ge         | facebook-mbart-large-50            | 0.68 $\\pm$ 0.01 | 0.71 $\\pm$ 0.01             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.01          | **0.74 $\\pm$ 0.01**                       | **0.74 $\\pm$ 0.01** |\n| ge         | gpt2                               | 0.62 $\\pm$ 0.02 | 0.66 $\\pm$ 0.03             | 0.66 $\\pm$ 0.03         | 0.67 $\\pm$ 0.02          | 0.66 $\\pm$ 0.01                           | 0.68 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.68 $\\pm$ 0.01 | 0.72 $\\pm$ 0.00             | 0.72 $\\pm$ 0.02         | **0.74 $\\pm$ 0.01**      | **0.74 $\\pm$ 0.02**                       | 0.73 $\\pm$ 0.02     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.61 $\\pm$ 0.01 | 0.64 $\\pm$ 0.03             | 0.66 $\\pm$ 0.02         | 0.67 $\\pm$ 0.02          | 0.67 $\\pm$ 0.03                           | 0.69 $\\pm$ 0.01     |\n| it         | EleutherAI-gpt-neo-125M            | 0.58 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.63 $\\pm$ 0.01         | 0.63 $\\pm$ 0.01          | 0.65 $\\pm$ 0.03                           | 0.65 $\\pm$ 0.01     |\n| it         | bert-base-multilingual-cased       | 0.64 $\\pm$ 0.01 | 0.68 $\\pm$ 0.01             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.02     |\n| it         | distilbert-base-multilingual-cased | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.69 $\\pm$ 0.03          | 0.67 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.01     |\n| it         | facebook-mbart-large-50            | 0.65 $\\pm$ 0.00 | 0.68 $\\pm$ 0.03             | 0.69 $\\pm$ 0.01         | 0.70 $\\pm$ 0.02          | 0.70 $\\pm$ 0.03                           | **0.72 $\\pm$ 0.03** |\n| it         | gpt2                               | 0.61 $\\pm$ 0.01 | 0.63 $\\pm$ 0.01             | 0.65 $\\pm$ 0.02         | 0.67 $\\pm$ 0.01          | 0.66 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.01     |\n| it         | xlm-roberta-large                  | 0.66 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.01         | 0.71 $\\pm$ 0.02          | 0.70 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.63 $\\pm$ 0.02 | 0.66 $\\pm$ 0.00             | 0.69 $\\pm$ 0.01         | 0.69 $\\pm$ 0.02          | 0.71 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-125M            | 0.57 $\\pm$ 0.02 | 0.61 $\\pm$ 0.01             | 0.66 $\\pm$ 0.01         | 0.64 $\\pm$ 0.03          | 0.66 $\\pm$ 0.02                           | 0.68 $\\pm$ 0.02     |\n| po         | bert-base-multilingual-cased       | 0.66 $\\pm$ 0.01 | 0.69 $\\pm$ 0.01             | 0.70 $\\pm$ 0.02         | 0.72 $\\pm$ 0.01          | 0.73 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| po         | distilbert-base-multilingual-cased | 0.65 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.72 $\\pm$ 0.01         | 0.70 $\\pm$ 0.01          | 0.70 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.66 $\\pm$ 0.02 | 0.70 $\\pm$ 0.00             | 0.72 $\\pm$ 0.01         | **0.74 $\\pm$ 0.01**      | 0.72 $\\pm$ 0.01                           | **0.74 $\\pm$ 0.01** |\n| po         | gpt2                               | 0.61 $\\pm$ 0.01 | 0.66 $\\pm$ 0.03             | 0.67 $\\pm$ 0.02         | 0.68 $\\pm$ 0.01          | 0.68 $\\pm$ 0.01                           | 0.69 $\\pm$ 0.02     |\n| po         | xlm-roberta-large                  | 0.68 $\\pm$ 0.02 | 0.69 $\\pm$ 0.02             | 0.72 $\\pm$ 0.02         | 0.73 $\\pm$ 0.02          | 0.73 $\\pm$ 0.01                           | **0.74 $\\pm$ 0.01** |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.55 $\\pm$ 0.01 | 0.57 $\\pm$ 0.01             | 0.62 $\\pm$ 0.02         | 0.62 $\\pm$ 0.02          | 0.63 $\\pm$ 0.01                           | 0.63 $\\pm$ 0.03     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.55 $\\pm$ 0.01 | 0.55 $\\pm$ 0.02             | 0.55 $\\pm$ 0.01         | 0.55 $\\pm$ 0.01          | 0.54 $\\pm$ 0.02                           | 0.54 $\\pm$ 0.01     |\n| ru         | bert-base-multilingual-cased       | 0.63 $\\pm$ 0.01 | 0.66 $\\pm$ 0.02             | 0.69 $\\pm$ 0.03         | 0.69 $\\pm$ 0.04          | 0.69 $\\pm$ 0.02                           | 0.69 $\\pm$ 0.01     |\n| ru         | distilbert-base-multilingual-cased | 0.59 $\\pm$ 0.01 | 0.64 $\\pm$ 0.01             | 0.65 $\\pm$ 0.01         | 0.66 $\\pm$ 0.03          | 0.67 $\\pm$ 0.02                           | 0.65 $\\pm$ 0.01     |\n| ru         | facebook-mbart-large-50            | 0.63 $\\pm$ 0.02 | 0.68 $\\pm$ 0.02             | 0.69 $\\pm$ 0.02         | 0.71 $\\pm$ 0.03          | 0.69 $\\pm$ 0.01                           | 0.71 $\\pm$ 0.00     |\n| ru         | gpt2                               | 0.53 $\\pm$ 0.02 | 0.52 $\\pm$ 0.02             | 0.51 $\\pm$ 0.01         | 0.53 $\\pm$ 0.02          | 0.53 $\\pm$ 0.01                           | 0.53 $\\pm$ 0.02     |\n| ru         | xlm-roberta-large                  | 0.67 $\\pm$ 0.01 | 0.70 $\\pm$ 0.03             | 0.71 $\\pm$ 0.02         | **0.72 $\\pm$ 0.01**      | 0.70 $\\pm$ 0.02                           | 0.70 $\\pm$ 0.02     |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## accuracy"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "| language   | model_name                         | title           | title and first paragraph   | title and 5 sentences   | title and 10 sentences   | title and first sentence each paragraph   | raw text            |\n|:-----------|:-----------------------------------|:----------------|:----------------------------|:------------------------|:-------------------------|:------------------------------------------|:--------------------|\n| en         | EleutherAI-gpt-neo-1.3B            | 0.08 $\\pm$ 0.02 | 0.12 $\\pm$ 0.02             | 0.08 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.10 $\\pm$ 0.02                           | **0.13 $\\pm$ 0.01** |\n| en         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.05 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.01          | 0.08 $\\pm$ 0.01                           | 0.09 $\\pm$ 0.03     |\n| en         | bert-base-multilingual-cased       | 0.07 $\\pm$ 0.01 | 0.10 $\\pm$ 0.03             | **0.13 $\\pm$ 0.03**     | 0.09 $\\pm$ 0.00          | 0.10 $\\pm$ 0.04                           | 0.11 $\\pm$ 0.01     |\n| en         | distilbert-base-multilingual-cased | 0.06 $\\pm$ 0.01 | 0.09 $\\pm$ 0.02             | 0.10 $\\pm$ 0.02         | 0.09 $\\pm$ 0.01          | 0.08 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.03     |\n| en         | facebook-mbart-large-50            | 0.07 $\\pm$ 0.04 | 0.10 $\\pm$ 0.03             | 0.11 $\\pm$ 0.03         | 0.12 $\\pm$ 0.03          | 0.12 $\\pm$ 0.02                           | 0.11 $\\pm$ 0.01     |\n| en         | gpt2                               | 0.06 $\\pm$ 0.01 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.06 $\\pm$ 0.03          | 0.08 $\\pm$ 0.00                           | 0.10 $\\pm$ 0.01     |\n| en         | xlm-roberta-large                  | 0.10 $\\pm$ 0.01 | 0.12 $\\pm$ 0.01             | 0.10 $\\pm$ 0.01         | 0.11 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | 0.11 $\\pm$ 0.01     |\n| fr         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.02 | 0.07 $\\pm$ 0.01             | 0.06 $\\pm$ 0.03         | 0.08 $\\pm$ 0.02          | 0.09 $\\pm$ 0.04                           | 0.09 $\\pm$ 0.03     |\n| fr         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.01 | 0.01 $\\pm$ 0.01             | 0.03 $\\pm$ 0.01         | 0.04 $\\pm$ 0.01          | 0.04 $\\pm$ 0.03                           | 0.08 $\\pm$ 0.05     |\n| fr         | bert-base-multilingual-cased       | 0.07 $\\pm$ 0.02 | 0.08 $\\pm$ 0.02             | 0.07 $\\pm$ 0.01         | 0.09 $\\pm$ 0.01          | 0.10 $\\pm$ 0.01                           | **0.11 $\\pm$ 0.02** |\n| fr         | distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.06 $\\pm$ 0.04          | 0.09 $\\pm$ 0.04                           | 0.09 $\\pm$ 0.01     |\n| fr         | facebook-mbart-large-50            | 0.08 $\\pm$ 0.04 | **0.11 $\\pm$ 0.02**         | 0.09 $\\pm$ 0.01         | 0.10 $\\pm$ 0.03          | **0.11 $\\pm$ 0.03**                       | **0.11 $\\pm$ 0.02** |\n| fr         | gpt2                               | 0.03 $\\pm$ 0.02 | 0.05 $\\pm$ 0.02             | 0.06 $\\pm$ 0.04         | 0.07 $\\pm$ 0.02          | 0.06 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.05     |\n| fr         | xlm-roberta-large                  | 0.07 $\\pm$ 0.03 | 0.07 $\\pm$ 0.04             | 0.10 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | **0.11 $\\pm$ 0.05**                       | 0.09 $\\pm$ 0.05     |\n| ge         | EleutherAI-gpt-neo-1.3B            | 0.02 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.05 $\\pm$ 0.03         | 0.05 $\\pm$ 0.03          | 0.04 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.03     |\n| ge         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.03 $\\pm$ 0.02         | 0.02 $\\pm$ 0.01          | 0.04 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.02     |\n| ge         | bert-base-multilingual-cased       | 0.05 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.03         | 0.09 $\\pm$ 0.05          | 0.06 $\\pm$ 0.01                           | 0.10 $\\pm$ 0.05     |\n| ge         | distilbert-base-multilingual-cased | 0.02 $\\pm$ 0.01 | 0.02 $\\pm$ 0.02             | 0.05 $\\pm$ 0.04         | 0.05 $\\pm$ 0.03          | 0.05 $\\pm$ 0.04                           | 0.04 $\\pm$ 0.04     |\n| ge         | facebook-mbart-large-50            | 0.05 $\\pm$ 0.03 | 0.06 $\\pm$ 0.04             | 0.06 $\\pm$ 0.03         | 0.05 $\\pm$ 0.02          | **0.11 $\\pm$ 0.03**                       | 0.08 $\\pm$ 0.04     |\n| ge         | gpt2                               | 0.01 $\\pm$ 0.01 | 0.01 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.03          | 0.03 $\\pm$ 0.01                           | 0.02 $\\pm$ 0.01     |\n| ge         | xlm-roberta-large                  | 0.03 $\\pm$ 0.02 | 0.07 $\\pm$ 0.03             | 0.09 $\\pm$ 0.03         | 0.10 $\\pm$ 0.04          | 0.07 $\\pm$ 0.04                           | 0.07 $\\pm$ 0.04     |\n| it         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.04             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.03          | 0.07 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.03     |\n| it         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.04 $\\pm$ 0.01             | 0.04 $\\pm$ 0.04         | 0.01 $\\pm$ 0.01          | 0.04 $\\pm$ 0.01                           | 0.08 $\\pm$ 0.03     |\n| it         | bert-base-multilingual-cased       | 0.04 $\\pm$ 0.01 | 0.05 $\\pm$ 0.03             | 0.06 $\\pm$ 0.01         | 0.09 $\\pm$ 0.02          | 0.09 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.03     |\n| it         | distilbert-base-multilingual-cased | 0.04 $\\pm$ 0.02 | 0.05 $\\pm$ 0.03             | 0.07 $\\pm$ 0.02         | 0.06 $\\pm$ 0.04          | 0.07 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.05     |\n| it         | facebook-mbart-large-50            | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.04             | 0.05 $\\pm$ 0.02         | 0.09 $\\pm$ 0.02          | 0.08 $\\pm$ 0.03                           | **0.11 $\\pm$ 0.02** |\n| it         | gpt2                               | 0.02 $\\pm$ 0.02 | 0.02 $\\pm$ 0.02             | 0.04 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.03 $\\pm$ 0.00                           | 0.07 $\\pm$ 0.03     |\n| it         | xlm-roberta-large                  | 0.06 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.08 $\\pm$ 0.01         | 0.07 $\\pm$ 0.02          | 0.08 $\\pm$ 0.04                           | 0.08 $\\pm$ 0.03     |\n| po         | EleutherAI-gpt-neo-1.3B            | 0.01 $\\pm$ 0.01 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.02         | 0.03 $\\pm$ 0.02          | 0.05 $\\pm$ 0.02                           | 0.06 $\\pm$ 0.02     |\n| po         | EleutherAI-gpt-neo-125M            | 0.01 $\\pm$ 0.01 | 0.02 $\\pm$ 0.01             | 0.02 $\\pm$ 0.00         | 0.02 $\\pm$ 0.01          | 0.03 $\\pm$ 0.01                           | 0.04 $\\pm$ 0.04     |\n| po         | bert-base-multilingual-cased       | 0.04 $\\pm$ 0.02 | 0.06 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.06 $\\pm$ 0.02          | 0.07 $\\pm$ 0.04                           | 0.06 $\\pm$ 0.02     |\n| po         | distilbert-base-multilingual-cased | 0.03 $\\pm$ 0.02 | 0.04 $\\pm$ 0.01             | 0.05 $\\pm$ 0.03         | 0.04 $\\pm$ 0.01          | 0.05 $\\pm$ 0.02                           | 0.04 $\\pm$ 0.01     |\n| po         | facebook-mbart-large-50            | 0.05 $\\pm$ 0.02 | 0.05 $\\pm$ 0.01             | 0.06 $\\pm$ 0.02         | 0.07 $\\pm$ 0.03          | 0.05 $\\pm$ 0.03                           | **0.11 $\\pm$ 0.03** |\n| po         | gpt2                               | 0.00 $\\pm$ 0.00 | 0.03 $\\pm$ 0.02             | 0.03 $\\pm$ 0.03         | 0.04 $\\pm$ 0.06          | 0.02 $\\pm$ 0.02                           | 0.03 $\\pm$ 0.02     |\n| po         | xlm-roberta-large                  | 0.04 $\\pm$ 0.01 | 0.07 $\\pm$ 0.02             | 0.07 $\\pm$ 0.03         | 0.07 $\\pm$ 0.04          | 0.06 $\\pm$ 0.03                           | 0.07 $\\pm$ 0.02     |\n| ru         | EleutherAI-gpt-neo-1.3B            | 0.04 $\\pm$ 0.03 | 0.06 $\\pm$ 0.03             | 0.10 $\\pm$ 0.01         | 0.08 $\\pm$ 0.03          | 0.13 $\\pm$ 0.05                           | 0.08 $\\pm$ 0.03     |\n| ru         | EleutherAI-gpt-neo-125M            | 0.03 $\\pm$ 0.03 | 0.04 $\\pm$ 0.05             | 0.06 $\\pm$ 0.02         | 0.05 $\\pm$ 0.04          | 0.03 $\\pm$ 0.03                           | 0.04 $\\pm$ 0.04     |\n| ru         | bert-base-multilingual-cased       | 0.09 $\\pm$ 0.03 | 0.12 $\\pm$ 0.04             | **0.17 $\\pm$ 0.03**     | 0.15 $\\pm$ 0.08          | **0.17 $\\pm$ 0.03**                       | 0.13 $\\pm$ 0.06     |\n| ru         | distilbert-base-multilingual-cased | 0.05 $\\pm$ 0.01 | 0.11 $\\pm$ 0.06             | 0.08 $\\pm$ 0.03         | 0.13 $\\pm$ 0.03          | 0.13 $\\pm$ 0.07                           | 0.12 $\\pm$ 0.03     |\n| ru         | facebook-mbart-large-50            | 0.11 $\\pm$ 0.03 | 0.16 $\\pm$ 0.03             | 0.14 $\\pm$ 0.03         | **0.17 $\\pm$ 0.05**      | 0.15 $\\pm$ 0.03                           | 0.14 $\\pm$ 0.02     |\n| ru         | gpt2                               | 0.02 $\\pm$ 0.02 | 0.03 $\\pm$ 0.02             | 0.02 $\\pm$ 0.02         | 0.02 $\\pm$ 0.00          | 0.02 $\\pm$ 0.01                           | 0.03 $\\pm$ 0.01     |\n| ru         | xlm-roberta-large                  | 0.12 $\\pm$ 0.05 | **0.17 $\\pm$ 0.04**         | 0.14 $\\pm$ 0.06         | 0.14 $\\pm$ 0.08          | 0.13 $\\pm$ 0.02                           | **0.17 $\\pm$ 0.06** |"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36756/2331756451.py:60: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  multi_language_report_table_metric.reset_index().to_latex(latex_file, index=False)\n"
     ]
    }
   ],
   "source": [
    "display_metrics_and_write_to_file(df=results_majority_vote_pred_df, grouping_criterion=['model_name'], output_dir='per_model_name_tables_majority_voting')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
