{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_from_disk\n",
    "from transformers import pipeline\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "LABELS = ('fairness_and_equality', 'security_and_defense', 'crime_and_punishment', 'morality',\n",
    "          'policy_prescription_and_evaluation', 'capacity_and_resources', 'economic', 'cultural_identity',\n",
    "          'health_and_safety', 'quality_of_life', 'legality_constitutionality_and_jurisprudence',\n",
    "          'political', 'public_opinion', 'external_regulation_and_reputation')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Join Datasets from different languages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df_paths = glob.glob(os.path.join('..', '..', 'data', 'preprocessed', '*train_and_dev.csv'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "for i, df_path_i in enumerate(df_paths):\n",
    "    df_i = pd.read_csv(df_path_i, index_col='id')\n",
    "    df_i['language'] = os.path.basename(df_path_i).split('_')[1]\n",
    "\n",
    "    if i == 0:\n",
    "        df = df_i\n",
    "    else:\n",
    "        df = pd.concat([df, df_i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  frames  \\\nid                                                         \n24151  Legality_Constitutionality_and_jurisprudence,H...   \n24150  Legality_Constitutionality_and_jurisprudence,H...   \n24153  Legality_Constitutionality_and_jurisprudence,F...   \n24152  Legality_Constitutionality_and_jurisprudence,P...   \n24147  Legality_Constitutionality_and_jurisprudence,E...   \n...                                                  ...   \n25143                     Political,Security_and_defense   \n2528   Policy_prescription_and_evaluation,Economic,Ex...   \n2530   Policy_prescription_and_evaluation,Economic,Ex...   \n2527   Policy_prescription_and_evaluation,Economic,Qu...   \n2564   Policy_prescription_and_evaluation,Morality,Ec...   \n\n                                                raw_text  \\\nid                                                         \n24151  Байдену напомнили о его отношению к абортам в ...   \n24150  В США после отмены права на аборт произошел на...   \n24153  Байден пообещал американским мужчинам, что они...   \n24152  Моё тело – моё дело: американки протестуют из-...   \n24147  В США начались погромы из-за запрета абортов: ...   \n...                                                  ...   \n25143  Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...   \n2528   Rosja może uniknąć bankructwa. Zachodnia machi...   \n2530   Embargo na gaz i ropę z Rosji? Jednoznaczne st...   \n2527   Korwin-Mikke: Nakładanie na Rosję sankcji jest...   \n2564   Grzegorz Braun i naukowcy ws. pokoju na Ukrain...   \n\n                                                   title  \\\nid                                                         \n24151  Байдену напомнили о его отношению к абортам в ...   \n24150  В США после отмены права на аборт произошел на...   \n24153  Байден пообещал американским мужчинам, что они...   \n24152  Моё тело – моё дело: американки протестуют из-...   \n24147  В США начались погромы из-за запрета абортов: ...   \n...                                                  ...   \n25143  Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...   \n2528   Rosja może uniknąć bankructwa. Zachodnia machi...   \n2530   Embargo na gaz i ropę z Rosji? Jednoznaczne st...   \n2527   Korwin-Mikke: Nakładanie na Rosję sankcji jest...   \n2564    Grzegorz Braun i naukowcy ws. pokoju na Ukrainie   \n\n                                                 content  \\\nid                                                         \n24151  CNN напомнил о трансформации отношения Байдена...   \n24150  В США после отмены права на аборт произошел на...   \n24153  Решение Верховного суда об абортах ужасно. Не ...   \n24152  Что случилось?\\n\\nВерховный суд США отменил ко...   \n24147  Почему скандальный закон приняли именно сейчас...   \n...                                                  ...   \n25143  Każdy, kto miał kiedyś nowy samochód wie, że p...   \n2528   Nie jest pewne, czy obecne sankcje wystarczą d...   \n2530   Marine Le Pen, rywalka Emmanuela Macrona w wyś...   \n2527   Prędzej Rosja sobie poradzi bez złota, niż my ...   \n2564   ako środowisko narodowo-radykalne mamy swoje s...   \n\n                                   title_and_5_sentences  \\\nid                                                         \n24151  Байдену напомнили о его отношению к абортам в ...   \n24150  В США после отмены права на аборт произошел на...   \n24153  Байден пообещал американским мужчинам, что они...   \n24152  Моё тело – моё дело: американки протестуют из-...   \n24147  В США начались погромы из-за запрета абортов: ...   \n...                                                  ...   \n25143  Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...   \n2528   Rosja może uniknąć bankructwa. Zachodnia machi...   \n2530   Embargo na gaz i ropę z Rosji? Jednoznaczne st...   \n2527   Korwin-Mikke: Nakładanie na Rosję sankcji jest...   \n2564   Grzegorz Braun i naukowcy ws. pokoju na Ukrain...   \n\n                                  title_and_10_sentences  \\\nid                                                         \n24151  Байдену напомнили о его отношению к абортам в ...   \n24150  В США после отмены права на аборт произошел на...   \n24153  Байден пообещал американским мужчинам, что они...   \n24152  Моё тело – моё дело: американки протестуют из-...   \n24147  В США начались погромы из-за запрета абортов: ...   \n...                                                  ...   \n25143  Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...   \n2528   Rosja może uniknąć bankructwa. Zachodnia machi...   \n2530   Embargo na gaz i ropę z Rosji? Jednoznaczne st...   \n2527   Korwin-Mikke: Nakładanie na Rosję sankcji jest...   \n2564   Grzegorz Braun i naukowcy ws. pokoju na Ukrain...   \n\n                               title_and_first_paragraph  \\\nid                                                         \n24151  Байдену напомнили о его отношению к абортам в ...   \n24150  В США после отмены права на аборт произошел на...   \n24153  Байден пообещал американским мужчинам, что они...   \n24152  Моё тело – моё дело: американки протестуют из-...   \n24147  В США начались погромы из-за запрета абортов: ...   \n...                                                  ...   \n25143  Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...   \n2528   Rosja może uniknąć bankructwa. Zachodnia machi...   \n2530   Embargo na gaz i ropę z Rosji? Jednoznaczne st...   \n2527   Korwin-Mikke: Nakładanie na Rosję sankcji jest...   \n2564   Grzegorz Braun i naukowcy ws. pokoju na Ukrain...   \n\n                 title_and_first_sentence_each_paragraph language  \nid                                                                 \n24151  CNN напомнил о трансформации отношения Байдена...       ru  \n24150  В США после отмены права на аборт произошел на...       ru  \n24153  Решение Верховного суда об абортах ужасно. «Эт...       ru  \n24152  Что случилось? Верховный суд США отменил конст...       ru  \n24147  Почему скандальный закон приняли именно сейчас...       ru  \n...                                                  ...      ...  \n25143  Każdy, kto miał kiedyś nowy samochód wie, że p...       po  \n2528   Nie jest pewne, czy obecne sankcje wystarczą d...       po  \n2530   Marine Le Pen, rywalka Emmanuela Macrona w wyś...       po  \n2527   Prędzej Rosja sobie poradzi bez złota, niż my ...       po  \n2564   ako środowisko narodowo-radykalne mamy swoje s...       po  \n\n[1589 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frames</th>\n      <th>raw_text</th>\n      <th>title</th>\n      <th>content</th>\n      <th>title_and_5_sentences</th>\n      <th>title_and_10_sentences</th>\n      <th>title_and_first_paragraph</th>\n      <th>title_and_first_sentence_each_paragraph</th>\n      <th>language</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24151</th>\n      <td>Legality_Constitutionality_and_jurisprudence,H...</td>\n      <td>Байдену напомнили о его отношению к абортам в ...</td>\n      <td>Байдену напомнили о его отношению к абортам в ...</td>\n      <td>CNN напомнил о трансформации отношения Байдена...</td>\n      <td>Байдену напомнили о его отношению к абортам в ...</td>\n      <td>Байдену напомнили о его отношению к абортам в ...</td>\n      <td>Байдену напомнили о его отношению к абортам в ...</td>\n      <td>CNN напомнил о трансформации отношения Байдена...</td>\n      <td>ru</td>\n    </tr>\n    <tr>\n      <th>24150</th>\n      <td>Legality_Constitutionality_and_jurisprudence,H...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>В США после отмены права на аборт произошел на...</td>\n      <td>ru</td>\n    </tr>\n    <tr>\n      <th>24153</th>\n      <td>Legality_Constitutionality_and_jurisprudence,F...</td>\n      <td>Байден пообещал американским мужчинам, что они...</td>\n      <td>Байден пообещал американским мужчинам, что они...</td>\n      <td>Решение Верховного суда об абортах ужасно. Не ...</td>\n      <td>Байден пообещал американским мужчинам, что они...</td>\n      <td>Байден пообещал американским мужчинам, что они...</td>\n      <td>Байден пообещал американским мужчинам, что они...</td>\n      <td>Решение Верховного суда об абортах ужасно. «Эт...</td>\n      <td>ru</td>\n    </tr>\n    <tr>\n      <th>24152</th>\n      <td>Legality_Constitutionality_and_jurisprudence,P...</td>\n      <td>Моё тело – моё дело: американки протестуют из-...</td>\n      <td>Моё тело – моё дело: американки протестуют из-...</td>\n      <td>Что случилось?\\n\\nВерховный суд США отменил ко...</td>\n      <td>Моё тело – моё дело: американки протестуют из-...</td>\n      <td>Моё тело – моё дело: американки протестуют из-...</td>\n      <td>Моё тело – моё дело: американки протестуют из-...</td>\n      <td>Что случилось? Верховный суд США отменил конст...</td>\n      <td>ru</td>\n    </tr>\n    <tr>\n      <th>24147</th>\n      <td>Legality_Constitutionality_and_jurisprudence,E...</td>\n      <td>В США начались погромы из-за запрета абортов: ...</td>\n      <td>В США начались погромы из-за запрета абортов: ...</td>\n      <td>Почему скандальный закон приняли именно сейчас...</td>\n      <td>В США начались погромы из-за запрета абортов: ...</td>\n      <td>В США начались погромы из-за запрета абортов: ...</td>\n      <td>В США начались погромы из-за запрета абортов: ...</td>\n      <td>Почему скандальный закон приняли именно сейчас...</td>\n      <td>ru</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25143</th>\n      <td>Political,Security_and_defense</td>\n      <td>Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...</td>\n      <td>Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...</td>\n      <td>Każdy, kto miał kiedyś nowy samochód wie, że p...</td>\n      <td>Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...</td>\n      <td>Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...</td>\n      <td>Kłopoty z dotarciem, czyli Objazdowy Cyrk Pana...</td>\n      <td>Każdy, kto miał kiedyś nowy samochód wie, że p...</td>\n      <td>po</td>\n    </tr>\n    <tr>\n      <th>2528</th>\n      <td>Policy_prescription_and_evaluation,Economic,Ex...</td>\n      <td>Rosja może uniknąć bankructwa. Zachodnia machi...</td>\n      <td>Rosja może uniknąć bankructwa. Zachodnia machi...</td>\n      <td>Nie jest pewne, czy obecne sankcje wystarczą d...</td>\n      <td>Rosja może uniknąć bankructwa. Zachodnia machi...</td>\n      <td>Rosja może uniknąć bankructwa. Zachodnia machi...</td>\n      <td>Rosja może uniknąć bankructwa. Zachodnia machi...</td>\n      <td>Nie jest pewne, czy obecne sankcje wystarczą d...</td>\n      <td>po</td>\n    </tr>\n    <tr>\n      <th>2530</th>\n      <td>Policy_prescription_and_evaluation,Economic,Ex...</td>\n      <td>Embargo na gaz i ropę z Rosji? Jednoznaczne st...</td>\n      <td>Embargo na gaz i ropę z Rosji? Jednoznaczne st...</td>\n      <td>Marine Le Pen, rywalka Emmanuela Macrona w wyś...</td>\n      <td>Embargo na gaz i ropę z Rosji? Jednoznaczne st...</td>\n      <td>Embargo na gaz i ropę z Rosji? Jednoznaczne st...</td>\n      <td>Embargo na gaz i ropę z Rosji? Jednoznaczne st...</td>\n      <td>Marine Le Pen, rywalka Emmanuela Macrona w wyś...</td>\n      <td>po</td>\n    </tr>\n    <tr>\n      <th>2527</th>\n      <td>Policy_prescription_and_evaluation,Economic,Qu...</td>\n      <td>Korwin-Mikke: Nakładanie na Rosję sankcji jest...</td>\n      <td>Korwin-Mikke: Nakładanie na Rosję sankcji jest...</td>\n      <td>Prędzej Rosja sobie poradzi bez złota, niż my ...</td>\n      <td>Korwin-Mikke: Nakładanie na Rosję sankcji jest...</td>\n      <td>Korwin-Mikke: Nakładanie na Rosję sankcji jest...</td>\n      <td>Korwin-Mikke: Nakładanie na Rosję sankcji jest...</td>\n      <td>Prędzej Rosja sobie poradzi bez złota, niż my ...</td>\n      <td>po</td>\n    </tr>\n    <tr>\n      <th>2564</th>\n      <td>Policy_prescription_and_evaluation,Morality,Ec...</td>\n      <td>Grzegorz Braun i naukowcy ws. pokoju na Ukrain...</td>\n      <td>Grzegorz Braun i naukowcy ws. pokoju na Ukrainie</td>\n      <td>ako środowisko narodowo-radykalne mamy swoje s...</td>\n      <td>Grzegorz Braun i naukowcy ws. pokoju na Ukrain...</td>\n      <td>Grzegorz Braun i naukowcy ws. pokoju na Ukrain...</td>\n      <td>Grzegorz Braun i naukowcy ws. pokoju na Ukrain...</td>\n      <td>ako środowisko narodowo-radykalne mamy swoje s...</td>\n      <td>po</td>\n    </tr>\n  </tbody>\n</table>\n<p>1589 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encode labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([LABELS])\n",
    "\n",
    "labels_npy = mlb.transform(df.frames.str.lower().str.split(','))\n",
    "df['label'] = [list(labels_npy[i, :]) for i in range(labels_npy.shape[0])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do iterative stratification to create a train and holdout set, stratifying per language"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "splits = 3\n",
    "mskf = MultilabelStratifiedKFold(n_splits=splits, shuffle=True, random_state=0)\n",
    "\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "for language, df_ in df.groupby('language'):\n",
    "    X = df_[[col for col in df.columns if col not in ['label', 'frames']]]\n",
    "    y = df_[[col for col in df.columns if col in ['label', 'frames']]]\n",
    "\n",
    "    for train_index, test_index in mskf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        train_dfs.append(X_train.join(y_train))\n",
    "        test_dfs.append(X_test.join(y_test))\n",
    "\n",
    "        break\n",
    "\n",
    "train_df = pd.concat(train_dfs)\n",
    "test_df = pd.concat(test_dfs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Dataset object"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a 1D array, got an array with shape (1589, 14)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3802\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/frame.py:4139\u001B[0m, in \u001B[0;36mDataFrame._set_item_mgr\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4138\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 4139\u001B[0m     loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_info_axis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4140\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m   4141\u001B[0m     \u001B[38;5;66;03m# This item wasn't present, just insert at end\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3808\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3809\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'label'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m mlb \u001B[38;5;241m=\u001B[39m MultiLabelBinarizer()\n\u001B[1;32m      2\u001B[0m mlb\u001B[38;5;241m.\u001B[39mfit([LABELS])\n\u001B[0;32m----> 3\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m mlb\u001B[38;5;241m.\u001B[39mtransform(df\u001B[38;5;241m.\u001B[39mframes\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      4\u001B[0m df\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/frame.py:3978\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   3975\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n\u001B[1;32m   3976\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3977\u001B[0m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[0;32m-> 3978\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/frame.py:4185\u001B[0m, in \u001B[0;36mDataFrame._set_item\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4182\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(existing_piece, DataFrame):\n\u001B[1;32m   4183\u001B[0m             value \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mtile(value, (\u001B[38;5;28mlen\u001B[39m(existing_piece\u001B[38;5;241m.\u001B[39mcolumns), \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m-> 4185\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/frame.py:4142\u001B[0m, in \u001B[0;36mDataFrame._set_item_mgr\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4139\u001B[0m     loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4140\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m   4141\u001B[0m     \u001B[38;5;66;03m# This item wasn't present, just insert at end\u001B[39;00m\n\u001B[0;32m-> 4142\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_info_axis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iset_item_mgr(loc, value)\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/pandas/core/internals/managers.py:1407\u001B[0m, in \u001B[0;36mBlockManager.insert\u001B[0;34m(self, loc, item, value)\u001B[0m\n\u001B[1;32m   1405\u001B[0m     value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mT\n\u001B[1;32m   1406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(value) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 1407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1408\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a 1D array, got an array with shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1409\u001B[0m         )\n\u001B[1;32m   1410\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1411\u001B[0m     value \u001B[38;5;241m=\u001B[39m ensure_block_shape(value, ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim)\n",
      "\u001B[0;31mValueError\u001B[0m: Expected a 1D array, got an array with shape (1589, 14)"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({'train': train_ds, 'test': test_ds})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['raw_text', 'title', 'content', 'title_and_5_sentences', 'title_and_10_sentences', 'title_and_first_paragraph', 'title_and_first_sentence_each_paragraph', 'language', 'frames', 'label', 'id'],\n        num_rows: 1059\n    })\n    test: Dataset({\n        features: ['raw_text', 'title', 'content', 'title_and_5_sentences', 'title_and_10_sentences', 'title_and_first_paragraph', 'title_and_first_sentence_each_paragraph', 'language', 'frames', 'label', 'id'],\n        num_rows: 530\n    })\n})"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/1059 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eca3591480d849f09ea08aeafb529a29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/530 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d7a90c783ac4654bd454265bdedebe1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(os.path.join('..', '..', 'data', 'preprocessed','multilingual_train_test_ds.hf'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test a model"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0337e99870d949fea15894f1faf00762"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "335a0216a22246aa939806dbb19e42ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c50a244c7094500a2a768e03fe502ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /home/juanbermeo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d0e66beb3a44e1848d151dcf8cf6e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7444b4eb1d54465b8f5b218dd016ba69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d017a6dbd13d41259a2751e6618641cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2324bc8f97f546acb49a65c5702f0c49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/juanbermeo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c9dd429bd634ef3abc6e7e0eb9f32e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "id2label= {idx:label for idx, label in enumerate(mlb.classes_)}\n",
    "label2id = {label:idx for idx, label in enumerate(mlb.classes_)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "dataset = load_from_disk(os.path.join('..', '..', 'data', 'preprocessed','multilingual_train_test_ds.hf'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nXLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m classifier \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjoeddav/xlm-roberta-large-xnli\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mzero-shot-classification\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/pipelines/__init__.py:828\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    825\u001B[0m             tokenizer_identifier \u001B[38;5;241m=\u001B[39m tokenizer\n\u001B[1;32m    826\u001B[0m             tokenizer_kwargs \u001B[38;5;241m=\u001B[39m model_kwargs\n\u001B[0;32m--> 828\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtokenizer_identifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtokenizer_kwargs\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_feature_extractor:\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;66;03m# Try to infer feature extractor from model or config name (if provided as str)\u001B[39;00m\n\u001B[1;32m    834\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m feature_extractor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:676\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    674\u001B[0m tokenizer_class_py, tokenizer_class_fast \u001B[38;5;241m=\u001B[39m TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 676\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class_fast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1804\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1801\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1802\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1804\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1805\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1806\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1807\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1808\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1809\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1810\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1811\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1812\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1813\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1814\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1959\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1957\u001B[0m \u001B[38;5;66;03m# Instantiate tokenizer.\u001B[39;00m\n\u001B[1;32m   1958\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1959\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1960\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   1961\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   1962\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load vocabulary from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1963\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1964\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155\u001B[0m, in \u001B[0;36mXLMRobertaTokenizerFast.__init__\u001B[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    141\u001B[0m     vocab_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    151\u001B[0m ):\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001B[39;00m\n\u001B[1;32m    153\u001B[0m     mask_token \u001B[38;5;241m=\u001B[39m AddedToken(mask_token, lstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, rstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mask_token, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m mask_token\n\u001B[0;32m--> 155\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbos_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43msep_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcls_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43munk_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file \u001B[38;5;241m=\u001B[39m vocab_file\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcan_save_slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:114\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m TokenizerFast\u001B[38;5;241m.\u001B[39mfrom_file(fast_tokenizer_file)\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m slow_tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_slow_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslow_tokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001B[39;00m\n\u001B[1;32m    117\u001B[0m     slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1162\u001B[0m, in \u001B[0;36mconvert_slow_tokenizer\u001B[0;34m(transformer_tokenizer)\u001B[0m\n\u001B[1;32m   1154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1155\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn instance of tokenizer class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokenizer_class_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m cannot be converted in a Fast tokenizer instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1156\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m No converter was found. Currently available slow->fast convertors:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1157\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(SLOW_TO_FAST_CONVERTERS\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1158\u001B[0m     )\n\u001B[1;32m   1160\u001B[0m converter_class \u001B[38;5;241m=\u001B[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001B[0;32m-> 1162\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconverter_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer_tokenizer\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconverted()\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:434\u001B[0m, in \u001B[0;36mSpmConverter.__init__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m--> 434\u001B[0m     \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprotobuf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sentencepiece_model_pb2 \u001B[38;5;28;01mas\u001B[39;00m model_pb2\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing_HF/lib/python3.9/site-packages/transformers/utils/import_utils.py:1038\u001B[0m, in \u001B[0;36mrequires_backends\u001B[0;34m(obj, backends)\u001B[0m\n\u001B[1;32m   1036\u001B[0m failed \u001B[38;5;241m=\u001B[39m [msg\u001B[38;5;241m.\u001B[39mformat(name) \u001B[38;5;28;01mfor\u001B[39;00m available, msg \u001B[38;5;129;01min\u001B[39;00m checks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m available()]\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failed:\n\u001B[0;32m-> 1038\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(failed))\n",
      "\u001B[0;31mImportError\u001B[0m: \nXLMRobertaConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(model='xlm-roberta-large', task='zero-shot-classification')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m classifier \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241m.\u001B[39mpipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzero-shot-classification\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      2\u001B[0m                       model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfacebook/bart-large-mnli\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(model=\"joeddav/xlm-roberta-large-xnli\", task=\"zero-shot-classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/utils/import_utils.py:1110\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1109\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/importlib/__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1030\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:986\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:680\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:850\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:228\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/pipelines/__init__.py:37\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_auto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForDepthEstimation\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_auto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOKENIZER_MAPPING, AutoTokenizer\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedTokenizer\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:25\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdynamic_module_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_class_from_dynamic_module\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedTokenizer\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOKENIZER_CONFIG_FILE\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/tokenization_utils.py:26\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, List, Optional, Tuple, Union, overload\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     27\u001B[0m     ENCODE_KWARGS_DOCSTRING,\n\u001B[1;32m     28\u001B[0m     ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,\n\u001B[1;32m     29\u001B[0m     INIT_TOKENIZER_DOCSTRING,\n\u001B[1;32m     30\u001B[0m     AddedToken,\n\u001B[1;32m     31\u001B[0m     BatchEncoding,\n\u001B[1;32m     32\u001B[0m     EncodedInput,\n\u001B[1;32m     33\u001B[0m     EncodedInputPair,\n\u001B[1;32m     34\u001B[0m     PreTokenizedInput,\n\u001B[1;32m     35\u001B[0m     PreTokenizedInputPair,\n\u001B[1;32m     36\u001B[0m     PreTrainedTokenizerBase,\n\u001B[1;32m     37\u001B[0m     TextInput,\n\u001B[1;32m     38\u001B[0m     TextInputPair,\n\u001B[1;32m     39\u001B[0m     TruncationStrategy,\n\u001B[1;32m     40\u001B[0m )\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PaddingStrategy, TensorType, add_end_docstrings, logging\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:74\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_tokenizers_available():\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AddedToken\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Encoding \u001B[38;5;28;01mas\u001B[39;00m EncodingFast\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/tokenizers/__init__.py:79\u001B[0m\n\u001B[1;32m     76\u001B[0m     CONTIGUOUS \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontiguous\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 79\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     80\u001B[0m     Tokenizer,\n\u001B[1;32m     81\u001B[0m     Encoding,\n\u001B[1;32m     82\u001B[0m     AddedToken,\n\u001B[1;32m     83\u001B[0m     Regex,\n\u001B[1;32m     84\u001B[0m     NormalizedString,\n\u001B[1;32m     85\u001B[0m     PreTokenizedString,\n\u001B[1;32m     86\u001B[0m     Token,\n\u001B[1;32m     87\u001B[0m )\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decoders\n",
      "\u001B[0;31mImportError\u001B[0m: libssl.so.10: cannot open shared object file: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m classifier \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipeline\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzero-shot-classification\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      2\u001B[0m                       model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfacebook/bart-large-mnli\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/utils/import_utils.py:1100\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1098\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1100\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1101\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m   1102\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/Framing/lib/python3.9/site-packages/transformers/utils/import_utils.py:1112\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1112\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1113\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1114\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1115\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "classifier = transformers.pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
